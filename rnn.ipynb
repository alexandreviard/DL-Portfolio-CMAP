{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision #for dataset \n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(PortfolioRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        # x -> (batch_size, timesteps/seq, input_size/feature_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X, r):\n",
    "        h0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(X,h0)\n",
    "        # out -> (batch_size, seq_length, hidden_size)\n",
    "        #out = out[:,-1,:] only take the last time step NO WE NEED ALL TIME STEPS\n",
    "        out = self.linear(out)\n",
    "        out = torch.softmax(out, dim=-1)\n",
    "        portfolio_returns = torch.sum(out[:, :, :] * r[:, :, :], dim=-1)\n",
    "        # portfolio_returns -> (batch_size)\n",
    "        sharpe = torch.mean(portfolio_returns, dim = -1) / torch.std(portfolio_returns, dim = -1)\n",
    "        return -sharpe.mean()\n",
    "    \n",
    "    def get_allocations(self, X):\n",
    "        with torch.no_grad():\n",
    "            h0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(device)\n",
    "            out, _ = self.rnn(X,h0)\n",
    "            out = self.linear(out)\n",
    "            out = torch.softmax(out, dim=-1)\n",
    "        return out[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output loss (negative Sharpe ratio): -1.6340763568878174\n"
     ]
    }
   ],
   "source": [
    "model = PortfolioRNN(5, 1, 1, 2).to(device)\n",
    "\n",
    "# Generate random input data\n",
    "# X -> (batch_size, seq_length, input_size)\n",
    "X = torch.rand(2, 10, 5).to(device)\n",
    "\n",
    "# r -> (batch_size, seq_length, output_size)\n",
    "r = torch.rand(2, 10, 2).to(device)\n",
    "\n",
    "# Call the forward method\n",
    "loss = model(X, r)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output loss (negative Sharpe ratio):\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  4 of 4 completed\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2234399925.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_R'] = data_na[f'{column}'].pct_change()\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2234399925.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_y'] = data_na[f'{column}_R'].shift(-1)\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2234399925.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_R'] = data_na[f'{column}'].pct_change()\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2234399925.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_y'] = data_na[f'{column}_R'].shift(-1)\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2234399925.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_R'] = data_na[f'{column}'].pct_change()\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2234399925.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_y'] = data_na[f'{column}_R'].shift(-1)\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2234399925.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_R'] = data_na[f'{column}'].pct_change()\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2234399925.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_y'] = data_na[f'{column}_R'].shift(-1)\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2234399925.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na.dropna(axis=0, inplace=True)\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2234399925.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na['Date'] = data_na['Date'].dt.date\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>AGG</th>\n",
       "      <th>DBC</th>\n",
       "      <th>VTI</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>AGG_R</th>\n",
       "      <th>AGG_y</th>\n",
       "      <th>DBC_R</th>\n",
       "      <th>DBC_y</th>\n",
       "      <th>VTI_R</th>\n",
       "      <th>VTI_y</th>\n",
       "      <th>^VIX_R</th>\n",
       "      <th>^VIX_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-02-07</td>\n",
       "      <td>56.438553</td>\n",
       "      <td>20.285254</td>\n",
       "      <td>44.219505</td>\n",
       "      <td>13.590000</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>-0.028926</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.009736</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.042178</td>\n",
       "      <td>-0.055923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-02-08</td>\n",
       "      <td>56.410431</td>\n",
       "      <td>20.198935</td>\n",
       "      <td>44.537621</td>\n",
       "      <td>12.830000</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>-0.001904</td>\n",
       "      <td>-0.055923</td>\n",
       "      <td>0.022603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-02-09</td>\n",
       "      <td>56.444218</td>\n",
       "      <td>20.388840</td>\n",
       "      <td>44.452812</td>\n",
       "      <td>13.120000</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>-0.002097</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>-0.018205</td>\n",
       "      <td>-0.001904</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.022603</td>\n",
       "      <td>-0.019055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-02-10</td>\n",
       "      <td>56.325867</td>\n",
       "      <td>20.017662</td>\n",
       "      <td>44.544685</td>\n",
       "      <td>12.870000</td>\n",
       "      <td>-0.002097</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>-0.018205</td>\n",
       "      <td>-0.015524</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>-0.004523</td>\n",
       "      <td>-0.019055</td>\n",
       "      <td>0.037296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-02-13</td>\n",
       "      <td>56.365341</td>\n",
       "      <td>19.706909</td>\n",
       "      <td>44.343220</td>\n",
       "      <td>13.350000</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>-0.001601</td>\n",
       "      <td>-0.015524</td>\n",
       "      <td>-0.008322</td>\n",
       "      <td>-0.004523</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>0.037296</td>\n",
       "      <td>-0.082397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>2020-12-22</td>\n",
       "      <td>106.759186</td>\n",
       "      <td>13.624199</td>\n",
       "      <td>181.857056</td>\n",
       "      <td>24.230000</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>-0.011004</td>\n",
       "      <td>0.012517</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>-0.036963</td>\n",
       "      <td>-0.037969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>106.686790</td>\n",
       "      <td>13.794738</td>\n",
       "      <td>182.168045</td>\n",
       "      <td>23.309999</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.012517</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>-0.037969</td>\n",
       "      <td>-0.076362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>106.786339</td>\n",
       "      <td>13.832635</td>\n",
       "      <td>182.472778</td>\n",
       "      <td>21.530001</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>-0.006164</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>-0.076362</td>\n",
       "      <td>0.007896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>106.804451</td>\n",
       "      <td>13.747366</td>\n",
       "      <td>183.627319</td>\n",
       "      <td>21.700001</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>-0.006164</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>-0.004175</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.063594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>106.822540</td>\n",
       "      <td>13.785263</td>\n",
       "      <td>182.860733</td>\n",
       "      <td>23.080000</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>-0.004175</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.063594</td>\n",
       "      <td>-0.013432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3750 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker        Date         AGG        DBC         VTI       ^VIX     AGG_R  \\\n",
       "0       2006-02-07   56.438553  20.285254   44.219505  13.590000 -0.000699   \n",
       "1       2006-02-08   56.410431  20.198935   44.537621  12.830000 -0.000498   \n",
       "2       2006-02-09   56.444218  20.388840   44.452812  13.120000  0.000599   \n",
       "3       2006-02-10   56.325867  20.017662   44.544685  12.870000 -0.002097   \n",
       "4       2006-02-13   56.365341  19.706909   44.343220  13.350000  0.000701   \n",
       "...            ...         ...        ...         ...        ...       ...   \n",
       "3745    2020-12-22  106.759186  13.624199  181.857056  24.230000  0.001443   \n",
       "3746    2020-12-23  106.686790  13.794738  182.168045  23.309999 -0.000678   \n",
       "3747    2020-12-24  106.786339  13.832635  182.472778  21.530001  0.000933   \n",
       "3748    2020-12-28  106.804451  13.747366  183.627319  21.700001  0.000170   \n",
       "3749    2020-12-29  106.822540  13.785263  182.860733  23.080000  0.000169   \n",
       "\n",
       "Ticker     AGG_y     DBC_R     DBC_y     VTI_R     VTI_y    ^VIX_R    ^VIX_y  \n",
       "0      -0.000498 -0.028926 -0.004255 -0.009736  0.007194  0.042178 -0.055923  \n",
       "1       0.000599 -0.004255  0.009402  0.007194 -0.001904 -0.055923  0.022603  \n",
       "2      -0.002097  0.009402 -0.018205 -0.001904  0.002067  0.022603 -0.019055  \n",
       "3       0.000701 -0.018205 -0.015524  0.002067 -0.004523 -0.019055  0.037296  \n",
       "4      -0.001601 -0.015524 -0.008322 -0.004523  0.009485  0.037296 -0.082397  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "3745   -0.000678 -0.011004  0.012517  0.000207  0.001710 -0.036963 -0.037969  \n",
       "3746    0.000933  0.012517  0.002747  0.001710  0.001673 -0.037969 -0.076362  \n",
       "3747    0.000170  0.002747 -0.006164  0.001673  0.006327 -0.076362  0.007896  \n",
       "3748    0.000169 -0.006164  0.002757  0.006327 -0.004175  0.007896  0.063594  \n",
       "3749    0.000593  0.002757  0.006186 -0.004175  0.002691  0.063594 -0.013432  \n",
       "\n",
       "[3750 rows x 13 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "tickers = ['VTI', 'AGG', 'DBC', '^VIX']\n",
    "data = yf.download(tickers, start=\"2006-01-01\", end=\"2020-12-31\", interval=\"1d\")['Adj Close']\n",
    "data_na = data.dropna(axis = 0)\n",
    "for column in data_na.columns:\n",
    "    data_na[f'{column}_R'] = data_na[f'{column}'].pct_change()\n",
    "    data_na[f'{column}_y'] = data_na[f'{column}_R'].shift(-1)\n",
    "data_na.dropna(axis=0, inplace=True)\n",
    "data_na.reset_index(inplace=True)\n",
    "data_na['Date'] = data_na['Date'].dt.date\n",
    "data_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>AGG</th>\n",
       "      <th>DBC</th>\n",
       "      <th>VTI</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>AGG_R</th>\n",
       "      <th>AGG_y</th>\n",
       "      <th>DBC_R</th>\n",
       "      <th>DBC_y</th>\n",
       "      <th>VTI_R</th>\n",
       "      <th>VTI_y</th>\n",
       "      <th>^VIX_R</th>\n",
       "      <th>^VIX_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-02-07</td>\n",
       "      <td>56.438553</td>\n",
       "      <td>20.285254</td>\n",
       "      <td>44.219505</td>\n",
       "      <td>13.59</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>-0.028926</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.009736</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.042178</td>\n",
       "      <td>-0.055923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-02-08</td>\n",
       "      <td>56.410431</td>\n",
       "      <td>20.198935</td>\n",
       "      <td>44.537621</td>\n",
       "      <td>12.83</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>-0.001904</td>\n",
       "      <td>-0.055923</td>\n",
       "      <td>0.022603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-02-09</td>\n",
       "      <td>56.444218</td>\n",
       "      <td>20.388840</td>\n",
       "      <td>44.452812</td>\n",
       "      <td>13.12</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>-0.002097</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>-0.018205</td>\n",
       "      <td>-0.001904</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.022603</td>\n",
       "      <td>-0.019055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-02-10</td>\n",
       "      <td>56.325867</td>\n",
       "      <td>20.017662</td>\n",
       "      <td>44.544685</td>\n",
       "      <td>12.87</td>\n",
       "      <td>-0.002097</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>-0.018205</td>\n",
       "      <td>-0.015524</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>-0.004523</td>\n",
       "      <td>-0.019055</td>\n",
       "      <td>0.037296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-02-13</td>\n",
       "      <td>56.365341</td>\n",
       "      <td>19.706909</td>\n",
       "      <td>44.343220</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>-0.001601</td>\n",
       "      <td>-0.015524</td>\n",
       "      <td>-0.008322</td>\n",
       "      <td>-0.004523</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>0.037296</td>\n",
       "      <td>-0.082397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2006-02-14</td>\n",
       "      <td>56.275108</td>\n",
       "      <td>19.542900</td>\n",
       "      <td>44.763836</td>\n",
       "      <td>12.25</td>\n",
       "      <td>-0.001601</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>-0.008322</td>\n",
       "      <td>-0.014576</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>-0.082397</td>\n",
       "      <td>0.004898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006-02-15</td>\n",
       "      <td>56.337143</td>\n",
       "      <td>19.258043</td>\n",
       "      <td>44.933498</td>\n",
       "      <td>12.31</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>-0.001001</td>\n",
       "      <td>-0.014576</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.007788</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>-0.067425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2006-02-16</td>\n",
       "      <td>56.280758</td>\n",
       "      <td>19.491110</td>\n",
       "      <td>45.283451</td>\n",
       "      <td>11.48</td>\n",
       "      <td>-0.001001</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>0.014172</td>\n",
       "      <td>0.007788</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>-0.067425</td>\n",
       "      <td>0.046167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2006-02-17</td>\n",
       "      <td>56.511803</td>\n",
       "      <td>19.767332</td>\n",
       "      <td>45.230442</td>\n",
       "      <td>12.01</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>0.014172</td>\n",
       "      <td>0.016594</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>-0.002813</td>\n",
       "      <td>0.046167</td>\n",
       "      <td>0.033306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2006-02-21</td>\n",
       "      <td>56.303352</td>\n",
       "      <td>20.095352</td>\n",
       "      <td>45.103188</td>\n",
       "      <td>12.41</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>0.016594</td>\n",
       "      <td>-0.016323</td>\n",
       "      <td>-0.002813</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>0.033306</td>\n",
       "      <td>-0.042707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker       Date        AGG        DBC        VTI   ^VIX     AGG_R     AGG_y  \\\n",
       "0      2006-02-07  56.438553  20.285254  44.219505  13.59 -0.000699 -0.000498   \n",
       "1      2006-02-08  56.410431  20.198935  44.537621  12.83 -0.000498  0.000599   \n",
       "2      2006-02-09  56.444218  20.388840  44.452812  13.12  0.000599 -0.002097   \n",
       "3      2006-02-10  56.325867  20.017662  44.544685  12.87 -0.002097  0.000701   \n",
       "4      2006-02-13  56.365341  19.706909  44.343220  13.35  0.000701 -0.001601   \n",
       "5      2006-02-14  56.275108  19.542900  44.763836  12.25 -0.001601  0.001102   \n",
       "6      2006-02-15  56.337143  19.258043  44.933498  12.31  0.001102 -0.001001   \n",
       "7      2006-02-16  56.280758  19.491110  45.283451  11.48 -0.001001  0.004105   \n",
       "8      2006-02-17  56.511803  19.767332  45.230442  12.01  0.004105 -0.003689   \n",
       "9      2006-02-21  56.303352  20.095352  45.103188  12.41 -0.003689  0.004003   \n",
       "\n",
       "Ticker     DBC_R     DBC_y     VTI_R     VTI_y    ^VIX_R    ^VIX_y  \n",
       "0      -0.028926 -0.004255 -0.009736  0.007194  0.042178 -0.055923  \n",
       "1      -0.004255  0.009402  0.007194 -0.001904 -0.055923  0.022603  \n",
       "2       0.009402 -0.018205 -0.001904  0.002067  0.022603 -0.019055  \n",
       "3      -0.018205 -0.015524  0.002067 -0.004523 -0.019055  0.037296  \n",
       "4      -0.015524 -0.008322 -0.004523  0.009485  0.037296 -0.082397  \n",
       "5      -0.008322 -0.014576  0.009485  0.003790 -0.082397  0.004898  \n",
       "6      -0.014576  0.012102  0.003790  0.007788  0.004898 -0.067425  \n",
       "7       0.012102  0.014172  0.007788 -0.001171 -0.067425  0.046167  \n",
       "8       0.014172  0.016594 -0.001171 -0.002813  0.046167  0.033306  \n",
       "9       0.016594 -0.016323 -0.002813  0.007837  0.033306 -0.042707  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_na.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2367803767.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n"
     ]
    }
   ],
   "source": [
    "def create_batches(dataframe, start_date, end_date, window_size=50):\n",
    "    dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n",
    "    \n",
    "    filtered_data = dataframe[(dataframe['Date'] >= start_date) & (dataframe['Date'] <= end_date)]\n",
    "    \n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    dates_batches = []\n",
    "    dates_per_feature = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(filtered_data) - window_size):\n",
    "        window_returns = filtered_data.iloc[i:i+window_size][[col for col in dataframe.columns if col.endswith('_R')]].values\n",
    "        window_prices = filtered_data.iloc[i:i+window_size][[col for col in dataframe.columns if col in ['AGG', 'DBC', 'VTI', '^VIX']]].values\n",
    "        \n",
    "        window_x = np.concatenate([window_returns, window_prices], axis=1)\n",
    "        window_y = filtered_data.iloc[i:i+window_size][[col for col in dataframe.columns if col.endswith('_y')]].values\n",
    "        \n",
    "        window_dates_batches = filtered_data.iloc[i:i+window_size]['Date'].values\n",
    "        dates_per_feature.append(filtered_data.iloc[i+window_size]['Date'].date()) \n",
    "\n",
    "        y.append(filtered_data.iloc[i+window_size][[col for col in dataframe.columns if col.endswith('_y')]].values)\n",
    "\n",
    "        x_batches.append(window_x)\n",
    "        y_batches.append(window_y)\n",
    "        dates_batches.append(window_dates_batches)\n",
    "\n",
    "    return np.array(x_batches), np.array(y_batches), np.array(y, dtype=np.float32), np.array(dates_batches), dates_per_feature\n",
    "\n",
    "x_batches, y_batches, y, dates_batches, dates_per_feature = create_batches(data_na, start_date=\"2006-01-01\", end_date=\"2020-12-29\")\n",
    "y = y.reshape(y.shape[0], 1, y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3700, 50, 8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3700"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates_per_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2006, 2, 22)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_na.iloc[10]['Date'].date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2006-02-07T00:00:00.000000000', '2006-02-08T00:00:00.000000000',\n",
       "       '2006-02-09T00:00:00.000000000', '2006-02-10T00:00:00.000000000',\n",
       "       '2006-02-13T00:00:00.000000000', '2006-02-14T00:00:00.000000000',\n",
       "       '2006-02-15T00:00:00.000000000', '2006-02-16T00:00:00.000000000',\n",
       "       '2006-02-17T00:00:00.000000000', '2006-02-21T00:00:00.000000000'],\n",
       "      dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_na.iloc[:10]['Date'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_returns = data_na.iloc[:10][['AGG_R', 'DBC_R']].values\n",
    "window_prices = data_na.iloc[:10][[col for col in data_na.columns if col in ['AGG', 'DBC']]].values\n",
    "\n",
    "window_returns.shape, window_prices.shape\n",
    "\n",
    "concatenaed = np.concatenate([window_returns, window_prices], axis=1)\n",
    "concatenaed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def get_idx_dates(start_date, end_date):\n",
    "    if start_date in dates_per_feature and end_date in dates_per_feature:\n",
    "        k_start = dates_per_feature.index(start_date)\n",
    "        k_end = dates_per_feature.index(end_date)\n",
    "        return k_start, k_end\n",
    "    \n",
    "    if start_date not in dates_per_feature: \n",
    "        if any(dates >= start_date for dates in dates_per_feature):    \n",
    "            k_start = min(j for j in range(len(dates_per_feature)) if dates_per_feature[j] >= start_date)\n",
    "        else:\n",
    "            k_start = 0\n",
    "    else:\n",
    "        k_start = dates_per_feature.index(start_date)\n",
    "\n",
    "    if end_date not in dates_per_feature:\n",
    "        if any(dates <= end_date for dates in dates_per_feature):\n",
    "            k_end = max(j for j in range(len(dates_per_feature)) if dates_per_feature[j] <= end_date) + 1\n",
    "        else:\n",
    "            k_end = len(dates_per_feature) -1\n",
    "    else: \n",
    "        k_end = dates_per_feature.index(end_date)\n",
    "\n",
    "    return k_start, k_end\n",
    "\n",
    "\n",
    "def training(x_batches, y_batches, model, batch_size):\n",
    "\n",
    "    x_batches_tensor = torch.tensor(x_batches, dtype=torch.float32)\n",
    "    y_batches_tensor = torch.tensor(y_batches, dtype=torch.float32)\n",
    "    num_batches = x_batches.shape[0]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 100\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_epoch = 0\n",
    "        \n",
    "        for i in range(0, num_batches, batch_size):\n",
    "            x_mini_batch = x_batches_tensor[i:i+batch_size]\n",
    "            y_mini_batch = y_batches_tensor[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = model(x_mini_batch, y_mini_batch)\n",
    "            loss_epoch += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"epoch [{epoch+1}/{num_epochs}], loss: {(loss_epoch/batch_size)}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def investing(x_batches, y, model):\n",
    "\n",
    "    x_tensor = torch.tensor(x_batches, dtype=torch.float32)\n",
    "    allocations = model.get_allocations(x_tensor)\n",
    "    # pourquoi ce shape ?\n",
    "    allocations = allocations.view(allocations.shape[0], 1, allocations.shape[1])\n",
    "    rdt = torch.sum(allocations*y, dim=2)\n",
    "\n",
    "    # return du portfeuille à t final du rnn, les allocations\n",
    "    return rdt, allocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss: -0.017145784542663023\n",
      "epoch [2/100], loss: -0.031068118376424536\n",
      "epoch [3/100], loss: -0.032567627684329636\n",
      "epoch [4/100], loss: -0.03829250670969486\n",
      "epoch [5/100], loss: -0.030205582857888658\n",
      "epoch [6/100], loss: -0.044016216779709794\n",
      "epoch [7/100], loss: -0.038969211251242086\n",
      "epoch [8/100], loss: -0.043501102803929825\n",
      "epoch [9/100], loss: -0.035723119603062514\n",
      "epoch [10/100], loss: -0.041922470030840486\n",
      "epoch [11/100], loss: -0.03999450944320415\n",
      "epoch [12/100], loss: -0.04672367420789669\n",
      "epoch [13/100], loss: -0.04587261657616182\n",
      "epoch [14/100], loss: -0.046662430537253385\n",
      "epoch [15/100], loss: -0.04547698769601993\n",
      "epoch [16/100], loss: -0.04166386016004253\n",
      "epoch [17/100], loss: -0.05181401543086395\n",
      "epoch [18/100], loss: -0.04006809197016992\n",
      "epoch [19/100], loss: -0.052555408685293514\n",
      "epoch [20/100], loss: -0.05317333483253606\n",
      "epoch [21/100], loss: -0.05676594477699837\n",
      "epoch [22/100], loss: -0.052053666120627895\n",
      "epoch [23/100], loss: -0.05445723296543292\n",
      "epoch [24/100], loss: -0.05256425912375562\n",
      "epoch [25/100], loss: -0.05179740974563174\n",
      "epoch [26/100], loss: -0.05693887744564563\n",
      "epoch [27/100], loss: -0.060176527593284845\n",
      "epoch [28/100], loss: -0.058621014293748885\n",
      "epoch [29/100], loss: -0.05633832752937451\n",
      "epoch [30/100], loss: -0.056722556066233665\n",
      "epoch [31/100], loss: -0.06055228773038834\n",
      "epoch [32/100], loss: -0.04821910712053068\n",
      "epoch [33/100], loss: -0.05954892500449205\n",
      "epoch [34/100], loss: -0.05515639786608517\n",
      "epoch [35/100], loss: -0.058665558623033576\n",
      "epoch [36/100], loss: -0.060270899441093206\n",
      "epoch [37/100], loss: -0.061933435732498765\n",
      "epoch [38/100], loss: -0.05269399654935114\n",
      "epoch [39/100], loss: -0.0644082062644884\n",
      "epoch [40/100], loss: -0.06235644228581805\n",
      "epoch [41/100], loss: -0.0665097814053297\n",
      "epoch [42/100], loss: -0.06869851730880328\n",
      "epoch [43/100], loss: -0.06298320187488571\n",
      "epoch [44/100], loss: -0.06280715158209205\n",
      "epoch [45/100], loss: -0.05694081899127923\n",
      "epoch [46/100], loss: -0.061694347532466054\n",
      "epoch [47/100], loss: -0.05672431802304345\n",
      "epoch [48/100], loss: -0.056590131018310785\n",
      "epoch [49/100], loss: -0.05746971449116245\n",
      "epoch [50/100], loss: -0.06286153974360786\n",
      "epoch [51/100], loss: -0.0677251408342272\n",
      "epoch [52/100], loss: -0.07043821725528687\n",
      "epoch [53/100], loss: -0.06974509940482676\n",
      "epoch [54/100], loss: -0.059895685233641416\n",
      "epoch [55/100], loss: -0.06425236241193488\n",
      "epoch [56/100], loss: -0.05996064096689224\n",
      "epoch [57/100], loss: -0.06735314289107919\n",
      "epoch [58/100], loss: -0.05822819331660867\n",
      "epoch [59/100], loss: -0.07025540166068822\n",
      "epoch [60/100], loss: -0.06614311737939715\n",
      "epoch [61/100], loss: -0.07178953057155013\n",
      "epoch [62/100], loss: -0.07054659281857312\n",
      "epoch [63/100], loss: -0.06633251384482719\n",
      "epoch [64/100], loss: -0.0732750702300109\n",
      "epoch [65/100], loss: -0.07678384718019515\n",
      "epoch [66/100], loss: -0.07741728459950536\n",
      "epoch [67/100], loss: -0.06491971551440656\n",
      "epoch [68/100], loss: -0.060155303799547255\n",
      "epoch [69/100], loss: -0.05377020384185016\n",
      "epoch [70/100], loss: -0.06970664684195071\n",
      "epoch [71/100], loss: -0.07136522588552907\n",
      "epoch [72/100], loss: -0.07555280870292336\n",
      "epoch [73/100], loss: -0.07327440124936402\n",
      "epoch [74/100], loss: -0.07816782454028726\n",
      "epoch [75/100], loss: -0.07829737919382751\n",
      "epoch [76/100], loss: -0.07212035357952118\n",
      "epoch [77/100], loss: -0.059467898798175156\n",
      "epoch [78/100], loss: -0.0787426105234772\n",
      "epoch [79/100], loss: -0.08220481313765049\n",
      "epoch [80/100], loss: -0.07474988300236873\n",
      "epoch [81/100], loss: -0.08137803710997105\n",
      "epoch [82/100], loss: -0.08068850380368531\n",
      "epoch [83/100], loss: -0.08311654650606215\n",
      "epoch [84/100], loss: -0.07756551261991262\n",
      "epoch [85/100], loss: -0.08373404573649168\n",
      "epoch [86/100], loss: -0.0806657646317035\n",
      "epoch [87/100], loss: -0.08334713894873857\n",
      "epoch [88/100], loss: -0.06554285369929858\n",
      "epoch [89/100], loss: -0.07402873924002051\n",
      "epoch [90/100], loss: -0.07345774711575359\n",
      "epoch [91/100], loss: -0.07800016502733342\n",
      "epoch [92/100], loss: -0.0829011993482709\n",
      "epoch [93/100], loss: -0.08155507873743773\n",
      "epoch [94/100], loss: -0.08268234017305076\n",
      "epoch [95/100], loss: -0.07203849661163986\n",
      "epoch [96/100], loss: -0.08625095803290606\n",
      "epoch [97/100], loss: -0.07586621306836605\n",
      "epoch [98/100], loss: -0.07838480570353568\n",
      "epoch [99/100], loss: -0.08656385820358992\n",
      "epoch [100/100], loss: -0.08801006502471864\n",
      "Idx start: 1005, Idx end: 1510\n",
      "2010-04-19 2012-04-18\n",
      "torch.Size([505, 1]) 505\n",
      "first_rdt shape: torch.Size([505, 1]), rdt_all shape before concat: torch.Size([0, 0])\n",
      "epoch [1/100], loss: -0.026251013739965856\n",
      "epoch [2/100], loss: -0.03346191067248583\n",
      "epoch [3/100], loss: -0.03941867547109723\n",
      "epoch [4/100], loss: -0.04128213133662939\n",
      "epoch [5/100], loss: -0.04315190948545933\n",
      "epoch [6/100], loss: -0.04474202450364828\n",
      "epoch [7/100], loss: -0.0453284727409482\n",
      "epoch [8/100], loss: -0.047336817253381014\n",
      "epoch [9/100], loss: -0.04759768256917596\n",
      "epoch [10/100], loss: -0.04996960377320647\n",
      "epoch [11/100], loss: -0.050934816244989634\n",
      "epoch [12/100], loss: -0.050178788136690855\n",
      "epoch [13/100], loss: -0.051278796046972275\n",
      "epoch [14/100], loss: -0.05177929671481252\n",
      "epoch [15/100], loss: -0.05247871158644557\n",
      "epoch [16/100], loss: -0.053986391983926296\n",
      "epoch [17/100], loss: -0.053381352219730616\n",
      "epoch [18/100], loss: -0.05574972927570343\n",
      "epoch [19/100], loss: -0.05408947262912989\n",
      "epoch [20/100], loss: -0.05499692028388381\n",
      "epoch [21/100], loss: -0.054472790099680424\n",
      "epoch [22/100], loss: -0.056097726337611675\n",
      "epoch [23/100], loss: -0.05781501717865467\n",
      "epoch [24/100], loss: -0.05686563439667225\n",
      "epoch [25/100], loss: -0.059465995989739895\n",
      "epoch [26/100], loss: -0.05807880382053554\n",
      "epoch [27/100], loss: -0.0565616344101727\n",
      "epoch [28/100], loss: -0.0545192020945251\n",
      "epoch [29/100], loss: -0.05496927909553051\n",
      "epoch [30/100], loss: -0.05946353962644935\n",
      "epoch [31/100], loss: -0.05994940036907792\n",
      "epoch [32/100], loss: -0.05627929838374257\n",
      "epoch [33/100], loss: -0.05869923671707511\n",
      "epoch [34/100], loss: -0.057733251713216305\n",
      "epoch [35/100], loss: -0.055949995294213295\n",
      "epoch [36/100], loss: -0.05655957106500864\n",
      "epoch [37/100], loss: -0.057677232660353184\n",
      "epoch [38/100], loss: -0.05657388875260949\n",
      "epoch [39/100], loss: -0.059358702041208744\n",
      "epoch [40/100], loss: -0.054616956040263176\n",
      "epoch [41/100], loss: -0.054779052734375\n",
      "epoch [42/100], loss: -0.05097011988982558\n",
      "epoch [43/100], loss: -0.05290020676329732\n",
      "epoch [44/100], loss: -0.04954199492931366\n",
      "epoch [45/100], loss: -0.053472787607461214\n",
      "epoch [46/100], loss: -0.03585210768505931\n",
      "epoch [47/100], loss: -0.04731364920735359\n",
      "epoch [48/100], loss: -0.05477094929665327\n",
      "epoch [49/100], loss: -0.06042371829971671\n",
      "epoch [50/100], loss: -0.061041613575071096\n",
      "epoch [51/100], loss: -0.06293445359915495\n",
      "epoch [52/100], loss: -0.06300356891006231\n",
      "epoch [53/100], loss: -0.0639752964489162\n",
      "epoch [54/100], loss: -0.06327527482062578\n",
      "epoch [55/100], loss: -0.06316356267780066\n",
      "epoch [56/100], loss: -0.06549780024215579\n",
      "epoch [57/100], loss: -0.06600606348365545\n",
      "epoch [58/100], loss: -0.06916552316397429\n",
      "epoch [59/100], loss: -0.06755107594653964\n",
      "epoch [60/100], loss: -0.06406077556312084\n",
      "epoch [61/100], loss: -0.06571842823177576\n",
      "epoch [62/100], loss: -0.0665973755531013\n",
      "epoch [63/100], loss: -0.06457949709147215\n",
      "epoch [64/100], loss: -0.062312270514667034\n",
      "epoch [65/100], loss: -0.06903127487748861\n",
      "epoch [66/100], loss: -0.07057405589148402\n",
      "epoch [67/100], loss: -0.0671819057315588\n",
      "epoch [68/100], loss: -0.060257647186517715\n",
      "epoch [69/100], loss: -0.06451725121587515\n",
      "epoch [70/100], loss: -0.06612733891233802\n",
      "epoch [71/100], loss: -0.06454178923740983\n",
      "epoch [72/100], loss: -0.06300106877461076\n",
      "epoch [73/100], loss: -0.06237879302352667\n",
      "epoch [74/100], loss: -0.06394070805981755\n",
      "epoch [75/100], loss: -0.06435546092689037\n",
      "epoch [76/100], loss: -0.06936669768765569\n",
      "epoch [77/100], loss: -0.07253743289038539\n",
      "epoch [78/100], loss: -0.06841522315517068\n",
      "epoch [79/100], loss: -0.07106757024303079\n",
      "epoch [80/100], loss: -0.07146696001291275\n",
      "epoch [81/100], loss: -0.07201657630503178\n",
      "epoch [82/100], loss: -0.06945182336494327\n",
      "epoch [83/100], loss: -0.06935458490625024\n",
      "epoch [84/100], loss: -0.06994445528835058\n",
      "epoch [85/100], loss: -0.06776618584990501\n",
      "epoch [86/100], loss: -0.061343743465840816\n",
      "epoch [87/100], loss: -0.06547368783503771\n",
      "epoch [88/100], loss: -0.06815003789961338\n",
      "epoch [89/100], loss: -0.07263029972091317\n",
      "epoch [90/100], loss: -0.07079846551641822\n",
      "epoch [91/100], loss: -0.07455096952617168\n",
      "epoch [92/100], loss: -0.07476010220125318\n",
      "epoch [93/100], loss: -0.07620881125330925\n",
      "epoch [94/100], loss: -0.07533169956877828\n",
      "epoch [95/100], loss: -0.07500338042154908\n",
      "epoch [96/100], loss: -0.06634554266929626\n",
      "epoch [97/100], loss: -0.06755746947601438\n",
      "epoch [98/100], loss: -0.06838867487385869\n",
      "epoch [99/100], loss: -0.06932901497930288\n",
      "epoch [100/100], loss: -0.07533950405195355\n",
      "torch.Size([503, 1]) 503\n",
      "len des rdt 1008\n",
      "len des dates 1008\n",
      "epoch [1/100], loss: -0.021182269556447864\n",
      "epoch [2/100], loss: -0.02974161773454398\n",
      "epoch [3/100], loss: -0.03311282233335078\n",
      "epoch [4/100], loss: -0.03591554658487439\n",
      "epoch [5/100], loss: -0.03814135631546378\n",
      "epoch [6/100], loss: -0.03940865700133145\n",
      "epoch [7/100], loss: -0.03997095231898129\n",
      "epoch [8/100], loss: -0.038516466272994876\n",
      "epoch [9/100], loss: -0.04085880774073303\n",
      "epoch [10/100], loss: -0.0426646894775331\n",
      "epoch [11/100], loss: -0.042658630292862654\n",
      "epoch [12/100], loss: -0.041724052745848894\n",
      "epoch [13/100], loss: -0.042304770555347204\n",
      "epoch [14/100], loss: -0.04356779530644417\n",
      "epoch [15/100], loss: -0.04191677970811725\n",
      "epoch [16/100], loss: -0.040337350917980075\n",
      "epoch [17/100], loss: -0.043026543222367764\n",
      "epoch [18/100], loss: -0.044721679762005806\n",
      "epoch [19/100], loss: -0.04676747415214777\n",
      "epoch [20/100], loss: -0.04667891701683402\n",
      "epoch [21/100], loss: -0.0472159618511796\n",
      "epoch [22/100], loss: -0.048258811701089144\n",
      "epoch [23/100], loss: -0.048294694628566504\n",
      "epoch [24/100], loss: -0.04762533190660179\n",
      "epoch [25/100], loss: -0.0493643197696656\n",
      "epoch [26/100], loss: -0.04970487463288009\n",
      "epoch [27/100], loss: -0.05000711418688297\n",
      "epoch [28/100], loss: -0.04422628507018089\n",
      "epoch [29/100], loss: -0.04577084444463253\n",
      "epoch [30/100], loss: -0.0489084729924798\n",
      "epoch [31/100], loss: -0.05067182588391006\n",
      "epoch [32/100], loss: -0.05118297738954425\n",
      "epoch [33/100], loss: -0.05108837736770511\n",
      "epoch [34/100], loss: -0.05219435831531882\n",
      "epoch [35/100], loss: -0.05051365541294217\n",
      "epoch [36/100], loss: -0.05073507875204086\n",
      "epoch [37/100], loss: -0.04958182433620095\n",
      "epoch [38/100], loss: -0.051539153791964054\n",
      "epoch [39/100], loss: -0.05068092094734311\n",
      "epoch [40/100], loss: -0.04661296959966421\n",
      "epoch [41/100], loss: -0.04954827902838588\n",
      "epoch [42/100], loss: -0.04637762252241373\n",
      "epoch [43/100], loss: -0.050716648576781154\n",
      "epoch [44/100], loss: -0.056029639672487974\n",
      "epoch [45/100], loss: -0.05723095266148448\n",
      "epoch [46/100], loss: -0.05662845214828849\n",
      "epoch [47/100], loss: -0.05662206141278148\n",
      "epoch [48/100], loss: -0.05869013490155339\n",
      "epoch [49/100], loss: -0.058394086081534624\n",
      "epoch [50/100], loss: -0.05266146268695593\n",
      "epoch [51/100], loss: -0.055352591909468174\n",
      "epoch [52/100], loss: -0.05525594996288419\n",
      "epoch [53/100], loss: -0.05083977524191141\n",
      "epoch [54/100], loss: -0.05278591183014214\n",
      "epoch [55/100], loss: -0.05771831423044205\n",
      "epoch [56/100], loss: -0.059306024108082056\n",
      "epoch [57/100], loss: -0.061505133751779795\n",
      "epoch [58/100], loss: -0.062031215988099575\n",
      "epoch [59/100], loss: -0.05794908315874636\n",
      "epoch [60/100], loss: -0.06393541116267443\n",
      "epoch [61/100], loss: -0.06422381708398461\n",
      "epoch [62/100], loss: -0.06198925944045186\n",
      "epoch [63/100], loss: -0.06402725866064429\n",
      "epoch [64/100], loss: -0.060661557130515575\n",
      "epoch [65/100], loss: -0.0559964207932353\n",
      "epoch [66/100], loss: -0.0610083038918674\n",
      "epoch [67/100], loss: -0.06179909175261855\n",
      "epoch [68/100], loss: -0.06054948014207184\n",
      "epoch [69/100], loss: -0.060660190880298615\n",
      "epoch [70/100], loss: -0.05781165463849902\n",
      "epoch [71/100], loss: -0.05687796510756016\n",
      "epoch [72/100], loss: -0.060656371992081404\n",
      "epoch [73/100], loss: -0.06397614069283009\n",
      "epoch [74/100], loss: -0.0674030939117074\n",
      "epoch [75/100], loss: -0.06737297074869275\n",
      "epoch [76/100], loss: -0.06698521273210645\n",
      "epoch [77/100], loss: -0.0621987022459507\n",
      "epoch [78/100], loss: -0.06315901828929782\n",
      "epoch [79/100], loss: -0.06615394400432706\n",
      "epoch [80/100], loss: -0.06640864256769419\n",
      "epoch [81/100], loss: -0.06733904546126723\n",
      "epoch [82/100], loss: -0.06875585578382015\n",
      "epoch [83/100], loss: -0.06693306937813759\n",
      "epoch [84/100], loss: -0.06292594270780683\n",
      "epoch [85/100], loss: -0.0589211699552834\n",
      "epoch [86/100], loss: -0.05839466908946633\n",
      "epoch [87/100], loss: -0.06226565223187208\n",
      "epoch [88/100], loss: -0.0682653896510601\n",
      "epoch [89/100], loss: -0.06414495082572103\n",
      "epoch [90/100], loss: -0.06613589497283101\n",
      "epoch [91/100], loss: -0.06124000018462539\n",
      "epoch [92/100], loss: -0.06212347513064742\n",
      "epoch [93/100], loss: -0.06816633511334658\n",
      "epoch [94/100], loss: -0.06763273663818836\n",
      "epoch [95/100], loss: -0.06750342715531588\n",
      "epoch [96/100], loss: -0.07192551624029875\n",
      "epoch [97/100], loss: -0.07190497918054461\n",
      "epoch [98/100], loss: -0.07040832377970219\n",
      "epoch [99/100], loss: -0.07193780923262239\n",
      "epoch [100/100], loss: -0.0710560823790729\n",
      "torch.Size([502, 1]) 502\n",
      "len des rdt 1510\n",
      "len des dates 1510\n",
      "epoch [1/100], loss: -0.01035165998064258\n",
      "epoch [2/100], loss: -0.019020154199097306\n",
      "epoch [3/100], loss: -0.02157978096511215\n",
      "epoch [4/100], loss: -0.023938020574860275\n",
      "epoch [5/100], loss: -0.028668422950431705\n",
      "epoch [6/100], loss: -0.030661598779261112\n",
      "epoch [7/100], loss: -0.03279553959146142\n",
      "epoch [8/100], loss: -0.03569407691247761\n",
      "epoch [9/100], loss: -0.03630376071669161\n",
      "epoch [10/100], loss: -0.039447461953386664\n",
      "epoch [11/100], loss: -0.040949974209070206\n",
      "epoch [12/100], loss: -0.04240899672731757\n",
      "epoch [13/100], loss: -0.04313350887969136\n",
      "epoch [14/100], loss: -0.04527621390298009\n",
      "epoch [15/100], loss: -0.04368976433761418\n",
      "epoch [16/100], loss: -0.046448177425190806\n",
      "epoch [17/100], loss: -0.046015154803171754\n",
      "epoch [18/100], loss: -0.047199829714372754\n",
      "epoch [19/100], loss: -0.0483768074773252\n",
      "epoch [20/100], loss: -0.05004992405883968\n",
      "epoch [21/100], loss: -0.050830554915592074\n",
      "epoch [22/100], loss: -0.0500529115088284\n",
      "epoch [23/100], loss: -0.05068039521574974\n",
      "epoch [24/100], loss: -0.05090546910651028\n",
      "epoch [25/100], loss: -0.048083605943247676\n",
      "epoch [26/100], loss: -0.048219018150120974\n",
      "epoch [27/100], loss: -0.05093170190230012\n",
      "epoch [28/100], loss: -0.051979930605739355\n",
      "epoch [29/100], loss: -0.052237427327781916\n",
      "epoch [30/100], loss: -0.05067333090119064\n",
      "epoch [31/100], loss: -0.04616733128204942\n",
      "epoch [32/100], loss: -0.0511651027482003\n",
      "epoch [33/100], loss: -0.047805034555494785\n",
      "epoch [34/100], loss: -0.05233126040548086\n",
      "epoch [35/100], loss: -0.054347732570022345\n",
      "epoch [36/100], loss: -0.05896933935582638\n",
      "epoch [37/100], loss: -0.05870115105062723\n",
      "epoch [38/100], loss: -0.05887095211073756\n",
      "epoch [39/100], loss: -0.05711021088063717\n",
      "epoch [40/100], loss: -0.058479068800807\n",
      "epoch [41/100], loss: -0.059305379167199135\n",
      "epoch [42/100], loss: -0.05984021397307515\n",
      "epoch [43/100], loss: -0.06124330637976527\n",
      "epoch [44/100], loss: -0.05923971440643072\n",
      "epoch [45/100], loss: -0.061644223518669605\n",
      "epoch [46/100], loss: -0.059600235195830464\n",
      "epoch [47/100], loss: -0.05588852474465966\n",
      "epoch [48/100], loss: -0.06007467024028301\n",
      "epoch [49/100], loss: -0.06082157976925373\n",
      "epoch [50/100], loss: -0.06366905709728599\n",
      "epoch [51/100], loss: -0.061895730905234814\n",
      "epoch [52/100], loss: -0.058482950553297997\n",
      "epoch [53/100], loss: -0.05780365923419595\n",
      "epoch [54/100], loss: -0.06246769893914461\n",
      "epoch [55/100], loss: -0.06422227015718818\n",
      "epoch [56/100], loss: -0.06726831663399935\n",
      "epoch [57/100], loss: -0.06946784211322665\n",
      "epoch [58/100], loss: -0.06840980425477028\n",
      "epoch [59/100], loss: -0.06828011246398091\n",
      "epoch [60/100], loss: -0.06622991291806102\n",
      "epoch [61/100], loss: -0.06360879773274064\n",
      "epoch [62/100], loss: -0.06637731194496155\n",
      "epoch [63/100], loss: -0.06400441564619541\n",
      "epoch [64/100], loss: -0.06738288421183825\n",
      "epoch [65/100], loss: -0.06871253298595548\n",
      "epoch [66/100], loss: -0.06891873059794307\n",
      "epoch [67/100], loss: -0.0714598442427814\n",
      "epoch [68/100], loss: -0.07152170408517122\n",
      "epoch [69/100], loss: -0.07092995848506689\n",
      "epoch [70/100], loss: -0.06934604281559587\n",
      "epoch [71/100], loss: -0.0707919392734766\n",
      "epoch [72/100], loss: -0.07269516447558999\n",
      "epoch [73/100], loss: -0.07449921080842614\n",
      "epoch [74/100], loss: -0.07469833176583052\n",
      "epoch [75/100], loss: -0.07674004044383764\n",
      "epoch [76/100], loss: -0.0721133565530181\n",
      "epoch [77/100], loss: -0.06717715272679925\n",
      "epoch [78/100], loss: -0.06230184552259743\n",
      "epoch [79/100], loss: -0.06129874335601926\n",
      "epoch [80/100], loss: -0.06405163882300258\n",
      "epoch [81/100], loss: -0.06642946787178516\n",
      "epoch [82/100], loss: -0.07277212338522077\n",
      "epoch [83/100], loss: -0.07237328216433525\n",
      "epoch [84/100], loss: -0.07269642548635602\n",
      "epoch [85/100], loss: -0.07703026244416833\n",
      "epoch [86/100], loss: -0.07297751028090715\n",
      "epoch [87/100], loss: -0.06998705212026834\n",
      "epoch [88/100], loss: -0.07287026010453701\n",
      "epoch [89/100], loss: -0.07013356545940042\n",
      "epoch [90/100], loss: -0.0676320674829185\n",
      "epoch [91/100], loss: -0.07584287086501718\n",
      "epoch [92/100], loss: -0.07842327700927854\n",
      "epoch [93/100], loss: -0.0794982984662056\n",
      "epoch [94/100], loss: -0.07685121009126306\n",
      "epoch [95/100], loss: -0.07597055705264211\n",
      "epoch [96/100], loss: -0.07341292686760426\n",
      "epoch [97/100], loss: -0.0793473832309246\n",
      "epoch [98/100], loss: -0.0799046321772039\n",
      "epoch [99/100], loss: -0.07879449985921383\n",
      "epoch [100/100], loss: -0.0705541642382741\n",
      "torch.Size([503, 1]) 503\n",
      "len des rdt 2013\n",
      "len des dates 2013\n",
      "epoch [1/100], loss: -0.022348333732225\n",
      "epoch [2/100], loss: -0.03262065682793036\n",
      "epoch [3/100], loss: -0.035993904108181596\n",
      "epoch [4/100], loss: -0.04197907540947199\n",
      "epoch [5/100], loss: -0.04727939097210765\n",
      "epoch [6/100], loss: -0.05015336815267801\n",
      "epoch [7/100], loss: -0.05144077306613326\n",
      "epoch [8/100], loss: -0.05366173526272178\n",
      "epoch [9/100], loss: -0.05657383194193244\n",
      "epoch [10/100], loss: -0.05229073669761419\n",
      "epoch [11/100], loss: -0.05090408120304346\n",
      "epoch [12/100], loss: -0.05581394815817475\n",
      "epoch [13/100], loss: -0.059119215700775385\n",
      "epoch [14/100], loss: -0.06017962982878089\n",
      "epoch [15/100], loss: -0.059956937097013\n",
      "epoch [16/100], loss: -0.057108425069600344\n",
      "epoch [17/100], loss: -0.0599418468773365\n",
      "epoch [18/100], loss: -0.06360947899520397\n",
      "epoch [19/100], loss: -0.06412676116451621\n",
      "epoch [20/100], loss: -0.06406811624765396\n",
      "epoch [21/100], loss: -0.06463288608938456\n",
      "epoch [22/100], loss: -0.06597703089937568\n",
      "epoch [23/100], loss: -0.06400926550850272\n",
      "epoch [24/100], loss: -0.054888558108359575\n",
      "epoch [25/100], loss: -0.06368240201845765\n",
      "epoch [26/100], loss: -0.06658716313540936\n",
      "epoch [27/100], loss: -0.06767303543165326\n",
      "epoch [28/100], loss: -0.06888586934655905\n",
      "epoch [29/100], loss: -0.07001356687396765\n",
      "epoch [30/100], loss: -0.06809647474437952\n",
      "epoch [31/100], loss: -0.0642660278826952\n",
      "epoch [32/100], loss: -0.060703723691403866\n",
      "epoch [33/100], loss: -0.06281020818278193\n",
      "epoch [34/100], loss: -0.06506677391007543\n",
      "epoch [35/100], loss: -0.06587705574929714\n",
      "epoch [36/100], loss: -0.07022061804309487\n",
      "epoch [37/100], loss: -0.06958505930379033\n",
      "epoch [38/100], loss: -0.07215324463322759\n",
      "epoch [39/100], loss: -0.07058613561093807\n",
      "epoch [40/100], loss: -0.07168207643553615\n",
      "epoch [41/100], loss: -0.07240941561758518\n",
      "epoch [42/100], loss: -0.06984685081988573\n",
      "epoch [43/100], loss: -0.059954635333269835\n",
      "epoch [44/100], loss: -0.059719300363212824\n",
      "epoch [45/100], loss: -0.06607662746682763\n",
      "epoch [46/100], loss: -0.07057239627465606\n",
      "epoch [47/100], loss: -0.07370829302817583\n",
      "epoch [48/100], loss: -0.07265531225129962\n",
      "epoch [49/100], loss: -0.061162910889834166\n",
      "epoch [50/100], loss: -0.06677694944664836\n",
      "epoch [51/100], loss: -0.0646831076592207\n",
      "epoch [52/100], loss: -0.06570845283567905\n",
      "epoch [53/100], loss: -0.069992505479604\n",
      "epoch [54/100], loss: -0.07346079545095563\n",
      "epoch [55/100], loss: -0.07523980364203453\n",
      "epoch [56/100], loss: -0.07596747996285558\n",
      "epoch [57/100], loss: -0.07739092642441392\n",
      "epoch [58/100], loss: -0.07672079652547836\n",
      "epoch [59/100], loss: -0.0773032265715301\n",
      "epoch [60/100], loss: -0.07776500284671783\n",
      "epoch [61/100], loss: -0.0791473463177681\n",
      "epoch [62/100], loss: -0.0794786917977035\n",
      "epoch [63/100], loss: -0.07658275123685598\n",
      "epoch [64/100], loss: -0.07391214231029153\n",
      "epoch [65/100], loss: -0.07221724605187774\n",
      "epoch [66/100], loss: -0.07017591083422303\n",
      "epoch [67/100], loss: -0.06932650320231915\n",
      "epoch [68/100], loss: -0.0761065986007452\n",
      "epoch [69/100], loss: -0.07699497789144516\n",
      "epoch [70/100], loss: -0.07868316164240241\n",
      "epoch [71/100], loss: -0.07776945363730192\n",
      "epoch [72/100], loss: -0.07851647352799773\n",
      "epoch [73/100], loss: -0.0809705974534154\n",
      "epoch [74/100], loss: -0.07715736189857125\n",
      "epoch [75/100], loss: -0.07192008150741458\n",
      "epoch [76/100], loss: -0.07454094896093011\n",
      "epoch [77/100], loss: -0.06244516558945179\n",
      "epoch [78/100], loss: -0.07032049866393209\n",
      "epoch [79/100], loss: -0.06523826951161027\n",
      "epoch [80/100], loss: -0.07459103129804134\n",
      "epoch [81/100], loss: -0.07115865172818303\n",
      "epoch [82/100], loss: -0.08098368532955647\n",
      "epoch [83/100], loss: -0.08067377470433712\n",
      "epoch [84/100], loss: -0.08300539571791887\n",
      "epoch [85/100], loss: -0.07956174155697227\n",
      "epoch [86/100], loss: -0.07652372727170587\n",
      "epoch [87/100], loss: -0.07738584699109197\n",
      "epoch [88/100], loss: -0.08041191333904862\n",
      "epoch [89/100], loss: -0.08110175793990493\n",
      "epoch [90/100], loss: -0.08572452422231436\n",
      "epoch [91/100], loss: -0.08721907716244459\n",
      "epoch [92/100], loss: -0.08736989367753267\n",
      "epoch [93/100], loss: -0.08660329412668943\n",
      "epoch [94/100], loss: -0.08430179487913847\n",
      "epoch [95/100], loss: -0.07799471449106932\n",
      "epoch [96/100], loss: -0.07880752813071012\n",
      "epoch [97/100], loss: -0.0822280622087419\n",
      "epoch [98/100], loss: -0.07822019327431917\n",
      "epoch [99/100], loss: -0.07772151147946715\n",
      "epoch [100/100], loss: -0.08025534125044942\n",
      "torch.Size([503, 1]) 503\n",
      "len des rdt 2516\n",
      "len des dates 2516\n",
      "epoch [1/100], loss: -0.02750065620057285\n",
      "epoch [2/100], loss: -0.04031416727229953\n",
      "epoch [3/100], loss: -0.03916765667963773\n",
      "epoch [4/100], loss: -0.04529993166215718\n",
      "epoch [5/100], loss: -0.0453837183304131\n",
      "epoch [6/100], loss: -0.04533582483418286\n",
      "epoch [7/100], loss: -0.04713986022397876\n",
      "epoch [8/100], loss: -0.0487400246784091\n",
      "epoch [9/100], loss: -0.0501175201497972\n",
      "epoch [10/100], loss: -0.051313777919858694\n",
      "epoch [11/100], loss: -0.05271572154015303\n",
      "epoch [12/100], loss: -0.05417812895029783\n",
      "epoch [13/100], loss: -0.05532159539870918\n",
      "epoch [14/100], loss: -0.05571604450233281\n",
      "epoch [15/100], loss: -0.05261632613837719\n",
      "epoch [16/100], loss: -0.05530473217368126\n",
      "epoch [17/100], loss: -0.05449529201723635\n",
      "epoch [18/100], loss: -0.05369528895244002\n",
      "epoch [19/100], loss: -0.054417232517153025\n",
      "epoch [20/100], loss: -0.0566372568719089\n",
      "epoch [21/100], loss: -0.054743292508646846\n",
      "epoch [22/100], loss: -0.05834555020555854\n",
      "epoch [23/100], loss: -0.05941824382171035\n",
      "epoch [24/100], loss: -0.0612255847081542\n",
      "epoch [25/100], loss: -0.060619876720011234\n",
      "epoch [26/100], loss: -0.06177054578438401\n",
      "epoch [27/100], loss: -0.06140554789453745\n",
      "epoch [28/100], loss: -0.06279453635215759\n",
      "epoch [29/100], loss: -0.06601969199255109\n",
      "epoch [30/100], loss: -0.06702499417588115\n",
      "epoch [31/100], loss: -0.06745955208316445\n",
      "epoch [32/100], loss: -0.06879838230088353\n",
      "epoch [33/100], loss: -0.06809077691286802\n",
      "epoch [34/100], loss: -0.06985432421788573\n",
      "epoch [35/100], loss: -0.06732082180678844\n",
      "epoch [36/100], loss: -0.06770998053252697\n",
      "epoch [37/100], loss: -0.06814078288152814\n",
      "epoch [38/100], loss: -0.06576869916170835\n",
      "epoch [39/100], loss: -0.0688351378776133\n",
      "epoch [40/100], loss: -0.06900162762030959\n",
      "epoch [41/100], loss: -0.07043995894491673\n",
      "epoch [42/100], loss: -0.07064345059916377\n",
      "epoch [43/100], loss: -0.07203048234805465\n",
      "epoch [44/100], loss: -0.07449577143415809\n",
      "epoch [45/100], loss: -0.07304614223539829\n",
      "epoch [46/100], loss: -0.07449005683884025\n",
      "epoch [47/100], loss: -0.07177466619759798\n",
      "epoch [48/100], loss: -0.07129696849733591\n",
      "epoch [49/100], loss: -0.06857495056465268\n",
      "epoch [50/100], loss: -0.07256768504157662\n",
      "epoch [51/100], loss: -0.07482083514332771\n",
      "epoch [52/100], loss: -0.07845482369884849\n",
      "epoch [53/100], loss: -0.07795505877584219\n",
      "epoch [54/100], loss: -0.07994444528594613\n",
      "epoch [55/100], loss: -0.07742164842784405\n",
      "epoch [56/100], loss: -0.07816496025770903\n",
      "epoch [57/100], loss: -0.07822834886610508\n",
      "epoch [58/100], loss: -0.08122284151613712\n",
      "epoch [59/100], loss: -0.0836853887885809\n",
      "epoch [60/100], loss: -0.08283360255882144\n",
      "epoch [61/100], loss: -0.08171679126098752\n",
      "epoch [62/100], loss: -0.07901733741164207\n",
      "epoch [63/100], loss: -0.08086707536131144\n",
      "epoch [64/100], loss: -0.0779991359449923\n",
      "epoch [65/100], loss: -0.07947048358619213\n",
      "epoch [66/100], loss: -0.0743797398172319\n",
      "epoch [67/100], loss: -0.08023462444543839\n",
      "epoch [68/100], loss: -0.0810027364641428\n",
      "epoch [69/100], loss: -0.08508277591317892\n",
      "epoch [70/100], loss: -0.08007891988381743\n",
      "epoch [71/100], loss: -0.08135525323450565\n",
      "epoch [72/100], loss: -0.08010393427684903\n",
      "epoch [73/100], loss: -0.08225815650075674\n",
      "epoch [74/100], loss: -0.07535322057083249\n",
      "epoch [75/100], loss: -0.08026823261752725\n",
      "epoch [76/100], loss: -0.07900399249047041\n",
      "epoch [77/100], loss: -0.08522216696292162\n",
      "epoch [78/100], loss: -0.08704891428351402\n",
      "epoch [79/100], loss: -0.08828411996364594\n",
      "epoch [80/100], loss: -0.09021354280412197\n",
      "epoch [81/100], loss: -0.08923746226355433\n",
      "epoch [82/100], loss: -0.08566444274038076\n",
      "epoch [83/100], loss: -0.08594745025038719\n",
      "epoch [84/100], loss: -0.0882540037855506\n",
      "epoch [85/100], loss: -0.09040553215891123\n",
      "epoch [86/100], loss: -0.08945202361792326\n",
      "epoch [87/100], loss: -0.08689425932243466\n",
      "epoch [88/100], loss: -0.08390187285840511\n",
      "epoch [89/100], loss: -0.08761002449318767\n",
      "epoch [90/100], loss: -0.08542665420100093\n",
      "epoch [91/100], loss: -0.08680381439626217\n",
      "epoch [92/100], loss: -0.09136288706213236\n",
      "epoch [93/100], loss: -0.09750874526798725\n",
      "epoch [94/100], loss: -0.09532170556485653\n",
      "epoch [95/100], loss: -0.09585914667695761\n",
      "epoch [96/100], loss: -0.09192465711385012\n",
      "epoch [97/100], loss: -0.08819038514047861\n",
      "epoch [98/100], loss: -0.08886595722287893\n",
      "epoch [99/100], loss: -0.08042401866987348\n",
      "epoch [100/100], loss: -0.09052630793303251\n",
      "torch.Size([179, 1]) 179\n",
      "len des rdt 2695\n",
      "len des dates 2695\n"
     ]
    }
   ],
   "source": [
    "input_size = 8\n",
    "hidden_size = 64\n",
    "output_size = 4\n",
    "batch_size = 64\n",
    "\n",
    "rdt_all = torch.empty(0,0)\n",
    "alloc_all = []\n",
    "idx_invest = []\n",
    "dates_invest = []\n",
    "\n",
    "# first training goes from 2006 to 2010\n",
    "first_date = dates_per_feature[0]\n",
    "end_date_1st_training = first_date + datetime.timedelta(days=365*4)\n",
    "# ?????\n",
    "idx_start, idx_end = get_idx_dates(first_date, end_date_1st_training)\n",
    "\n",
    "model = PortfolioRNN(input_size=input_size, hidden_size=hidden_size, num_layers = 2,output_size=output_size)\n",
    "\n",
    "\n",
    "model = training(x_batches = x_batches[idx_start:idx_end, :, :],\n",
    "                      y_batches= y_batches[idx_start:idx_end, :, :],\n",
    "                      model=model,\n",
    "                      batch_size=64)\n",
    "\n",
    "# un an c pas 365 jours mais 252 jours de trading\n",
    "date_end_invest = end_date_1st_training + datetime.timedelta(days=365*2)\n",
    "idx_start_invest, idx_end_invest = get_idx_dates(end_date_1st_training, date_end_invest)\n",
    "\n",
    "print(f\"Idx start: {idx_start_invest}, Idx end: {idx_end_invest}\")\n",
    "print(dates_per_feature[idx_start_invest], dates_per_feature[idx_end_invest])\n",
    "\n",
    "idx_invest.append((idx_start_invest, idx_end_invest))\n",
    "dates_invest.append((end_date_1st_training, date_end_invest))\n",
    "\n",
    "x_investing = x_batches[idx_start_invest:idx_end_invest, :, :]\n",
    "y_investing = y_batches[idx_start_invest:idx_end_invest, :, :]\n",
    "y_real = y[idx_start_invest:idx_end_invest, :, :]\n",
    "\n",
    "#faut commencer de x_batches - size_window, car ça donne les allcations pour le dernier jour du batch/window out[:,-1,:]\n",
    "first_rdt, first_alloc = investing(x_batches= x_investing, y=y_real, model=model)\n",
    "\n",
    "print(first_rdt.shape, len(dates_per_feature[idx_start_invest:idx_end_invest]))\n",
    "\n",
    "print(f\"first_rdt shape: {first_rdt.shape}, rdt_all shape before concat: {rdt_all.shape}\")\n",
    "\n",
    "\n",
    "rdt_all = first_rdt\n",
    "alloc_all = first_alloc\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    model = training(x_batches = x_batches[idx_start_invest:idx_end_invest, :, :],\n",
    "                      y_batches= y_batches[idx_start_invest:idx_end_invest, :, :],\n",
    "                      model=model,\n",
    "                      batch_size=64)\n",
    "    \n",
    "    date_end_invest = date_end_invest + datetime.timedelta(days=365*2)\n",
    "    end_date_1st_training = end_date_1st_training + datetime.timedelta(days=365*2)\n",
    "    idx_start_invest, idx_end_invest = get_idx_dates(end_date_1st_training, date_end_invest)\n",
    "\n",
    "    x_investing = x_batches[idx_start_invest:idx_end_invest, :, :]\n",
    "    y_investing = y_batches[idx_start_invest:idx_end_invest, :, :]\n",
    "    y_real = y[idx_start_invest:idx_end_invest, :, :]\n",
    "\n",
    "\n",
    "    first_rdt, first_alloc = investing(x_batches= x_investing, y=y_real, model=model)\n",
    "\n",
    "    print(first_rdt.shape, len(dates_per_feature[idx_start_invest:idx_end_invest]))\n",
    "\n",
    "\n",
    "    rdt_all = torch.cat([rdt_all, first_rdt], dim=0)\n",
    "    print('len des rdt', rdt_all.shape[0])\n",
    "    alloc_all = torch.cat([alloc_all, first_alloc], dim=0)\n",
    "    idx_invest.append((idx_start_invest, idx_end_invest))\n",
    "    print('len des dates', len(dates_per_feature[idx_invest[0][0]:idx_invest[-1][1]]))\n",
    "    dates_invest.append((end_date_1st_training, date_end_invest))\n",
    "\n",
    "\n",
    "alloc_all = alloc_all.numpy()\n",
    "alloc_all = alloc_all.reshape(alloc_all.shape[0], alloc_all.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_25860\\2977585624.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na['Date'] = pd.to_datetime(data_na['Date'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Return</th>\n",
       "      <th>Alloc_AGG</th>\n",
       "      <th>Alloc_DBC</th>\n",
       "      <th>Alloc_VTI</th>\n",
       "      <th>Alloc_^VIX</th>\n",
       "      <th>AGG_y</th>\n",
       "      <th>DBC_y</th>\n",
       "      <th>VTI_y</th>\n",
       "      <th>^VIX_y</th>\n",
       "      <th>Final_Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-04-19</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.386544</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.545118</td>\n",
       "      <td>0.062192</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.007936</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>-0.092849</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-04-20</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.560603</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>0.390092</td>\n",
       "      <td>0.043487</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.004973</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>0.037508</td>\n",
       "      <td>0.002311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-04-21</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.543324</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.368171</td>\n",
       "      <td>0.078537</td>\n",
       "      <td>-0.002008</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.001444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-04-22</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>0.286030</td>\n",
       "      <td>0.009746</td>\n",
       "      <td>0.588584</td>\n",
       "      <td>0.115641</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>0.006582</td>\n",
       "      <td>0.006769</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.004745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-04-23</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.206501</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.707580</td>\n",
       "      <td>0.078922</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.004904</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.051143</td>\n",
       "      <td>0.001490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>2020-12-22</td>\n",
       "      <td>-0.000683</td>\n",
       "      <td>0.996060</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>0.012517</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>-0.037969</td>\n",
       "      <td>-0.000683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.992632</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>-0.076362</td>\n",
       "      <td>0.000775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.979257</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>-0.006164</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.988904</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>-0.004175</td>\n",
       "      <td>0.063594</td>\n",
       "      <td>0.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.990088</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.008971</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>-0.013432</td>\n",
       "      <td>0.000603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2695 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date    Return  Alloc_AGG  Alloc_DBC  Alloc_VTI  Alloc_^VIX  \\\n",
       "0    2010-04-19  0.000069   0.386544   0.006146   0.545118    0.062192   \n",
       "1    2010-04-20  0.002311   0.560603   0.005818   0.390092    0.043487   \n",
       "2    2010-04-21  0.001444   0.543324   0.009969   0.368171    0.078537   \n",
       "3    2010-04-22  0.004745   0.286030   0.009746   0.588584    0.115641   \n",
       "4    2010-04-23  0.001490   0.206501   0.006997   0.707580    0.078922   \n",
       "...         ...       ...        ...        ...        ...         ...   \n",
       "2690 2020-12-22 -0.000683   0.996060   0.000088   0.003473    0.000380   \n",
       "2691 2020-12-23  0.000775   0.992632   0.000129   0.005147    0.002093   \n",
       "2692 2020-12-24  0.000299   0.979257   0.000121   0.018896    0.001727   \n",
       "2693 2020-12-28  0.000286   0.988904   0.000417   0.008284    0.002395   \n",
       "2694 2020-12-29  0.000603   0.990088   0.000245   0.008971    0.000697   \n",
       "\n",
       "         AGG_y     DBC_y     VTI_y    ^VIX_y  Final_Return  \n",
       "0     0.001632  0.007936  0.009474 -0.092849      0.000069  \n",
       "1     0.001724  0.004973 -0.000809  0.037508      0.002311  \n",
       "2    -0.002008  0.002474  0.004858  0.009191      0.001444  \n",
       "3    -0.001246  0.006582  0.006769  0.009108      0.004745  \n",
       "4    -0.000096 -0.004904 -0.003522  0.051143      0.001490  \n",
       "...        ...       ...       ...       ...           ...  \n",
       "2690 -0.000678  0.012517  0.001710 -0.037969     -0.000683  \n",
       "2691  0.000933  0.002747  0.001673 -0.076362      0.000775  \n",
       "2692  0.000170 -0.006164  0.006327  0.007896      0.000299  \n",
       "2693  0.000169  0.002757 -0.004175  0.063594      0.000286  \n",
       "2694  0.000593  0.006186  0.002691 -0.013432      0.000603  \n",
       "\n",
       "[2695 rows x 11 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataframe_result = pd.DataFrame({\n",
    "    'Date': pd.Series(dates_per_feature[idx_invest[0][0]:idx_invest[-1][1]]),\n",
    "    'Return': pd.Series(rdt_all.numpy().flatten()),\n",
    "    'Alloc_AGG': pd.Series(alloc_all[:, 0]),\n",
    "    'Alloc_DBC': pd.Series(alloc_all[:, 1]),\n",
    "    'Alloc_VTI': pd.Series(alloc_all[:, 2]),\n",
    "    'Alloc_^VIX': pd.Series(alloc_all[:, 3])\n",
    "})\n",
    "\n",
    "dataframe_result['Date'] = pd.to_datetime(dataframe_result['Date'])\n",
    "data_na['Date'] = pd.to_datetime(data_na['Date'])\n",
    "dataframe_result = pd.merge(dataframe_result, data_na[['Date', 'AGG_y', 'DBC_y', 'VTI_y', '^VIX_y']], on='Date', how='left')\n",
    "\n",
    "dataframe_result['Final_Return'] = (\n",
    "    dataframe_result['Alloc_AGG'] * dataframe_result['AGG_y'] +\n",
    "    dataframe_result['Alloc_DBC'] * dataframe_result['DBC_y'] +\n",
    "    dataframe_result['Alloc_VTI'] * dataframe_result['VTI_y'] +\n",
    "    dataframe_result['Alloc_^VIX'] * dataframe_result['^VIX_y']\n",
    ")\n",
    "\n",
    "dataframe_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annualized mean return: 0.2298\n",
      "annualized std: 0.1574\n",
      "annualized dd: 0.0952\n",
      "Sharpe Ratio: 1.4604\n",
      "Sortino Ratio: 2.4155\n"
     ]
    }
   ],
   "source": [
    "trading_days_per_year = 252\n",
    "\n",
    "mean_daily_return = dataframe_result['Final_Return'].mean()\n",
    "annualized_return = (1+mean_daily_return) ** trading_days_per_year -1\n",
    "\n",
    "std_daily_return = dataframe_result['Final_Return'].std()\n",
    "std_dd_return = dataframe_result[dataframe_result['Final_Return'] < 0]['Final_Return'].std()\n",
    "\n",
    "annualized_volatility = std_daily_return * np.sqrt(trading_days_per_year)\n",
    "annualized_volatility_dd = std_dd_return * np.sqrt(trading_days_per_year)\n",
    "\n",
    "sharpe_ratio = annualized_return / annualized_volatility\n",
    "sortino_ratio = annualized_return / annualized_volatility_dd\n",
    "\n",
    "print(f\"annualized mean return: {annualized_return:.4f}\")\n",
    "print(f\"annualized std: {annualized_volatility:.4f}\")\n",
    "print(f\"annualized dd: {annualized_volatility_dd:.4f}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
    "print(f\"Sortino Ratio: {sortino_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 64\n",
    "\n",
    "x_batches_tensor = torch.tensor(x_batches, dtype=torch.float32)\n",
    "y_batches_tensor = torch.tensor(y_batches, dtype=torch.float32)\n",
    "\n",
    "num_batches = x_batches_tensor.shape[0]\n",
    "\n",
    "input_size = 8\n",
    "hidden_size = 64\n",
    "output_size = 4\n",
    "model_rnn = PortfolioRNN(input_size=input_size, hidden_size=hidden_size, num_layers=2, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "x_mini_batch shape: torch.Size([64, 50, 8])\n",
      "y_mini_batch shape: torch.Size([64, 50, 0])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_mini_batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_mini_batch\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mini_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mini_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 17\u001b[0m, in \u001b[0;36mPortfolioRNN.forward\u001b[1;34m(self, X, r)\u001b[0m\n\u001b[0;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n\u001b[0;32m     16\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m portfolio_returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# portfolio_returns -> (batch_size)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m sharpe \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(portfolio_returns, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mstd(portfolio_returns, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_rnn.train()\n",
    "    loss_epoch = 0\n",
    "    \n",
    "    for i in range(0, num_batches - mini_batch_size + 1, mini_batch_size):\n",
    "        x_mini_batch = x_batches_tensor[i:i+mini_batch_size]\n",
    "        y_mini_batch = y_batches_tensor[i:i+mini_batch_size]\n",
    "        \n",
    "        # Check the shapes\n",
    "        print(f\"Batch {i // mini_batch_size + 1}:\")\n",
    "        print(f\"x_mini_batch shape: {x_mini_batch.shape}\")\n",
    "        print(f\"y_mini_batch shape: {y_mini_batch.shape}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model_rnn(x_mini_batch, y_mini_batch)\n",
    "        loss_epoch += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"epoch [{epoch+1}/{num_epochs}], loss: {(loss_epoch/mini_batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PortfolioRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Step 100/600, Loss = 1.7561472654342651\n",
      "Epoch 1/2, Step 200/600, Loss = 1.1913491487503052\n",
      "Epoch 1/2, Step 300/600, Loss = 0.4377589523792267\n",
      "Epoch 1/2, Step 400/600, Loss = 1.1186847686767578\n",
      "Epoch 1/2, Step 500/600, Loss = 1.1297461986541748\n",
      "Epoch 1/2, Step 600/600, Loss = 1.624846339225769\n",
      "Epoch 2/2, Step 100/600, Loss = 0.710981011390686\n",
      "Epoch 2/2, Step 200/600, Loss = 0.7087716460227966\n",
      "Epoch 2/2, Step 300/600, Loss = 1.1653952598571777\n",
      "Epoch 2/2, Step 400/600, Loss = 1.0352438688278198\n",
      "Epoch 2/2, Step 500/600, Loss = 0.6385011076927185\n",
      "Epoch 2/2, Step 600/600, Loss = 0.748671293258667\n",
      "Epoch 3/2, Step 100/600, Loss = 1.1222331523895264\n",
      "Epoch 3/2, Step 200/600, Loss = 0.8393617272377014\n",
      "Epoch 3/2, Step 300/600, Loss = 0.7829921245574951\n",
      "Epoch 3/2, Step 400/600, Loss = 0.897032618522644\n",
      "Epoch 3/2, Step 500/600, Loss = 0.9496373534202576\n",
      "Epoch 3/2, Step 600/600, Loss = 0.8489352464675903\n",
      "Epoch 4/2, Step 100/600, Loss = 0.5647134780883789\n",
      "Epoch 4/2, Step 200/600, Loss = 1.4857374429702759\n",
      "Epoch 4/2, Step 300/600, Loss = 2.0306169986724854\n",
      "Epoch 4/2, Step 400/600, Loss = 0.7122843861579895\n",
      "Epoch 4/2, Step 500/600, Loss = 0.6505269408226013\n",
      "Epoch 4/2, Step 600/600, Loss = 1.3644652366638184\n",
      "Epoch 5/2, Step 100/600, Loss = 0.7575669884681702\n",
      "Epoch 5/2, Step 200/600, Loss = 0.7221808433532715\n",
      "Epoch 5/2, Step 300/600, Loss = 1.284300684928894\n",
      "Epoch 5/2, Step 400/600, Loss = 0.6143010258674622\n",
      "Epoch 5/2, Step 500/600, Loss = 0.6969509124755859\n",
      "Epoch 5/2, Step 600/600, Loss = 1.0164573192596436\n",
      "Epoch 6/2, Step 100/600, Loss = 0.7617648243904114\n",
      "Epoch 6/2, Step 200/600, Loss = 0.7624671459197998\n",
      "Epoch 6/2, Step 300/600, Loss = 0.9380752444267273\n",
      "Epoch 6/2, Step 400/600, Loss = 0.5861667394638062\n",
      "Epoch 6/2, Step 500/600, Loss = 0.5562633872032166\n",
      "Epoch 6/2, Step 600/600, Loss = 1.563233494758606\n",
      "Epoch 7/2, Step 100/600, Loss = 0.8195065259933472\n",
      "Epoch 7/2, Step 200/600, Loss = 0.7155972123146057\n",
      "Epoch 7/2, Step 300/600, Loss = 0.5961833596229553\n",
      "Epoch 7/2, Step 400/600, Loss = 0.6594519019126892\n",
      "Epoch 7/2, Step 500/600, Loss = 0.605135440826416\n",
      "Epoch 7/2, Step 600/600, Loss = 0.5903542041778564\n",
      "Epoch 8/2, Step 100/600, Loss = 0.7346264123916626\n",
      "Epoch 8/2, Step 200/600, Loss = 0.5904850363731384\n",
      "Epoch 8/2, Step 300/600, Loss = 0.7969841957092285\n",
      "Epoch 8/2, Step 400/600, Loss = 0.8449658155441284\n",
      "Epoch 8/2, Step 500/600, Loss = 0.42836788296699524\n",
      "Epoch 8/2, Step 600/600, Loss = 0.5024669170379639\n",
      "Epoch 9/2, Step 100/600, Loss = 0.5732094049453735\n",
      "Epoch 9/2, Step 200/600, Loss = 1.3130611181259155\n",
      "Epoch 9/2, Step 300/600, Loss = 0.5626366138458252\n",
      "Epoch 9/2, Step 400/600, Loss = 0.650200366973877\n",
      "Epoch 9/2, Step 500/600, Loss = 0.8351550102233887\n",
      "Epoch 9/2, Step 600/600, Loss = 0.6711447238922119\n",
      "Epoch 10/2, Step 100/600, Loss = 0.5907195806503296\n",
      "Epoch 10/2, Step 200/600, Loss = 0.6687739491462708\n",
      "Epoch 10/2, Step 300/600, Loss = 1.0175180435180664\n",
      "Epoch 10/2, Step 400/600, Loss = 0.8386268019676208\n",
      "Epoch 10/2, Step 500/600, Loss = 1.9594967365264893\n",
      "Epoch 10/2, Step 600/600, Loss = 1.8622478246688843\n",
      "Epoch 11/2, Step 100/600, Loss = 0.739504873752594\n",
      "Epoch 11/2, Step 200/600, Loss = 1.1423866748809814\n",
      "Epoch 11/2, Step 300/600, Loss = 1.3713940382003784\n",
      "Epoch 11/2, Step 400/600, Loss = 0.4809513986110687\n",
      "Epoch 11/2, Step 500/600, Loss = 1.4489398002624512\n",
      "Epoch 11/2, Step 600/600, Loss = 0.7419864535331726\n",
      "Epoch 12/2, Step 100/600, Loss = 0.9915297031402588\n",
      "Epoch 12/2, Step 200/600, Loss = 0.6670812964439392\n",
      "Epoch 12/2, Step 300/600, Loss = 0.7344872951507568\n",
      "Epoch 12/2, Step 400/600, Loss = 0.8222957849502563\n",
      "Epoch 12/2, Step 500/600, Loss = 0.6569140553474426\n",
      "Epoch 12/2, Step 600/600, Loss = 0.7752962708473206\n",
      "Epoch 13/2, Step 100/600, Loss = 0.9757022261619568\n",
      "Epoch 13/2, Step 200/600, Loss = 0.5459726452827454\n",
      "Epoch 13/2, Step 300/600, Loss = 0.930605411529541\n",
      "Epoch 13/2, Step 400/600, Loss = 0.49197474122047424\n",
      "Epoch 13/2, Step 500/600, Loss = 0.8342377543449402\n",
      "Epoch 13/2, Step 600/600, Loss = 0.8709094524383545\n",
      "Epoch 14/2, Step 100/600, Loss = 0.5298974514007568\n",
      "Epoch 14/2, Step 200/600, Loss = 0.9134870767593384\n",
      "Epoch 14/2, Step 300/600, Loss = 0.7345489263534546\n",
      "Epoch 14/2, Step 400/600, Loss = 0.8496286869049072\n",
      "Epoch 14/2, Step 500/600, Loss = 1.4958851337432861\n",
      "Epoch 14/2, Step 600/600, Loss = 0.5591602921485901\n",
      "Epoch 15/2, Step 100/600, Loss = 0.6103951334953308\n",
      "Epoch 15/2, Step 200/600, Loss = 1.1229645013809204\n",
      "Epoch 15/2, Step 300/600, Loss = 1.2025777101516724\n",
      "Epoch 15/2, Step 400/600, Loss = 1.0284849405288696\n",
      "Epoch 15/2, Step 500/600, Loss = 0.6610007286071777\n",
      "Epoch 15/2, Step 600/600, Loss = 0.768892765045166\n",
      "Epoch 16/2, Step 100/600, Loss = 0.8539714217185974\n",
      "Epoch 16/2, Step 200/600, Loss = 0.8649458289146423\n",
      "Epoch 16/2, Step 300/600, Loss = 0.7634660601615906\n",
      "Epoch 16/2, Step 400/600, Loss = 0.8506960868835449\n",
      "Epoch 16/2, Step 500/600, Loss = 1.6302151679992676\n",
      "Epoch 16/2, Step 600/600, Loss = 2.000943183898926\n",
      "Epoch 17/2, Step 100/600, Loss = 0.6317209601402283\n",
      "Epoch 17/2, Step 200/600, Loss = 0.7260487079620361\n",
      "Epoch 17/2, Step 300/600, Loss = 0.6685819029808044\n",
      "Epoch 17/2, Step 400/600, Loss = 0.7773380279541016\n",
      "Epoch 17/2, Step 500/600, Loss = 0.6713677048683167\n",
      "Epoch 17/2, Step 600/600, Loss = 0.6607370972633362\n",
      "Epoch 18/2, Step 100/600, Loss = 0.7722377181053162\n",
      "Epoch 18/2, Step 200/600, Loss = 1.1283657550811768\n",
      "Epoch 18/2, Step 300/600, Loss = 0.5733255743980408\n",
      "Epoch 18/2, Step 400/600, Loss = 0.6588343977928162\n",
      "Epoch 18/2, Step 500/600, Loss = 0.6263440251350403\n",
      "Epoch 18/2, Step 600/600, Loss = 1.1657681465148926\n",
      "Epoch 19/2, Step 100/600, Loss = 0.7161742448806763\n",
      "Epoch 19/2, Step 200/600, Loss = 0.6132200360298157\n",
      "Epoch 19/2, Step 300/600, Loss = 0.5802550315856934\n",
      "Epoch 19/2, Step 400/600, Loss = 0.9011296033859253\n",
      "Epoch 19/2, Step 500/600, Loss = 0.49279919266700745\n",
      "Epoch 19/2, Step 600/600, Loss = 0.9239047765731812\n",
      "Epoch 20/2, Step 100/600, Loss = 1.531754493713379\n",
      "Epoch 20/2, Step 200/600, Loss = 1.2620530128479004\n",
      "Epoch 20/2, Step 300/600, Loss = 0.6601241230964661\n",
      "Epoch 20/2, Step 400/600, Loss = 0.8971971273422241\n",
      "Epoch 20/2, Step 500/600, Loss = 0.6400142908096313\n",
      "Epoch 20/2, Step 600/600, Loss = 0.5631607174873352\n",
      "Epoch 21/2, Step 100/600, Loss = 0.39439907670021057\n",
      "Epoch 21/2, Step 200/600, Loss = 0.6170299649238586\n",
      "Epoch 21/2, Step 300/600, Loss = 0.6768676042556763\n",
      "Epoch 21/2, Step 400/600, Loss = 0.5733846426010132\n",
      "Epoch 21/2, Step 500/600, Loss = 0.8819712996482849\n",
      "Epoch 21/2, Step 600/600, Loss = 3.2813897132873535\n",
      "Epoch 22/2, Step 100/600, Loss = 0.5047858953475952\n",
      "Epoch 22/2, Step 200/600, Loss = 0.6917144060134888\n",
      "Epoch 22/2, Step 300/600, Loss = 1.1757091283798218\n",
      "Epoch 22/2, Step 400/600, Loss = 0.7386131286621094\n",
      "Epoch 22/2, Step 500/600, Loss = 0.8562155365943909\n",
      "Epoch 22/2, Step 600/600, Loss = 0.8531270027160645\n",
      "Epoch 23/2, Step 100/600, Loss = 0.8660126328468323\n",
      "Epoch 23/2, Step 200/600, Loss = 0.600343644618988\n",
      "Epoch 23/2, Step 300/600, Loss = 0.7649305462837219\n",
      "Epoch 23/2, Step 400/600, Loss = 0.6384910345077515\n",
      "Epoch 23/2, Step 500/600, Loss = 1.0258156061172485\n",
      "Epoch 23/2, Step 600/600, Loss = 0.6553298830986023\n",
      "Epoch 24/2, Step 100/600, Loss = 0.9524953961372375\n",
      "Epoch 24/2, Step 200/600, Loss = 1.0845978260040283\n",
      "Epoch 24/2, Step 300/600, Loss = 0.5988004207611084\n",
      "Epoch 24/2, Step 400/600, Loss = 0.7168669104576111\n",
      "Epoch 24/2, Step 500/600, Loss = 0.7573811411857605\n",
      "Epoch 24/2, Step 600/600, Loss = 0.8024942874908447\n",
      "Epoch 25/2, Step 100/600, Loss = 0.8617520928382874\n",
      "Epoch 25/2, Step 200/600, Loss = 0.7465834617614746\n",
      "Epoch 25/2, Step 300/600, Loss = 0.7329591512680054\n",
      "Epoch 25/2, Step 400/600, Loss = 0.5433840155601501\n",
      "Epoch 25/2, Step 500/600, Loss = 0.6190046072006226\n",
      "Epoch 25/2, Step 600/600, Loss = 0.6118175387382507\n",
      "Epoch 26/2, Step 100/600, Loss = 1.5131378173828125\n",
      "Epoch 26/2, Step 200/600, Loss = 0.7999809384346008\n",
      "Epoch 26/2, Step 300/600, Loss = 0.7084048986434937\n",
      "Epoch 26/2, Step 400/600, Loss = 0.8253446221351624\n",
      "Epoch 26/2, Step 500/600, Loss = 0.8204764723777771\n",
      "Epoch 26/2, Step 600/600, Loss = 0.8822677731513977\n",
      "Epoch 27/2, Step 100/600, Loss = 0.7891409397125244\n",
      "Epoch 27/2, Step 200/600, Loss = 0.7617610096931458\n",
      "Epoch 27/2, Step 300/600, Loss = 0.8034088015556335\n",
      "Epoch 27/2, Step 400/600, Loss = 0.6653174757957458\n",
      "Epoch 27/2, Step 500/600, Loss = 0.8489576578140259\n",
      "Epoch 27/2, Step 600/600, Loss = 1.1742459535598755\n",
      "Epoch 28/2, Step 100/600, Loss = 0.710274338722229\n",
      "Epoch 28/2, Step 200/600, Loss = 0.8989841938018799\n",
      "Epoch 28/2, Step 300/600, Loss = 0.8624563813209534\n",
      "Epoch 28/2, Step 400/600, Loss = 0.8329631090164185\n",
      "Epoch 28/2, Step 500/600, Loss = 0.6513237953186035\n",
      "Epoch 28/2, Step 600/600, Loss = 1.076900601387024\n",
      "Epoch 29/2, Step 100/600, Loss = 0.888508677482605\n",
      "Epoch 29/2, Step 200/600, Loss = 0.907931923866272\n",
      "Epoch 29/2, Step 300/600, Loss = 0.7209890484809875\n",
      "Epoch 29/2, Step 400/600, Loss = 0.7701407670974731\n",
      "Epoch 29/2, Step 500/600, Loss = 0.5986765027046204\n",
      "Epoch 29/2, Step 600/600, Loss = 0.8734838962554932\n",
      "Epoch 30/2, Step 100/600, Loss = 0.7614267468452454\n",
      "Epoch 30/2, Step 200/600, Loss = 0.8631970286369324\n",
      "Epoch 30/2, Step 300/600, Loss = 1.1526579856872559\n",
      "Epoch 30/2, Step 400/600, Loss = 0.7819502353668213\n",
      "Epoch 30/2, Step 500/600, Loss = 0.6480865478515625\n",
      "Epoch 30/2, Step 600/600, Loss = 0.887617290019989\n",
      "Epoch 31/2, Step 100/600, Loss = 0.9643345475196838\n",
      "Epoch 31/2, Step 200/600, Loss = 0.595050036907196\n",
      "Epoch 31/2, Step 300/600, Loss = 1.5502982139587402\n",
      "Epoch 31/2, Step 400/600, Loss = 0.7374916672706604\n",
      "Epoch 31/2, Step 500/600, Loss = 0.6865798830986023\n",
      "Epoch 31/2, Step 600/600, Loss = 0.7294139862060547\n",
      "Epoch 32/2, Step 100/600, Loss = 0.6465306282043457\n",
      "Epoch 32/2, Step 200/600, Loss = 0.8552631139755249\n",
      "Epoch 32/2, Step 300/600, Loss = 3.4819915294647217\n",
      "Epoch 32/2, Step 400/600, Loss = 0.7181789875030518\n",
      "Epoch 32/2, Step 500/600, Loss = 0.6711331009864807\n",
      "Epoch 32/2, Step 600/600, Loss = 0.6172187328338623\n",
      "Epoch 33/2, Step 100/600, Loss = 0.7470406293869019\n",
      "Epoch 33/2, Step 200/600, Loss = 0.6817488074302673\n",
      "Epoch 33/2, Step 300/600, Loss = 0.6035128235816956\n",
      "Epoch 33/2, Step 400/600, Loss = 0.5421103835105896\n",
      "Epoch 33/2, Step 500/600, Loss = 0.8045623898506165\n",
      "Epoch 33/2, Step 600/600, Loss = 0.781386137008667\n",
      "Epoch 34/2, Step 100/600, Loss = 0.7818173170089722\n",
      "Epoch 34/2, Step 200/600, Loss = 0.725482702255249\n",
      "Epoch 34/2, Step 300/600, Loss = 0.9114531874656677\n",
      "Epoch 34/2, Step 400/600, Loss = 0.8208680748939514\n",
      "Epoch 34/2, Step 500/600, Loss = 0.9238123297691345\n",
      "Epoch 34/2, Step 600/600, Loss = 0.809952974319458\n",
      "Epoch 35/2, Step 100/600, Loss = 0.7560074329376221\n",
      "Epoch 35/2, Step 200/600, Loss = 0.684143602848053\n",
      "Epoch 35/2, Step 300/600, Loss = 0.652571439743042\n",
      "Epoch 35/2, Step 400/600, Loss = 0.5949814319610596\n",
      "Epoch 35/2, Step 500/600, Loss = 0.8285614848136902\n",
      "Epoch 35/2, Step 600/600, Loss = 0.6646358966827393\n",
      "Epoch 36/2, Step 100/600, Loss = 0.5190450549125671\n",
      "Epoch 36/2, Step 200/600, Loss = 0.723816454410553\n",
      "Epoch 36/2, Step 300/600, Loss = 0.8312334418296814\n",
      "Epoch 36/2, Step 400/600, Loss = 0.8716031908988953\n",
      "Epoch 36/2, Step 500/600, Loss = 0.5928308963775635\n",
      "Epoch 36/2, Step 600/600, Loss = 0.903198778629303\n",
      "Epoch 37/2, Step 100/600, Loss = 0.7850469350814819\n",
      "Epoch 37/2, Step 200/600, Loss = 0.6187580227851868\n",
      "Epoch 37/2, Step 300/600, Loss = 0.9586089253425598\n",
      "Epoch 37/2, Step 400/600, Loss = 0.6985734701156616\n",
      "Epoch 37/2, Step 500/600, Loss = 0.7176287770271301\n",
      "Epoch 37/2, Step 600/600, Loss = 0.5085269808769226\n",
      "Epoch 38/2, Step 100/600, Loss = 0.9902064800262451\n",
      "Epoch 38/2, Step 200/600, Loss = 0.4828740358352661\n",
      "Epoch 38/2, Step 300/600, Loss = 0.7143802642822266\n",
      "Epoch 38/2, Step 400/600, Loss = 0.9414036273956299\n",
      "Epoch 38/2, Step 500/600, Loss = 0.9218670725822449\n",
      "Epoch 38/2, Step 600/600, Loss = 0.9076974987983704\n",
      "Epoch 39/2, Step 100/600, Loss = 0.8396714925765991\n",
      "Epoch 39/2, Step 200/600, Loss = 0.8204292058944702\n",
      "Epoch 39/2, Step 300/600, Loss = 0.9364330172538757\n",
      "Epoch 39/2, Step 400/600, Loss = 0.5672619938850403\n",
      "Epoch 39/2, Step 500/600, Loss = 0.8444033265113831\n",
      "Epoch 39/2, Step 600/600, Loss = 0.9053214192390442\n",
      "Epoch 40/2, Step 100/600, Loss = 0.8409113883972168\n",
      "Epoch 40/2, Step 200/600, Loss = 0.85040283203125\n",
      "Epoch 40/2, Step 300/600, Loss = 0.8429701328277588\n",
      "Epoch 40/2, Step 400/600, Loss = 0.6031198501586914\n",
      "Epoch 40/2, Step 500/600, Loss = 0.6203529834747314\n",
      "Epoch 40/2, Step 600/600, Loss = 0.6934195756912231\n",
      "Epoch 41/2, Step 100/600, Loss = 0.6637219786643982\n",
      "Epoch 41/2, Step 200/600, Loss = 0.738513708114624\n",
      "Epoch 41/2, Step 300/600, Loss = 0.6676951050758362\n",
      "Epoch 41/2, Step 400/600, Loss = 0.8608128428459167\n",
      "Epoch 41/2, Step 500/600, Loss = 0.8691378831863403\n",
      "Epoch 41/2, Step 600/600, Loss = 0.6413754224777222\n",
      "Epoch 42/2, Step 100/600, Loss = 0.9395986795425415\n",
      "Epoch 42/2, Step 200/600, Loss = 0.8113289475440979\n",
      "Epoch 42/2, Step 300/600, Loss = 0.5430511236190796\n",
      "Epoch 42/2, Step 400/600, Loss = 0.9028068780899048\n",
      "Epoch 42/2, Step 500/600, Loss = 0.8118918538093567\n",
      "Epoch 42/2, Step 600/600, Loss = 1.1058297157287598\n",
      "Epoch 43/2, Step 100/600, Loss = 1.0374239683151245\n",
      "Epoch 43/2, Step 200/600, Loss = 0.5511253476142883\n",
      "Epoch 43/2, Step 300/600, Loss = 0.9021445512771606\n",
      "Epoch 43/2, Step 400/600, Loss = 0.6996267437934875\n",
      "Epoch 43/2, Step 500/600, Loss = 0.9565595388412476\n",
      "Epoch 43/2, Step 600/600, Loss = 0.8424228429794312\n",
      "Epoch 44/2, Step 100/600, Loss = 1.6096408367156982\n",
      "Epoch 44/2, Step 200/600, Loss = 0.865861177444458\n",
      "Epoch 44/2, Step 300/600, Loss = 0.7614272236824036\n",
      "Epoch 44/2, Step 400/600, Loss = 1.0585589408874512\n",
      "Epoch 44/2, Step 500/600, Loss = 0.6986647844314575\n",
      "Epoch 44/2, Step 600/600, Loss = 0.7304273843765259\n",
      "Epoch 45/2, Step 100/600, Loss = 0.754163920879364\n",
      "Epoch 45/2, Step 200/600, Loss = 0.8604484796524048\n",
      "Epoch 45/2, Step 300/600, Loss = 0.9806235432624817\n",
      "Epoch 45/2, Step 400/600, Loss = 0.866619884967804\n",
      "Epoch 45/2, Step 500/600, Loss = 0.8604305386543274\n",
      "Epoch 45/2, Step 600/600, Loss = 0.9301736354827881\n",
      "Epoch 46/2, Step 100/600, Loss = 0.7244088649749756\n",
      "Epoch 46/2, Step 200/600, Loss = 0.9225350022315979\n",
      "Epoch 46/2, Step 300/600, Loss = 3.6537821292877197\n",
      "Epoch 46/2, Step 400/600, Loss = 0.9145888686180115\n",
      "Epoch 46/2, Step 500/600, Loss = 0.8453271985054016\n",
      "Epoch 46/2, Step 600/600, Loss = 0.7818326354026794\n",
      "Epoch 47/2, Step 100/600, Loss = 0.8238754868507385\n",
      "Epoch 47/2, Step 200/600, Loss = 0.7648931741714478\n",
      "Epoch 47/2, Step 300/600, Loss = 0.6933243274688721\n",
      "Epoch 47/2, Step 400/600, Loss = 0.8874970078468323\n",
      "Epoch 47/2, Step 500/600, Loss = 0.5355274081230164\n",
      "Epoch 47/2, Step 600/600, Loss = 0.6876716613769531\n",
      "Epoch 48/2, Step 100/600, Loss = 0.6383880376815796\n",
      "Epoch 48/2, Step 200/600, Loss = 0.6246317028999329\n",
      "Epoch 48/2, Step 300/600, Loss = 0.7892712354660034\n",
      "Epoch 48/2, Step 400/600, Loss = 0.7557430863380432\n",
      "Epoch 48/2, Step 500/600, Loss = 0.7588984966278076\n",
      "Epoch 48/2, Step 600/600, Loss = 0.8474327921867371\n",
      "Epoch 49/2, Step 100/600, Loss = 1.0829877853393555\n",
      "Epoch 49/2, Step 200/600, Loss = 0.7474930286407471\n",
      "Epoch 49/2, Step 300/600, Loss = 0.8560986518859863\n",
      "Epoch 49/2, Step 400/600, Loss = 0.9343897104263306\n",
      "Epoch 49/2, Step 500/600, Loss = 0.7338613867759705\n",
      "Epoch 49/2, Step 600/600, Loss = 0.843356728553772\n",
      "Epoch 50/2, Step 100/600, Loss = 0.8779810070991516\n",
      "Epoch 50/2, Step 200/600, Loss = 0.8285555839538574\n",
      "Epoch 50/2, Step 300/600, Loss = 0.9262111186981201\n",
      "Epoch 50/2, Step 400/600, Loss = 0.8336938619613647\n",
      "Epoch 50/2, Step 500/600, Loss = 0.6532850861549377\n",
      "Epoch 50/2, Step 600/600, Loss = 0.8745595812797546\n",
      "Epoch 51/2, Step 100/600, Loss = 0.8605689406394958\n",
      "Epoch 51/2, Step 200/600, Loss = 0.771584153175354\n",
      "Epoch 51/2, Step 300/600, Loss = 0.7493828535079956\n",
      "Epoch 51/2, Step 400/600, Loss = 0.7442831993103027\n",
      "Epoch 51/2, Step 500/600, Loss = 0.7894020676612854\n",
      "Epoch 51/2, Step 600/600, Loss = 0.6373575329780579\n",
      "Epoch 52/2, Step 100/600, Loss = 0.6643072366714478\n",
      "Epoch 52/2, Step 200/600, Loss = 0.8138694763183594\n",
      "Epoch 52/2, Step 300/600, Loss = 2.1739394664764404\n",
      "Epoch 52/2, Step 400/600, Loss = 0.7776638269424438\n",
      "Epoch 52/2, Step 500/600, Loss = 0.8706198334693909\n",
      "Epoch 52/2, Step 600/600, Loss = 0.9089850783348083\n",
      "Epoch 53/2, Step 100/600, Loss = 0.7240298390388489\n",
      "Epoch 53/2, Step 200/600, Loss = 0.8786166310310364\n",
      "Epoch 53/2, Step 300/600, Loss = 0.8061319589614868\n",
      "Epoch 53/2, Step 400/600, Loss = 0.7214257717132568\n",
      "Epoch 53/2, Step 500/600, Loss = 1.0539085865020752\n",
      "Epoch 53/2, Step 600/600, Loss = 0.7684303522109985\n",
      "Epoch 54/2, Step 100/600, Loss = 0.9607977271080017\n",
      "Epoch 54/2, Step 200/600, Loss = 0.772838294506073\n",
      "Epoch 54/2, Step 300/600, Loss = 0.822664737701416\n",
      "Epoch 54/2, Step 400/600, Loss = 0.8233007192611694\n",
      "Epoch 54/2, Step 500/600, Loss = 0.799691915512085\n",
      "Epoch 54/2, Step 600/600, Loss = 0.8335254788398743\n",
      "Epoch 55/2, Step 100/600, Loss = 1.3871296644210815\n",
      "Epoch 55/2, Step 200/600, Loss = 0.7128295302391052\n",
      "Epoch 55/2, Step 300/600, Loss = 0.6869668364524841\n",
      "Epoch 55/2, Step 400/600, Loss = 0.817756712436676\n",
      "Epoch 55/2, Step 500/600, Loss = 0.9235390424728394\n",
      "Epoch 55/2, Step 600/600, Loss = 0.9095311760902405\n",
      "Epoch 56/2, Step 100/600, Loss = 0.7642176151275635\n",
      "Epoch 56/2, Step 200/600, Loss = 0.9066895842552185\n",
      "Epoch 56/2, Step 300/600, Loss = 0.9389721155166626\n",
      "Epoch 56/2, Step 400/600, Loss = 0.7939554452896118\n",
      "Epoch 56/2, Step 500/600, Loss = 0.7562054991722107\n",
      "Epoch 56/2, Step 600/600, Loss = 0.7721818685531616\n",
      "Epoch 57/2, Step 100/600, Loss = 0.7752978801727295\n",
      "Epoch 57/2, Step 200/600, Loss = 0.8923677206039429\n",
      "Epoch 57/2, Step 300/600, Loss = 0.7458853125572205\n",
      "Epoch 57/2, Step 400/600, Loss = 1.1021661758422852\n",
      "Epoch 57/2, Step 500/600, Loss = 0.9921493530273438\n",
      "Epoch 57/2, Step 600/600, Loss = 1.1468864679336548\n",
      "Epoch 58/2, Step 100/600, Loss = 0.821464478969574\n",
      "Epoch 58/2, Step 200/600, Loss = 0.864921510219574\n",
      "Epoch 58/2, Step 300/600, Loss = 0.9042745232582092\n",
      "Epoch 58/2, Step 400/600, Loss = 0.6289334893226624\n",
      "Epoch 58/2, Step 500/600, Loss = 0.9342457056045532\n",
      "Epoch 58/2, Step 600/600, Loss = 1.04622483253479\n",
      "Epoch 59/2, Step 100/600, Loss = 0.95982825756073\n",
      "Epoch 59/2, Step 200/600, Loss = 0.6327556371688843\n",
      "Epoch 59/2, Step 300/600, Loss = 0.6799561381340027\n",
      "Epoch 59/2, Step 400/600, Loss = 0.9146010875701904\n",
      "Epoch 59/2, Step 500/600, Loss = 1.0242719650268555\n",
      "Epoch 59/2, Step 600/600, Loss = 0.9742301106452942\n",
      "Epoch 60/2, Step 100/600, Loss = 0.9237036108970642\n",
      "Epoch 60/2, Step 200/600, Loss = 0.7985848784446716\n",
      "Epoch 60/2, Step 300/600, Loss = 0.8218581676483154\n",
      "Epoch 60/2, Step 400/600, Loss = 0.8857024908065796\n",
      "Epoch 60/2, Step 500/600, Loss = 1.0288262367248535\n",
      "Epoch 60/2, Step 600/600, Loss = 1.7373173236846924\n",
      "Epoch 61/2, Step 100/600, Loss = 0.8786763548851013\n",
      "Epoch 61/2, Step 200/600, Loss = 0.9589526653289795\n",
      "Epoch 61/2, Step 300/600, Loss = 0.7869260907173157\n",
      "Epoch 61/2, Step 400/600, Loss = 0.6346504092216492\n",
      "Epoch 61/2, Step 500/600, Loss = 0.7409112453460693\n",
      "Epoch 61/2, Step 600/600, Loss = 0.6607446074485779\n",
      "Epoch 62/2, Step 100/600, Loss = 0.9944191575050354\n",
      "Epoch 62/2, Step 200/600, Loss = 0.9073131084442139\n",
      "Epoch 62/2, Step 300/600, Loss = 0.7909668684005737\n",
      "Epoch 62/2, Step 400/600, Loss = 0.8969340324401855\n",
      "Epoch 62/2, Step 500/600, Loss = 0.782854437828064\n",
      "Epoch 62/2, Step 600/600, Loss = 0.9825572371482849\n",
      "Epoch 63/2, Step 100/600, Loss = 0.9607381224632263\n",
      "Epoch 63/2, Step 200/600, Loss = 0.626170814037323\n",
      "Epoch 63/2, Step 300/600, Loss = 1.1679586172103882\n",
      "Epoch 63/2, Step 400/600, Loss = 0.8676430583000183\n",
      "Epoch 63/2, Step 500/600, Loss = 1.029709815979004\n",
      "Epoch 63/2, Step 600/600, Loss = 0.8421533703804016\n",
      "Epoch 64/2, Step 100/600, Loss = 0.9655473828315735\n",
      "Epoch 64/2, Step 200/600, Loss = 1.102617621421814\n",
      "Epoch 64/2, Step 300/600, Loss = 0.9997897148132324\n",
      "Epoch 64/2, Step 400/600, Loss = 1.0044769048690796\n",
      "Epoch 64/2, Step 500/600, Loss = 1.0231014490127563\n",
      "Epoch 64/2, Step 600/600, Loss = 0.8460622429847717\n",
      "Epoch 65/2, Step 100/600, Loss = 0.9563268423080444\n",
      "Epoch 65/2, Step 200/600, Loss = 1.0712289810180664\n",
      "Epoch 65/2, Step 300/600, Loss = 1.0145468711853027\n",
      "Epoch 65/2, Step 400/600, Loss = 0.936400294303894\n",
      "Epoch 65/2, Step 500/600, Loss = 0.8708556890487671\n",
      "Epoch 65/2, Step 600/600, Loss = 0.9188586473464966\n",
      "Epoch 66/2, Step 100/600, Loss = 0.9151170253753662\n",
      "Epoch 66/2, Step 200/600, Loss = 1.0221091508865356\n",
      "Epoch 66/2, Step 300/600, Loss = 0.9822889566421509\n",
      "Epoch 66/2, Step 400/600, Loss = 0.9971446394920349\n",
      "Epoch 66/2, Step 500/600, Loss = 0.9323722124099731\n",
      "Epoch 66/2, Step 600/600, Loss = 1.0345121622085571\n",
      "Epoch 67/2, Step 100/600, Loss = 0.8229498863220215\n",
      "Epoch 67/2, Step 200/600, Loss = 0.8262150287628174\n",
      "Epoch 67/2, Step 300/600, Loss = 0.9133915901184082\n",
      "Epoch 67/2, Step 400/600, Loss = 1.0396239757537842\n",
      "Epoch 67/2, Step 500/600, Loss = 0.7756903171539307\n",
      "Epoch 67/2, Step 600/600, Loss = 1.0034682750701904\n",
      "Epoch 68/2, Step 100/600, Loss = 0.8286200165748596\n",
      "Epoch 68/2, Step 200/600, Loss = 0.989119291305542\n",
      "Epoch 68/2, Step 300/600, Loss = 0.7903922200202942\n",
      "Epoch 68/2, Step 400/600, Loss = 1.0311951637268066\n",
      "Epoch 68/2, Step 500/600, Loss = 0.9589021801948547\n",
      "Epoch 68/2, Step 600/600, Loss = 0.9134978652000427\n",
      "Epoch 69/2, Step 100/600, Loss = 1.1227142810821533\n",
      "Epoch 69/2, Step 200/600, Loss = 0.8311914801597595\n",
      "Epoch 69/2, Step 300/600, Loss = 1.1528064012527466\n",
      "Epoch 69/2, Step 400/600, Loss = 1.0613876581192017\n",
      "Epoch 69/2, Step 500/600, Loss = 0.8016231060028076\n",
      "Epoch 69/2, Step 600/600, Loss = 0.9253050088882446\n",
      "Epoch 70/2, Step 100/600, Loss = 0.8102806806564331\n",
      "Epoch 70/2, Step 200/600, Loss = 0.9444114565849304\n",
      "Epoch 70/2, Step 300/600, Loss = 0.757402777671814\n",
      "Epoch 70/2, Step 400/600, Loss = 0.81574547290802\n",
      "Epoch 70/2, Step 500/600, Loss = 0.8763905167579651\n",
      "Epoch 70/2, Step 600/600, Loss = 0.959601640701294\n",
      "Epoch 71/2, Step 100/600, Loss = 0.9977297782897949\n",
      "Epoch 71/2, Step 200/600, Loss = 0.9363073706626892\n",
      "Epoch 71/2, Step 300/600, Loss = 1.0037455558776855\n",
      "Epoch 71/2, Step 400/600, Loss = 0.9176404476165771\n",
      "Epoch 71/2, Step 500/600, Loss = 0.8400599956512451\n",
      "Epoch 71/2, Step 600/600, Loss = 2.4334917068481445\n",
      "Epoch 72/2, Step 100/600, Loss = 0.8276694416999817\n",
      "Epoch 72/2, Step 200/600, Loss = 0.9166362285614014\n",
      "Epoch 72/2, Step 300/600, Loss = 0.8022077083587646\n",
      "Epoch 72/2, Step 400/600, Loss = 0.7659963965415955\n",
      "Epoch 72/2, Step 500/600, Loss = 0.8771619200706482\n",
      "Epoch 72/2, Step 600/600, Loss = 0.8225677609443665\n",
      "Epoch 73/2, Step 100/600, Loss = 0.7587130665779114\n",
      "Epoch 73/2, Step 200/600, Loss = 0.6616339683532715\n",
      "Epoch 73/2, Step 300/600, Loss = 0.7815963625907898\n",
      "Epoch 73/2, Step 400/600, Loss = 0.9546018242835999\n",
      "Epoch 73/2, Step 500/600, Loss = 5.921698093414307\n",
      "Epoch 73/2, Step 600/600, Loss = 0.9141525030136108\n",
      "Epoch 74/2, Step 100/600, Loss = 0.8893991112709045\n",
      "Epoch 74/2, Step 200/600, Loss = 0.7694206833839417\n",
      "Epoch 74/2, Step 300/600, Loss = 0.8998444080352783\n",
      "Epoch 74/2, Step 400/600, Loss = 0.7087329626083374\n",
      "Epoch 74/2, Step 500/600, Loss = 0.8680969476699829\n",
      "Epoch 74/2, Step 600/600, Loss = 0.9780117273330688\n",
      "Epoch 75/2, Step 100/600, Loss = 1.6939826011657715\n",
      "Epoch 75/2, Step 200/600, Loss = 0.7068251967430115\n",
      "Epoch 75/2, Step 300/600, Loss = 0.9376473426818848\n",
      "Epoch 75/2, Step 400/600, Loss = 1.14107084274292\n",
      "Epoch 75/2, Step 500/600, Loss = 0.955722451210022\n",
      "Epoch 75/2, Step 600/600, Loss = 0.8213096857070923\n",
      "Epoch 76/2, Step 100/600, Loss = 0.9172865152359009\n",
      "Epoch 76/2, Step 200/600, Loss = 0.8169113397598267\n",
      "Epoch 76/2, Step 300/600, Loss = 0.710384726524353\n",
      "Epoch 76/2, Step 400/600, Loss = 1.1048778295516968\n",
      "Epoch 76/2, Step 500/600, Loss = 0.8891716003417969\n",
      "Epoch 76/2, Step 600/600, Loss = 0.8272075057029724\n",
      "Epoch 77/2, Step 100/600, Loss = 0.8126407861709595\n",
      "Epoch 77/2, Step 200/600, Loss = 0.8466867208480835\n",
      "Epoch 77/2, Step 300/600, Loss = 1.0065170526504517\n",
      "Epoch 77/2, Step 400/600, Loss = 0.6775640845298767\n",
      "Epoch 77/2, Step 500/600, Loss = 0.9572769999504089\n",
      "Epoch 77/2, Step 600/600, Loss = 0.9532716870307922\n",
      "Epoch 78/2, Step 100/600, Loss = 1.0479949712753296\n",
      "Epoch 78/2, Step 200/600, Loss = 0.9154074788093567\n",
      "Epoch 78/2, Step 300/600, Loss = 0.6233729720115662\n",
      "Epoch 78/2, Step 400/600, Loss = 0.8596016764640808\n",
      "Epoch 78/2, Step 500/600, Loss = 0.9395269155502319\n",
      "Epoch 78/2, Step 600/600, Loss = 1.0131011009216309\n",
      "Epoch 79/2, Step 100/600, Loss = 0.8205362558364868\n",
      "Epoch 79/2, Step 200/600, Loss = 0.8916376233100891\n",
      "Epoch 79/2, Step 300/600, Loss = 0.8988061547279358\n",
      "Epoch 79/2, Step 400/600, Loss = 0.5995625257492065\n",
      "Epoch 79/2, Step 500/600, Loss = 0.9631499648094177\n",
      "Epoch 79/2, Step 600/600, Loss = 0.8346110582351685\n",
      "Epoch 80/2, Step 100/600, Loss = 0.7983418107032776\n",
      "Epoch 80/2, Step 200/600, Loss = 0.8950348496437073\n",
      "Epoch 80/2, Step 300/600, Loss = 0.8150343298912048\n",
      "Epoch 80/2, Step 400/600, Loss = 0.9952149987220764\n",
      "Epoch 80/2, Step 500/600, Loss = 1.126387596130371\n",
      "Epoch 80/2, Step 600/600, Loss = 0.8570480942726135\n",
      "Epoch 81/2, Step 100/600, Loss = 0.9779614210128784\n",
      "Epoch 81/2, Step 200/600, Loss = 1.0079542398452759\n",
      "Epoch 81/2, Step 300/600, Loss = 0.7070785760879517\n",
      "Epoch 81/2, Step 400/600, Loss = 0.6565331220626831\n",
      "Epoch 81/2, Step 500/600, Loss = 1.0250641107559204\n",
      "Epoch 81/2, Step 600/600, Loss = 0.877247154712677\n",
      "Epoch 82/2, Step 100/600, Loss = 0.8409615159034729\n",
      "Epoch 82/2, Step 200/600, Loss = 0.9361103773117065\n",
      "Epoch 82/2, Step 300/600, Loss = 0.7590369582176208\n",
      "Epoch 82/2, Step 400/600, Loss = 1.0127264261245728\n",
      "Epoch 82/2, Step 500/600, Loss = 0.9753439426422119\n",
      "Epoch 82/2, Step 600/600, Loss = 0.8921718001365662\n",
      "Epoch 83/2, Step 100/600, Loss = 0.8432972431182861\n",
      "Epoch 83/2, Step 200/600, Loss = 0.6873670816421509\n",
      "Epoch 83/2, Step 300/600, Loss = 0.8355440497398376\n",
      "Epoch 83/2, Step 400/600, Loss = 0.8778874278068542\n",
      "Epoch 83/2, Step 500/600, Loss = 1.1201670169830322\n",
      "Epoch 83/2, Step 600/600, Loss = 1.1318559646606445\n",
      "Epoch 84/2, Step 100/600, Loss = 1.057093620300293\n",
      "Epoch 84/2, Step 200/600, Loss = 0.856097936630249\n",
      "Epoch 84/2, Step 300/600, Loss = 1.0674091577529907\n",
      "Epoch 84/2, Step 400/600, Loss = 0.907604455947876\n",
      "Epoch 84/2, Step 500/600, Loss = 0.8491188883781433\n",
      "Epoch 84/2, Step 600/600, Loss = 0.7484123110771179\n",
      "Epoch 85/2, Step 100/600, Loss = 0.8030826449394226\n",
      "Epoch 85/2, Step 200/600, Loss = 0.7657334804534912\n",
      "Epoch 85/2, Step 300/600, Loss = 0.9630773067474365\n",
      "Epoch 85/2, Step 400/600, Loss = 0.8245598077774048\n",
      "Epoch 85/2, Step 500/600, Loss = 0.8036302328109741\n",
      "Epoch 85/2, Step 600/600, Loss = 0.7438449859619141\n",
      "Epoch 86/2, Step 100/600, Loss = 0.9383687376976013\n",
      "Epoch 86/2, Step 200/600, Loss = 0.9587273597717285\n",
      "Epoch 86/2, Step 300/600, Loss = 0.9953945279121399\n",
      "Epoch 86/2, Step 400/600, Loss = 0.7897275686264038\n",
      "Epoch 86/2, Step 500/600, Loss = 0.92121422290802\n",
      "Epoch 86/2, Step 600/600, Loss = 0.9359541535377502\n",
      "Epoch 87/2, Step 100/600, Loss = 1.0321874618530273\n",
      "Epoch 87/2, Step 200/600, Loss = 1.0204918384552002\n",
      "Epoch 87/2, Step 300/600, Loss = 0.9164116382598877\n",
      "Epoch 87/2, Step 400/600, Loss = 1.0213702917099\n",
      "Epoch 87/2, Step 500/600, Loss = 0.8573287129402161\n",
      "Epoch 87/2, Step 600/600, Loss = 0.9878326654434204\n",
      "Epoch 88/2, Step 100/600, Loss = 0.8011358380317688\n",
      "Epoch 88/2, Step 200/600, Loss = 1.0026497840881348\n",
      "Epoch 88/2, Step 300/600, Loss = 0.7799833416938782\n",
      "Epoch 88/2, Step 400/600, Loss = 0.944084644317627\n",
      "Epoch 88/2, Step 500/600, Loss = 0.755047619342804\n",
      "Epoch 88/2, Step 600/600, Loss = 0.9036090970039368\n",
      "Epoch 89/2, Step 100/600, Loss = 1.4520796537399292\n",
      "Epoch 89/2, Step 200/600, Loss = 1.122624397277832\n",
      "Epoch 89/2, Step 300/600, Loss = 0.7912085652351379\n",
      "Epoch 89/2, Step 400/600, Loss = 1.0401997566223145\n",
      "Epoch 89/2, Step 500/600, Loss = 3.5015125274658203\n",
      "Epoch 89/2, Step 600/600, Loss = 1.0399471521377563\n",
      "Epoch 90/2, Step 100/600, Loss = 0.8917107582092285\n",
      "Epoch 90/2, Step 200/600, Loss = 0.9354947805404663\n",
      "Epoch 90/2, Step 300/600, Loss = 0.9291160702705383\n",
      "Epoch 90/2, Step 400/600, Loss = 0.869597315788269\n",
      "Epoch 90/2, Step 500/600, Loss = 0.9538225531578064\n",
      "Epoch 90/2, Step 600/600, Loss = 0.9695737957954407\n",
      "Epoch 91/2, Step 100/600, Loss = 1.6480680704116821\n",
      "Epoch 91/2, Step 200/600, Loss = 0.9642238020896912\n",
      "Epoch 91/2, Step 300/600, Loss = 0.9838369488716125\n",
      "Epoch 91/2, Step 400/600, Loss = 0.9681933522224426\n",
      "Epoch 91/2, Step 500/600, Loss = 0.8909254670143127\n",
      "Epoch 91/2, Step 600/600, Loss = 1.2341666221618652\n",
      "Epoch 92/2, Step 100/600, Loss = 0.9124168157577515\n",
      "Epoch 92/2, Step 200/600, Loss = 1.1711885929107666\n",
      "Epoch 92/2, Step 300/600, Loss = 0.9614720940589905\n",
      "Epoch 92/2, Step 400/600, Loss = 0.9646548628807068\n",
      "Epoch 92/2, Step 500/600, Loss = 0.9675973653793335\n",
      "Epoch 92/2, Step 600/600, Loss = 0.9125069379806519\n",
      "Epoch 93/2, Step 100/600, Loss = 1.2194929122924805\n",
      "Epoch 93/2, Step 200/600, Loss = 0.8865460753440857\n",
      "Epoch 93/2, Step 300/600, Loss = 1.0864875316619873\n",
      "Epoch 93/2, Step 400/600, Loss = 0.7770749926567078\n",
      "Epoch 93/2, Step 500/600, Loss = 0.9440035223960876\n",
      "Epoch 93/2, Step 600/600, Loss = 1.0080904960632324\n",
      "Epoch 94/2, Step 100/600, Loss = 0.9012796878814697\n",
      "Epoch 94/2, Step 200/600, Loss = 1.0530650615692139\n",
      "Epoch 94/2, Step 300/600, Loss = 1.1763874292373657\n",
      "Epoch 94/2, Step 400/600, Loss = 1.0310311317443848\n",
      "Epoch 94/2, Step 500/600, Loss = 0.9436666965484619\n",
      "Epoch 94/2, Step 600/600, Loss = 1.0767890214920044\n",
      "Epoch 95/2, Step 100/600, Loss = 0.9275002479553223\n",
      "Epoch 95/2, Step 200/600, Loss = 1.2771872282028198\n",
      "Epoch 95/2, Step 300/600, Loss = 1.1046490669250488\n",
      "Epoch 95/2, Step 400/600, Loss = 0.9791279435157776\n",
      "Epoch 95/2, Step 500/600, Loss = 1.1177902221679688\n",
      "Epoch 95/2, Step 600/600, Loss = 1.2872079610824585\n",
      "Epoch 96/2, Step 100/600, Loss = 0.9654865264892578\n",
      "Epoch 96/2, Step 200/600, Loss = 0.9471855163574219\n",
      "Epoch 96/2, Step 300/600, Loss = 0.8630461692810059\n",
      "Epoch 96/2, Step 400/600, Loss = 0.9796386957168579\n",
      "Epoch 96/2, Step 500/600, Loss = 0.9356235265731812\n",
      "Epoch 96/2, Step 600/600, Loss = 4.69180154800415\n",
      "Epoch 97/2, Step 100/600, Loss = 1.0166940689086914\n",
      "Epoch 97/2, Step 200/600, Loss = 1.0060267448425293\n",
      "Epoch 97/2, Step 300/600, Loss = 1.0530285835266113\n",
      "Epoch 97/2, Step 400/600, Loss = 0.9554122686386108\n",
      "Epoch 97/2, Step 500/600, Loss = 1.1762325763702393\n",
      "Epoch 97/2, Step 600/600, Loss = 0.8420665264129639\n",
      "Epoch 98/2, Step 100/600, Loss = 0.9024140238761902\n",
      "Epoch 98/2, Step 200/600, Loss = 0.9047486782073975\n",
      "Epoch 98/2, Step 300/600, Loss = 1.0019949674606323\n",
      "Epoch 98/2, Step 400/600, Loss = 0.9895340800285339\n",
      "Epoch 98/2, Step 500/600, Loss = 1.1961630582809448\n",
      "Epoch 98/2, Step 600/600, Loss = 1.0367282629013062\n",
      "Epoch 99/2, Step 100/600, Loss = 0.9898934364318848\n",
      "Epoch 99/2, Step 200/600, Loss = 1.0586587190628052\n",
      "Epoch 99/2, Step 300/600, Loss = 0.9589261412620544\n",
      "Epoch 99/2, Step 400/600, Loss = 1.0684748888015747\n",
      "Epoch 99/2, Step 500/600, Loss = 1.2666999101638794\n",
      "Epoch 99/2, Step 600/600, Loss = 1.1515848636627197\n",
      "Epoch 100/2, Step 100/600, Loss = 0.8500739336013794\n",
      "Epoch 100/2, Step 200/600, Loss = 0.826601505279541\n",
      "Epoch 100/2, Step 300/600, Loss = 0.875834047794342\n",
      "Epoch 100/2, Step 400/600, Loss = 1.0153920650482178\n",
      "Epoch 100/2, Step 500/600, Loss = 1.005009412765503\n",
      "Epoch 100/2, Step 600/600, Loss = 0.907603919506073\n",
      "Epoch 101/2, Step 100/600, Loss = 1.0590808391571045\n",
      "Epoch 101/2, Step 200/600, Loss = 0.9065120220184326\n",
      "Epoch 101/2, Step 300/600, Loss = 1.0213462114334106\n",
      "Epoch 101/2, Step 400/600, Loss = 1.0667978525161743\n",
      "Epoch 101/2, Step 500/600, Loss = 0.955455482006073\n",
      "Epoch 101/2, Step 600/600, Loss = 3.5367236137390137\n",
      "Epoch 102/2, Step 100/600, Loss = 0.9317905306816101\n",
      "Epoch 102/2, Step 200/600, Loss = 1.0963882207870483\n",
      "Epoch 102/2, Step 300/600, Loss = 0.6783016324043274\n",
      "Epoch 102/2, Step 400/600, Loss = 1.168168306350708\n",
      "Epoch 102/2, Step 500/600, Loss = 0.8513214588165283\n",
      "Epoch 102/2, Step 600/600, Loss = 0.9252774119377136\n",
      "Epoch 103/2, Step 100/600, Loss = 0.8893258571624756\n",
      "Epoch 103/2, Step 200/600, Loss = 0.9581059813499451\n",
      "Epoch 103/2, Step 300/600, Loss = 0.7706426382064819\n",
      "Epoch 103/2, Step 400/600, Loss = 0.7496181726455688\n",
      "Epoch 103/2, Step 500/600, Loss = 0.8433468341827393\n",
      "Epoch 103/2, Step 600/600, Loss = 0.9925335049629211\n",
      "Epoch 104/2, Step 100/600, Loss = 0.8410821557044983\n",
      "Epoch 104/2, Step 200/600, Loss = 0.9447781443595886\n",
      "Epoch 104/2, Step 300/600, Loss = 0.9556362628936768\n",
      "Epoch 104/2, Step 400/600, Loss = 0.8837915658950806\n",
      "Epoch 104/2, Step 500/600, Loss = 1.0019049644470215\n",
      "Epoch 104/2, Step 600/600, Loss = 1.0533541440963745\n",
      "Epoch 105/2, Step 100/600, Loss = 1.3196237087249756\n",
      "Epoch 105/2, Step 200/600, Loss = 0.919048547744751\n",
      "Epoch 105/2, Step 300/600, Loss = 1.0391062498092651\n",
      "Epoch 105/2, Step 400/600, Loss = 1.037855625152588\n",
      "Epoch 105/2, Step 500/600, Loss = 0.8680476546287537\n",
      "Epoch 105/2, Step 600/600, Loss = 1.2745983600616455\n",
      "Epoch 106/2, Step 100/600, Loss = 1.1984158754348755\n",
      "Epoch 106/2, Step 200/600, Loss = 0.9801643490791321\n",
      "Epoch 106/2, Step 300/600, Loss = 0.9832670092582703\n",
      "Epoch 106/2, Step 400/600, Loss = 0.9933632612228394\n",
      "Epoch 106/2, Step 500/600, Loss = 1.0529181957244873\n",
      "Epoch 106/2, Step 600/600, Loss = 0.9988800287246704\n",
      "Epoch 107/2, Step 100/600, Loss = 0.9070172905921936\n",
      "Epoch 107/2, Step 200/600, Loss = 1.1326758861541748\n",
      "Epoch 107/2, Step 300/600, Loss = 0.9648040533065796\n",
      "Epoch 107/2, Step 400/600, Loss = 1.170366644859314\n",
      "Epoch 107/2, Step 500/600, Loss = 0.8446612358093262\n",
      "Epoch 107/2, Step 600/600, Loss = 0.9501925706863403\n",
      "Epoch 108/2, Step 100/600, Loss = 0.9924488663673401\n",
      "Epoch 108/2, Step 200/600, Loss = 1.048979640007019\n",
      "Epoch 108/2, Step 300/600, Loss = 1.0696320533752441\n",
      "Epoch 108/2, Step 400/600, Loss = 0.8801918029785156\n",
      "Epoch 108/2, Step 500/600, Loss = 1.2900538444519043\n",
      "Epoch 108/2, Step 600/600, Loss = 1.052261471748352\n",
      "Epoch 109/2, Step 100/600, Loss = 1.144097924232483\n",
      "Epoch 109/2, Step 200/600, Loss = 1.0813944339752197\n",
      "Epoch 109/2, Step 300/600, Loss = 1.1906710863113403\n",
      "Epoch 109/2, Step 400/600, Loss = 1.2320189476013184\n",
      "Epoch 109/2, Step 500/600, Loss = 0.9057646989822388\n",
      "Epoch 109/2, Step 600/600, Loss = 0.9706736207008362\n",
      "Epoch 110/2, Step 100/600, Loss = 0.8514292240142822\n",
      "Epoch 110/2, Step 200/600, Loss = 1.1878613233566284\n",
      "Epoch 110/2, Step 300/600, Loss = 1.4449124336242676\n",
      "Epoch 110/2, Step 400/600, Loss = 1.163486361503601\n",
      "Epoch 110/2, Step 500/600, Loss = 1.0940959453582764\n",
      "Epoch 110/2, Step 600/600, Loss = 1.1176899671554565\n",
      "Epoch 111/2, Step 100/600, Loss = 0.9693779945373535\n",
      "Epoch 111/2, Step 200/600, Loss = 1.1693451404571533\n",
      "Epoch 111/2, Step 300/600, Loss = 0.8107582926750183\n",
      "Epoch 111/2, Step 400/600, Loss = 0.9729028940200806\n",
      "Epoch 111/2, Step 500/600, Loss = 1.0023093223571777\n",
      "Epoch 111/2, Step 600/600, Loss = 0.9537531137466431\n",
      "Epoch 112/2, Step 100/600, Loss = 0.969768762588501\n",
      "Epoch 112/2, Step 200/600, Loss = 1.0881648063659668\n",
      "Epoch 112/2, Step 300/600, Loss = 0.8262264728546143\n",
      "Epoch 112/2, Step 400/600, Loss = 1.0947322845458984\n",
      "Epoch 112/2, Step 500/600, Loss = 0.8692848086357117\n",
      "Epoch 112/2, Step 600/600, Loss = 1.0472480058670044\n",
      "Epoch 113/2, Step 100/600, Loss = 0.9491748809814453\n",
      "Epoch 113/2, Step 200/600, Loss = 1.2301030158996582\n",
      "Epoch 113/2, Step 300/600, Loss = 0.8939557075500488\n",
      "Epoch 113/2, Step 400/600, Loss = 1.1831598281860352\n",
      "Epoch 113/2, Step 500/600, Loss = 0.8809939026832581\n",
      "Epoch 113/2, Step 600/600, Loss = 1.2115919589996338\n",
      "Epoch 114/2, Step 100/600, Loss = 1.0799161195755005\n",
      "Epoch 114/2, Step 200/600, Loss = 1.2084256410598755\n",
      "Epoch 114/2, Step 300/600, Loss = 1.292026400566101\n",
      "Epoch 114/2, Step 400/600, Loss = 0.997346818447113\n",
      "Epoch 114/2, Step 500/600, Loss = 0.8822446465492249\n",
      "Epoch 114/2, Step 600/600, Loss = 0.9611825346946716\n",
      "Epoch 115/2, Step 100/600, Loss = 1.2011313438415527\n",
      "Epoch 115/2, Step 200/600, Loss = 0.9471514821052551\n",
      "Epoch 115/2, Step 300/600, Loss = 1.0996307134628296\n",
      "Epoch 115/2, Step 400/600, Loss = 0.9519289135932922\n",
      "Epoch 115/2, Step 500/600, Loss = 1.0959968566894531\n",
      "Epoch 115/2, Step 600/600, Loss = 1.023389458656311\n",
      "Epoch 116/2, Step 100/600, Loss = 1.091320514678955\n",
      "Epoch 116/2, Step 200/600, Loss = 1.5815461874008179\n",
      "Epoch 116/2, Step 300/600, Loss = 1.1701761484146118\n",
      "Epoch 116/2, Step 400/600, Loss = 0.9376722574234009\n",
      "Epoch 116/2, Step 500/600, Loss = 0.9826606512069702\n",
      "Epoch 116/2, Step 600/600, Loss = 1.155881404876709\n",
      "Epoch 117/2, Step 100/600, Loss = 1.14252507686615\n",
      "Epoch 117/2, Step 200/600, Loss = 1.2842718362808228\n",
      "Epoch 117/2, Step 300/600, Loss = 0.9447442889213562\n",
      "Epoch 117/2, Step 400/600, Loss = 0.9432163238525391\n",
      "Epoch 117/2, Step 500/600, Loss = 0.9805875420570374\n",
      "Epoch 117/2, Step 600/600, Loss = 0.8650454878807068\n",
      "Epoch 118/2, Step 100/600, Loss = 1.0672706365585327\n",
      "Epoch 118/2, Step 200/600, Loss = 0.9769856333732605\n",
      "Epoch 118/2, Step 300/600, Loss = 1.25490140914917\n",
      "Epoch 118/2, Step 400/600, Loss = 1.1666159629821777\n",
      "Epoch 118/2, Step 500/600, Loss = 1.1793015003204346\n",
      "Epoch 118/2, Step 600/600, Loss = 1.308611273765564\n",
      "Epoch 119/2, Step 100/600, Loss = 0.9204397797584534\n",
      "Epoch 119/2, Step 200/600, Loss = 1.1694483757019043\n",
      "Epoch 119/2, Step 300/600, Loss = 1.0589327812194824\n",
      "Epoch 119/2, Step 400/600, Loss = 1.120448350906372\n",
      "Epoch 119/2, Step 500/600, Loss = 1.0813891887664795\n",
      "Epoch 119/2, Step 600/600, Loss = 1.0140063762664795\n",
      "Epoch 120/2, Step 100/600, Loss = 1.1402735710144043\n",
      "Epoch 120/2, Step 200/600, Loss = 1.0905108451843262\n",
      "Epoch 120/2, Step 300/600, Loss = 1.172010898590088\n",
      "Epoch 120/2, Step 400/600, Loss = 1.1697546243667603\n",
      "Epoch 120/2, Step 500/600, Loss = 0.9990275502204895\n",
      "Epoch 120/2, Step 600/600, Loss = 0.9151715040206909\n",
      "Epoch 121/2, Step 100/600, Loss = 1.0135456323623657\n",
      "Epoch 121/2, Step 200/600, Loss = 1.139292597770691\n",
      "Epoch 121/2, Step 300/600, Loss = 1.008939266204834\n",
      "Epoch 121/2, Step 400/600, Loss = 0.9783251285552979\n",
      "Epoch 121/2, Step 500/600, Loss = 1.1758157014846802\n",
      "Epoch 121/2, Step 600/600, Loss = 1.0372105836868286\n",
      "Epoch 122/2, Step 100/600, Loss = 0.9431461095809937\n",
      "Epoch 122/2, Step 200/600, Loss = 0.9736201763153076\n",
      "Epoch 122/2, Step 300/600, Loss = 1.1448436975479126\n",
      "Epoch 122/2, Step 400/600, Loss = 1.0905797481536865\n",
      "Epoch 122/2, Step 500/600, Loss = 0.917555570602417\n",
      "Epoch 122/2, Step 600/600, Loss = 1.107119083404541\n",
      "Epoch 123/2, Step 100/600, Loss = 1.223411202430725\n",
      "Epoch 123/2, Step 200/600, Loss = 0.9071429371833801\n",
      "Epoch 123/2, Step 300/600, Loss = 1.3191577196121216\n",
      "Epoch 123/2, Step 400/600, Loss = 1.002575159072876\n",
      "Epoch 123/2, Step 500/600, Loss = 1.2181555032730103\n",
      "Epoch 123/2, Step 600/600, Loss = 0.9828943610191345\n",
      "Epoch 124/2, Step 100/600, Loss = 1.0466817617416382\n",
      "Epoch 124/2, Step 200/600, Loss = 1.2025505304336548\n",
      "Epoch 124/2, Step 300/600, Loss = 1.2675625085830688\n",
      "Epoch 124/2, Step 400/600, Loss = 13.327413558959961\n",
      "Epoch 124/2, Step 500/600, Loss = 1.0765613317489624\n",
      "Epoch 124/2, Step 600/600, Loss = 1.1245561838150024\n",
      "Epoch 125/2, Step 100/600, Loss = 1.0164203643798828\n",
      "Epoch 125/2, Step 200/600, Loss = 1.06329345703125\n",
      "Epoch 125/2, Step 300/600, Loss = 1.078710675239563\n",
      "Epoch 125/2, Step 400/600, Loss = 0.8508219718933105\n",
      "Epoch 125/2, Step 500/600, Loss = 1.2426761388778687\n",
      "Epoch 125/2, Step 600/600, Loss = 1.2162847518920898\n",
      "Epoch 126/2, Step 100/600, Loss = 1.0583581924438477\n",
      "Epoch 126/2, Step 200/600, Loss = 1.088435411453247\n",
      "Epoch 126/2, Step 300/600, Loss = 1.1687506437301636\n",
      "Epoch 126/2, Step 400/600, Loss = 1.2771549224853516\n",
      "Epoch 126/2, Step 500/600, Loss = 1.088971495628357\n",
      "Epoch 126/2, Step 600/600, Loss = 1.2270170450210571\n",
      "Epoch 127/2, Step 100/600, Loss = 0.7867478728294373\n",
      "Epoch 127/2, Step 200/600, Loss = 1.0641082525253296\n",
      "Epoch 127/2, Step 300/600, Loss = 1.2510879039764404\n",
      "Epoch 127/2, Step 400/600, Loss = 1.1165189743041992\n",
      "Epoch 127/2, Step 500/600, Loss = 1.0194871425628662\n",
      "Epoch 127/2, Step 600/600, Loss = 1.165619969367981\n",
      "Epoch 128/2, Step 100/600, Loss = 0.9449299573898315\n",
      "Epoch 128/2, Step 200/600, Loss = 1.2520818710327148\n",
      "Epoch 128/2, Step 300/600, Loss = 1.0289477109909058\n",
      "Epoch 128/2, Step 400/600, Loss = 1.1345975399017334\n",
      "Epoch 128/2, Step 500/600, Loss = 1.2280763387680054\n",
      "Epoch 128/2, Step 600/600, Loss = 1.1817834377288818\n",
      "Epoch 129/2, Step 100/600, Loss = 1.1544253826141357\n",
      "Epoch 129/2, Step 200/600, Loss = 1.2719769477844238\n",
      "Epoch 129/2, Step 300/600, Loss = 0.952714204788208\n",
      "Epoch 129/2, Step 400/600, Loss = 1.0924572944641113\n",
      "Epoch 129/2, Step 500/600, Loss = 1.0895817279815674\n",
      "Epoch 129/2, Step 600/600, Loss = 1.0272266864776611\n",
      "Epoch 130/2, Step 100/600, Loss = 1.3978177309036255\n",
      "Epoch 130/2, Step 200/600, Loss = 0.9922372698783875\n",
      "Epoch 130/2, Step 300/600, Loss = 1.2450186014175415\n",
      "Epoch 130/2, Step 400/600, Loss = 1.3136422634124756\n",
      "Epoch 130/2, Step 500/600, Loss = 1.1767349243164062\n",
      "Epoch 130/2, Step 600/600, Loss = 1.0402214527130127\n",
      "Epoch 131/2, Step 100/600, Loss = 0.9488855600357056\n",
      "Epoch 131/2, Step 200/600, Loss = 1.2891830205917358\n",
      "Epoch 131/2, Step 300/600, Loss = 5.819146156311035\n",
      "Epoch 131/2, Step 400/600, Loss = 1.0394635200500488\n",
      "Epoch 131/2, Step 500/600, Loss = 1.2524383068084717\n",
      "Epoch 131/2, Step 600/600, Loss = 0.9309182167053223\n",
      "Epoch 132/2, Step 100/600, Loss = 1.3829259872436523\n",
      "Epoch 132/2, Step 200/600, Loss = 1.2943233251571655\n",
      "Epoch 132/2, Step 300/600, Loss = 1.3904768228530884\n",
      "Epoch 132/2, Step 400/600, Loss = 1.1972460746765137\n",
      "Epoch 132/2, Step 500/600, Loss = 1.125728726387024\n",
      "Epoch 132/2, Step 600/600, Loss = 1.0503937005996704\n",
      "Epoch 133/2, Step 100/600, Loss = 1.2865720987319946\n",
      "Epoch 133/2, Step 200/600, Loss = 1.1195122003555298\n",
      "Epoch 133/2, Step 300/600, Loss = 1.0243679285049438\n",
      "Epoch 133/2, Step 400/600, Loss = 1.222158432006836\n",
      "Epoch 133/2, Step 500/600, Loss = 1.1251733303070068\n",
      "Epoch 133/2, Step 600/600, Loss = 1.098484754562378\n",
      "Epoch 134/2, Step 100/600, Loss = 1.0416301488876343\n",
      "Epoch 134/2, Step 200/600, Loss = 1.18071448802948\n",
      "Epoch 134/2, Step 300/600, Loss = 1.4911465644836426\n",
      "Epoch 134/2, Step 400/600, Loss = 1.0304076671600342\n",
      "Epoch 134/2, Step 500/600, Loss = 1.1119945049285889\n",
      "Epoch 134/2, Step 600/600, Loss = 1.034232258796692\n",
      "Epoch 135/2, Step 100/600, Loss = 1.2132830619812012\n",
      "Epoch 135/2, Step 200/600, Loss = 0.9041855335235596\n",
      "Epoch 135/2, Step 300/600, Loss = 1.2977746725082397\n",
      "Epoch 135/2, Step 400/600, Loss = 1.0435404777526855\n",
      "Epoch 135/2, Step 500/600, Loss = 1.2607557773590088\n",
      "Epoch 135/2, Step 600/600, Loss = 0.9320670366287231\n",
      "Epoch 136/2, Step 100/600, Loss = 1.1915870904922485\n",
      "Epoch 136/2, Step 200/600, Loss = 1.0273414850234985\n",
      "Epoch 136/2, Step 300/600, Loss = 1.0501269102096558\n",
      "Epoch 136/2, Step 400/600, Loss = 1.119990587234497\n",
      "Epoch 136/2, Step 500/600, Loss = 1.1824095249176025\n",
      "Epoch 136/2, Step 600/600, Loss = 1.0864462852478027\n",
      "Epoch 137/2, Step 100/600, Loss = 1.0646263360977173\n",
      "Epoch 137/2, Step 200/600, Loss = 1.2496929168701172\n",
      "Epoch 137/2, Step 300/600, Loss = 1.0207858085632324\n",
      "Epoch 137/2, Step 400/600, Loss = 1.3947466611862183\n",
      "Epoch 137/2, Step 500/600, Loss = 1.2226680517196655\n",
      "Epoch 137/2, Step 600/600, Loss = 1.1095365285873413\n",
      "Epoch 138/2, Step 100/600, Loss = 1.0456393957138062\n",
      "Epoch 138/2, Step 200/600, Loss = 1.0698119401931763\n",
      "Epoch 138/2, Step 300/600, Loss = 1.166853666305542\n",
      "Epoch 138/2, Step 400/600, Loss = 1.1210620403289795\n",
      "Epoch 138/2, Step 500/600, Loss = 1.3521723747253418\n",
      "Epoch 138/2, Step 600/600, Loss = 1.2136839628219604\n",
      "Epoch 139/2, Step 100/600, Loss = 1.1335511207580566\n",
      "Epoch 139/2, Step 200/600, Loss = 1.2536982297897339\n",
      "Epoch 139/2, Step 300/600, Loss = 0.9985596537590027\n",
      "Epoch 139/2, Step 400/600, Loss = 1.0793848037719727\n",
      "Epoch 139/2, Step 500/600, Loss = 1.1473824977874756\n",
      "Epoch 139/2, Step 600/600, Loss = 1.1756478548049927\n",
      "Epoch 140/2, Step 100/600, Loss = 1.2361572980880737\n",
      "Epoch 140/2, Step 200/600, Loss = 1.094132661819458\n",
      "Epoch 140/2, Step 300/600, Loss = 0.9738401770591736\n",
      "Epoch 140/2, Step 400/600, Loss = 1.1800965070724487\n",
      "Epoch 140/2, Step 500/600, Loss = 1.2730231285095215\n",
      "Epoch 140/2, Step 600/600, Loss = 1.018224835395813\n",
      "Epoch 141/2, Step 100/600, Loss = 1.2341639995574951\n",
      "Epoch 141/2, Step 200/600, Loss = 1.0333207845687866\n",
      "Epoch 141/2, Step 300/600, Loss = 1.3870291709899902\n",
      "Epoch 141/2, Step 400/600, Loss = 1.2132821083068848\n",
      "Epoch 141/2, Step 500/600, Loss = 1.3050148487091064\n",
      "Epoch 141/2, Step 600/600, Loss = 0.997501015663147\n",
      "Epoch 142/2, Step 100/600, Loss = 1.3193330764770508\n",
      "Epoch 142/2, Step 200/600, Loss = 1.3172357082366943\n",
      "Epoch 142/2, Step 300/600, Loss = 1.3333957195281982\n",
      "Epoch 142/2, Step 400/600, Loss = 1.2446669340133667\n",
      "Epoch 142/2, Step 500/600, Loss = 1.2386672496795654\n",
      "Epoch 142/2, Step 600/600, Loss = 0.9944078326225281\n",
      "Epoch 143/2, Step 100/600, Loss = 1.3145158290863037\n",
      "Epoch 143/2, Step 200/600, Loss = 1.359148383140564\n",
      "Epoch 143/2, Step 300/600, Loss = 1.0515986680984497\n",
      "Epoch 143/2, Step 400/600, Loss = 1.0801550149917603\n",
      "Epoch 143/2, Step 500/600, Loss = 1.046901822090149\n",
      "Epoch 143/2, Step 600/600, Loss = 1.2093522548675537\n",
      "Epoch 144/2, Step 100/600, Loss = 1.2091902494430542\n",
      "Epoch 144/2, Step 200/600, Loss = 1.3072932958602905\n",
      "Epoch 144/2, Step 300/600, Loss = 1.1951725482940674\n",
      "Epoch 144/2, Step 400/600, Loss = 1.2403814792633057\n",
      "Epoch 144/2, Step 500/600, Loss = 1.0948610305786133\n",
      "Epoch 144/2, Step 600/600, Loss = 1.31231689453125\n",
      "Epoch 145/2, Step 100/600, Loss = 1.2031632661819458\n",
      "Epoch 145/2, Step 200/600, Loss = 1.2033263444900513\n",
      "Epoch 145/2, Step 300/600, Loss = 1.0942308902740479\n",
      "Epoch 145/2, Step 400/600, Loss = 0.9675359129905701\n",
      "Epoch 145/2, Step 500/600, Loss = 1.801308035850525\n",
      "Epoch 145/2, Step 600/600, Loss = 1.1572340726852417\n",
      "Epoch 146/2, Step 100/600, Loss = 1.1104809045791626\n",
      "Epoch 146/2, Step 200/600, Loss = 1.1997987031936646\n",
      "Epoch 146/2, Step 300/600, Loss = 1.2436091899871826\n",
      "Epoch 146/2, Step 400/600, Loss = 1.105444312095642\n",
      "Epoch 146/2, Step 500/600, Loss = 1.122992753982544\n",
      "Epoch 146/2, Step 600/600, Loss = 1.062130093574524\n",
      "Epoch 147/2, Step 100/600, Loss = 1.1248481273651123\n",
      "Epoch 147/2, Step 200/600, Loss = 1.174499273300171\n",
      "Epoch 147/2, Step 300/600, Loss = 1.1245090961456299\n",
      "Epoch 147/2, Step 400/600, Loss = 1.2787727117538452\n",
      "Epoch 147/2, Step 500/600, Loss = 1.0317386388778687\n",
      "Epoch 147/2, Step 600/600, Loss = 1.2237049341201782\n",
      "Epoch 148/2, Step 100/600, Loss = 1.137756586074829\n",
      "Epoch 148/2, Step 200/600, Loss = 1.3253456354141235\n",
      "Epoch 148/2, Step 300/600, Loss = 1.2172574996948242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader)):\n\u001b[1;32m----> 2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torchvision\\transforms\\functional.py:170\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 170\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sequence_length = 28\n",
    "input_size = 28\n",
    "\n",
    "for epoch in range(len(train_loader)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1)% 100 ==0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(train_loader)}, Loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 784).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape(0)\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy = {acc}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riemann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
