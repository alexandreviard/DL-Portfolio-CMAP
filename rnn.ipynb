{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] La procédure spécifiée est introuvable'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision #for dataset \n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset= train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset= test_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(PortfolioRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        # x -> (batch_size, timesteps/seq, input_size/feature_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X, r):\n",
    "        h0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(X,h0)\n",
    "        # out -> (batch_size, seq_length, hidden_size)\n",
    "        #out = out[:,-1,:] only take the last time step NO WE NEED ALL TIME STEPS\n",
    "        out = self.linear(out)\n",
    "        out = torch.softmax(out, dim=-1)\n",
    "        portfolio_returns = torch.sum(out[:, :, :] * r[:, :, :], dim=-1)\n",
    "        # portfolio_returns -> (batch_size)\n",
    "        sharpe = torch.mean(portfolio_returns, dim = -1) / torch.std(portfolio_returns, dim = -1)\n",
    "        return -sharpe.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output loss (negative Sharpe ratio): -2.727998733520508\n"
     ]
    }
   ],
   "source": [
    "model = PortfolioRNN(5, 1, 1, 2).to(device)\n",
    "\n",
    "# Generate random input data\n",
    "# X -> (batch_size, seq_length, input_size)\n",
    "X = torch.rand(2, 10, 5).to(device)\n",
    "\n",
    "# r -> (batch_size, seq_length, output_size)\n",
    "r = torch.rand(2, 10, 2).to(device)\n",
    "\n",
    "# Call the forward method\n",
    "loss = model(X, r)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output loss (negative Sharpe ratio):\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  4 of 4 completed\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_14048\\121768413.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_R'] = data_na[f'{column}'].pct_change()\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_14048\\121768413.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_R'] = data_na[f'{column}'].pct_change()\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_14048\\121768413.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_R'] = data_na[f'{column}'].pct_change()\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_14048\\121768413.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na[f'{column}_R'] = data_na[f'{column}'].pct_change()\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_14048\\121768413.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na.dropna(axis=0, inplace=True)\n",
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_14048\\121768413.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_na['Date'] = data_na['Date'].dt.date\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>AGG</th>\n",
       "      <th>DBC</th>\n",
       "      <th>VTI</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>AGG_R</th>\n",
       "      <th>DBC_R</th>\n",
       "      <th>VTI_R</th>\n",
       "      <th>^VIX_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-02-07</td>\n",
       "      <td>56.438545</td>\n",
       "      <td>20.285255</td>\n",
       "      <td>44.219509</td>\n",
       "      <td>13.590000</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>-0.028926</td>\n",
       "      <td>-0.009736</td>\n",
       "      <td>0.042178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-02-08</td>\n",
       "      <td>56.410370</td>\n",
       "      <td>20.198933</td>\n",
       "      <td>44.537621</td>\n",
       "      <td>12.830000</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>-0.055923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-02-09</td>\n",
       "      <td>56.444210</td>\n",
       "      <td>20.388840</td>\n",
       "      <td>44.452801</td>\n",
       "      <td>13.120000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>-0.001904</td>\n",
       "      <td>0.022603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-02-10</td>\n",
       "      <td>56.325832</td>\n",
       "      <td>20.017662</td>\n",
       "      <td>44.544701</td>\n",
       "      <td>12.870000</td>\n",
       "      <td>-0.002097</td>\n",
       "      <td>-0.018205</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>-0.019055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-02-13</td>\n",
       "      <td>56.365330</td>\n",
       "      <td>19.706909</td>\n",
       "      <td>44.343197</td>\n",
       "      <td>13.350000</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>-0.015524</td>\n",
       "      <td>-0.004524</td>\n",
       "      <td>0.037296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>106.686798</td>\n",
       "      <td>13.794738</td>\n",
       "      <td>182.168091</td>\n",
       "      <td>23.309999</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>0.012517</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>-0.037969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>106.786331</td>\n",
       "      <td>13.832635</td>\n",
       "      <td>182.472778</td>\n",
       "      <td>21.530001</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>-0.076362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>106.804459</td>\n",
       "      <td>13.747366</td>\n",
       "      <td>183.627289</td>\n",
       "      <td>21.700001</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>-0.006164</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.007896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>106.822563</td>\n",
       "      <td>13.785263</td>\n",
       "      <td>182.860779</td>\n",
       "      <td>23.080000</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>-0.004174</td>\n",
       "      <td>0.063594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>106.885895</td>\n",
       "      <td>13.870533</td>\n",
       "      <td>183.352859</td>\n",
       "      <td>22.770000</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>-0.013432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3751 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker        Date         AGG        DBC         VTI       ^VIX     AGG_R  \\\n",
       "0       2006-02-07   56.438545  20.285255   44.219509  13.590000 -0.000699   \n",
       "1       2006-02-08   56.410370  20.198933   44.537621  12.830000 -0.000499   \n",
       "2       2006-02-09   56.444210  20.388840   44.452801  13.120000  0.000600   \n",
       "3       2006-02-10   56.325832  20.017662   44.544701  12.870000 -0.002097   \n",
       "4       2006-02-13   56.365330  19.706909   44.343197  13.350000  0.000701   \n",
       "...            ...         ...        ...         ...        ...       ...   \n",
       "3746    2020-12-23  106.686798  13.794738  182.168091  23.309999 -0.000678   \n",
       "3747    2020-12-24  106.786331  13.832635  182.472778  21.530001  0.000933   \n",
       "3748    2020-12-28  106.804459  13.747366  183.627289  21.700001  0.000170   \n",
       "3749    2020-12-29  106.822563  13.785263  182.860779  23.080000  0.000170   \n",
       "3750    2020-12-30  106.885895  13.870533  183.352859  22.770000  0.000593   \n",
       "\n",
       "Ticker     DBC_R     VTI_R    ^VIX_R  \n",
       "0      -0.028926 -0.009736  0.042178  \n",
       "1      -0.004255  0.007194 -0.055923  \n",
       "2       0.009402 -0.001904  0.022603  \n",
       "3      -0.018205  0.002067 -0.019055  \n",
       "4      -0.015524 -0.004524  0.037296  \n",
       "...          ...       ...       ...  \n",
       "3746    0.012517  0.001711 -0.037969  \n",
       "3747    0.002747  0.001673 -0.076362  \n",
       "3748   -0.006164  0.006327  0.007896  \n",
       "3749    0.002757 -0.004174  0.063594  \n",
       "3750    0.006186  0.002691 -0.013432  \n",
       "\n",
       "[3751 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "tickers = ['VTI', 'AGG', 'DBC', '^VIX']\n",
    "data = yf.download(tickers, start=\"2006-01-01\", end=\"2020-12-31\", interval=\"1d\")['Adj Close']\n",
    "data_na = data.dropna(axis = 0)\n",
    "for column in data_na.columns:\n",
    "    data_na[f'{column}_R'] = data_na[f'{column}'].pct_change()\n",
    "    data_na[f'{column}_y'] = data_na[f'{column}_R'].shift(-1)\n",
    "data_na.dropna(axis=0, inplace=True)\n",
    "data_na.reset_index(inplace=True)\n",
    "data_na['Date'] = data_na['Date'].dt.date\n",
    "data_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahah\\AppData\\Local\\Temp\\ipykernel_14048\\3333825469.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n"
     ]
    }
   ],
   "source": [
    "def create_batches(dataframe, start_date, end_date, window_size=50):\n",
    "    dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n",
    "    \n",
    "    filtered_data = dataframe[(dataframe['Date'] >= start_date) & (dataframe['Date'] <= end_date)]\n",
    "    \n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    dates_batches = []\n",
    "\n",
    "    for i in range(len(filtered_data) - window_size):\n",
    "        window_returns = filtered_data.iloc[i:i+window_size][[col for col in dataframe.columns if col.endswith('_R')]].values\n",
    "        window_prices = filtered_data.iloc[i:i+window_size][[col for col in dataframe.columns if col in ['AGG', 'DBC', 'VTI', '^VIX']]].values\n",
    "        \n",
    "        window_x = np.concatenate([window_returns, window_prices], axis=1)\n",
    "        window_y = filtered_data.iloc[i:i+window_size][[col for col in dataframe.columns if col.endswith('_y')]].values\n",
    "        \n",
    "        window_dates = filtered_data.iloc[i:i+window_size]['Date'].values\n",
    "\n",
    "        x_batches.append(window_x)\n",
    "        y_batches.append(window_y)\n",
    "        dates_batches.append(window_dates)\n",
    "\n",
    "    return np.array(x_batches), np.array(y_batches), np.array(dates_batches)\n",
    "\n",
    "x_batches, y_batches, dates_batches = create_batches(data_na, start_date=\"2006-01-01\", end_date=\"2010-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 64\n",
    "\n",
    "x_batches_tensor = torch.tensor(x_batches, dtype=torch.float32)\n",
    "y_batches_tensor = torch.tensor(y_batches, dtype=torch.float32)\n",
    "\n",
    "num_batches = x_batches_tensor.shape[0]\n",
    "\n",
    "input_size = 8\n",
    "hidden_size = 64\n",
    "output_size = 4\n",
    "model_rnn = PortfolioRNN(input_size=input_size, hidden_size=hidden_size, num_layers=2, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "x_mini_batch shape: torch.Size([64, 50, 8])\n",
      "y_mini_batch shape: torch.Size([64, 50, 0])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_mini_batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_mini_batch\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mini_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mini_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 17\u001b[0m, in \u001b[0;36mPortfolioRNN.forward\u001b[1;34m(self, X, r)\u001b[0m\n\u001b[0;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n\u001b[0;32m     16\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m portfolio_returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# portfolio_returns -> (batch_size)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m sharpe \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(portfolio_returns, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mstd(portfolio_returns, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_rnn.train()\n",
    "    loss_epoch = 0\n",
    "    \n",
    "    for i in range(0, num_batches - mini_batch_size + 1, mini_batch_size):\n",
    "        x_mini_batch = x_batches_tensor[i:i+mini_batch_size]\n",
    "        y_mini_batch = y_batches_tensor[i:i+mini_batch_size]\n",
    "        \n",
    "        # Check the shapes\n",
    "        print(f\"Batch {i // mini_batch_size + 1}:\")\n",
    "        print(f\"x_mini_batch shape: {x_mini_batch.shape}\")\n",
    "        print(f\"y_mini_batch shape: {y_mini_batch.shape}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model_rnn(x_mini_batch, y_mini_batch)\n",
    "        loss_epoch += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"epoch [{epoch+1}/{num_epochs}], loss: {(loss_epoch/mini_batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PortfolioRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Step 100/600, Loss = 1.7561472654342651\n",
      "Epoch 1/2, Step 200/600, Loss = 1.1913491487503052\n",
      "Epoch 1/2, Step 300/600, Loss = 0.4377589523792267\n",
      "Epoch 1/2, Step 400/600, Loss = 1.1186847686767578\n",
      "Epoch 1/2, Step 500/600, Loss = 1.1297461986541748\n",
      "Epoch 1/2, Step 600/600, Loss = 1.624846339225769\n",
      "Epoch 2/2, Step 100/600, Loss = 0.710981011390686\n",
      "Epoch 2/2, Step 200/600, Loss = 0.7087716460227966\n",
      "Epoch 2/2, Step 300/600, Loss = 1.1653952598571777\n",
      "Epoch 2/2, Step 400/600, Loss = 1.0352438688278198\n",
      "Epoch 2/2, Step 500/600, Loss = 0.6385011076927185\n",
      "Epoch 2/2, Step 600/600, Loss = 0.748671293258667\n",
      "Epoch 3/2, Step 100/600, Loss = 1.1222331523895264\n",
      "Epoch 3/2, Step 200/600, Loss = 0.8393617272377014\n",
      "Epoch 3/2, Step 300/600, Loss = 0.7829921245574951\n",
      "Epoch 3/2, Step 400/600, Loss = 0.897032618522644\n",
      "Epoch 3/2, Step 500/600, Loss = 0.9496373534202576\n",
      "Epoch 3/2, Step 600/600, Loss = 0.8489352464675903\n",
      "Epoch 4/2, Step 100/600, Loss = 0.5647134780883789\n",
      "Epoch 4/2, Step 200/600, Loss = 1.4857374429702759\n",
      "Epoch 4/2, Step 300/600, Loss = 2.0306169986724854\n",
      "Epoch 4/2, Step 400/600, Loss = 0.7122843861579895\n",
      "Epoch 4/2, Step 500/600, Loss = 0.6505269408226013\n",
      "Epoch 4/2, Step 600/600, Loss = 1.3644652366638184\n",
      "Epoch 5/2, Step 100/600, Loss = 0.7575669884681702\n",
      "Epoch 5/2, Step 200/600, Loss = 0.7221808433532715\n",
      "Epoch 5/2, Step 300/600, Loss = 1.284300684928894\n",
      "Epoch 5/2, Step 400/600, Loss = 0.6143010258674622\n",
      "Epoch 5/2, Step 500/600, Loss = 0.6969509124755859\n",
      "Epoch 5/2, Step 600/600, Loss = 1.0164573192596436\n",
      "Epoch 6/2, Step 100/600, Loss = 0.7617648243904114\n",
      "Epoch 6/2, Step 200/600, Loss = 0.7624671459197998\n",
      "Epoch 6/2, Step 300/600, Loss = 0.9380752444267273\n",
      "Epoch 6/2, Step 400/600, Loss = 0.5861667394638062\n",
      "Epoch 6/2, Step 500/600, Loss = 0.5562633872032166\n",
      "Epoch 6/2, Step 600/600, Loss = 1.563233494758606\n",
      "Epoch 7/2, Step 100/600, Loss = 0.8195065259933472\n",
      "Epoch 7/2, Step 200/600, Loss = 0.7155972123146057\n",
      "Epoch 7/2, Step 300/600, Loss = 0.5961833596229553\n",
      "Epoch 7/2, Step 400/600, Loss = 0.6594519019126892\n",
      "Epoch 7/2, Step 500/600, Loss = 0.605135440826416\n",
      "Epoch 7/2, Step 600/600, Loss = 0.5903542041778564\n",
      "Epoch 8/2, Step 100/600, Loss = 0.7346264123916626\n",
      "Epoch 8/2, Step 200/600, Loss = 0.5904850363731384\n",
      "Epoch 8/2, Step 300/600, Loss = 0.7969841957092285\n",
      "Epoch 8/2, Step 400/600, Loss = 0.8449658155441284\n",
      "Epoch 8/2, Step 500/600, Loss = 0.42836788296699524\n",
      "Epoch 8/2, Step 600/600, Loss = 0.5024669170379639\n",
      "Epoch 9/2, Step 100/600, Loss = 0.5732094049453735\n",
      "Epoch 9/2, Step 200/600, Loss = 1.3130611181259155\n",
      "Epoch 9/2, Step 300/600, Loss = 0.5626366138458252\n",
      "Epoch 9/2, Step 400/600, Loss = 0.650200366973877\n",
      "Epoch 9/2, Step 500/600, Loss = 0.8351550102233887\n",
      "Epoch 9/2, Step 600/600, Loss = 0.6711447238922119\n",
      "Epoch 10/2, Step 100/600, Loss = 0.5907195806503296\n",
      "Epoch 10/2, Step 200/600, Loss = 0.6687739491462708\n",
      "Epoch 10/2, Step 300/600, Loss = 1.0175180435180664\n",
      "Epoch 10/2, Step 400/600, Loss = 0.8386268019676208\n",
      "Epoch 10/2, Step 500/600, Loss = 1.9594967365264893\n",
      "Epoch 10/2, Step 600/600, Loss = 1.8622478246688843\n",
      "Epoch 11/2, Step 100/600, Loss = 0.739504873752594\n",
      "Epoch 11/2, Step 200/600, Loss = 1.1423866748809814\n",
      "Epoch 11/2, Step 300/600, Loss = 1.3713940382003784\n",
      "Epoch 11/2, Step 400/600, Loss = 0.4809513986110687\n",
      "Epoch 11/2, Step 500/600, Loss = 1.4489398002624512\n",
      "Epoch 11/2, Step 600/600, Loss = 0.7419864535331726\n",
      "Epoch 12/2, Step 100/600, Loss = 0.9915297031402588\n",
      "Epoch 12/2, Step 200/600, Loss = 0.6670812964439392\n",
      "Epoch 12/2, Step 300/600, Loss = 0.7344872951507568\n",
      "Epoch 12/2, Step 400/600, Loss = 0.8222957849502563\n",
      "Epoch 12/2, Step 500/600, Loss = 0.6569140553474426\n",
      "Epoch 12/2, Step 600/600, Loss = 0.7752962708473206\n",
      "Epoch 13/2, Step 100/600, Loss = 0.9757022261619568\n",
      "Epoch 13/2, Step 200/600, Loss = 0.5459726452827454\n",
      "Epoch 13/2, Step 300/600, Loss = 0.930605411529541\n",
      "Epoch 13/2, Step 400/600, Loss = 0.49197474122047424\n",
      "Epoch 13/2, Step 500/600, Loss = 0.8342377543449402\n",
      "Epoch 13/2, Step 600/600, Loss = 0.8709094524383545\n",
      "Epoch 14/2, Step 100/600, Loss = 0.5298974514007568\n",
      "Epoch 14/2, Step 200/600, Loss = 0.9134870767593384\n",
      "Epoch 14/2, Step 300/600, Loss = 0.7345489263534546\n",
      "Epoch 14/2, Step 400/600, Loss = 0.8496286869049072\n",
      "Epoch 14/2, Step 500/600, Loss = 1.4958851337432861\n",
      "Epoch 14/2, Step 600/600, Loss = 0.5591602921485901\n",
      "Epoch 15/2, Step 100/600, Loss = 0.6103951334953308\n",
      "Epoch 15/2, Step 200/600, Loss = 1.1229645013809204\n",
      "Epoch 15/2, Step 300/600, Loss = 1.2025777101516724\n",
      "Epoch 15/2, Step 400/600, Loss = 1.0284849405288696\n",
      "Epoch 15/2, Step 500/600, Loss = 0.6610007286071777\n",
      "Epoch 15/2, Step 600/600, Loss = 0.768892765045166\n",
      "Epoch 16/2, Step 100/600, Loss = 0.8539714217185974\n",
      "Epoch 16/2, Step 200/600, Loss = 0.8649458289146423\n",
      "Epoch 16/2, Step 300/600, Loss = 0.7634660601615906\n",
      "Epoch 16/2, Step 400/600, Loss = 0.8506960868835449\n",
      "Epoch 16/2, Step 500/600, Loss = 1.6302151679992676\n",
      "Epoch 16/2, Step 600/600, Loss = 2.000943183898926\n",
      "Epoch 17/2, Step 100/600, Loss = 0.6317209601402283\n",
      "Epoch 17/2, Step 200/600, Loss = 0.7260487079620361\n",
      "Epoch 17/2, Step 300/600, Loss = 0.6685819029808044\n",
      "Epoch 17/2, Step 400/600, Loss = 0.7773380279541016\n",
      "Epoch 17/2, Step 500/600, Loss = 0.6713677048683167\n",
      "Epoch 17/2, Step 600/600, Loss = 0.6607370972633362\n",
      "Epoch 18/2, Step 100/600, Loss = 0.7722377181053162\n",
      "Epoch 18/2, Step 200/600, Loss = 1.1283657550811768\n",
      "Epoch 18/2, Step 300/600, Loss = 0.5733255743980408\n",
      "Epoch 18/2, Step 400/600, Loss = 0.6588343977928162\n",
      "Epoch 18/2, Step 500/600, Loss = 0.6263440251350403\n",
      "Epoch 18/2, Step 600/600, Loss = 1.1657681465148926\n",
      "Epoch 19/2, Step 100/600, Loss = 0.7161742448806763\n",
      "Epoch 19/2, Step 200/600, Loss = 0.6132200360298157\n",
      "Epoch 19/2, Step 300/600, Loss = 0.5802550315856934\n",
      "Epoch 19/2, Step 400/600, Loss = 0.9011296033859253\n",
      "Epoch 19/2, Step 500/600, Loss = 0.49279919266700745\n",
      "Epoch 19/2, Step 600/600, Loss = 0.9239047765731812\n",
      "Epoch 20/2, Step 100/600, Loss = 1.531754493713379\n",
      "Epoch 20/2, Step 200/600, Loss = 1.2620530128479004\n",
      "Epoch 20/2, Step 300/600, Loss = 0.6601241230964661\n",
      "Epoch 20/2, Step 400/600, Loss = 0.8971971273422241\n",
      "Epoch 20/2, Step 500/600, Loss = 0.6400142908096313\n",
      "Epoch 20/2, Step 600/600, Loss = 0.5631607174873352\n",
      "Epoch 21/2, Step 100/600, Loss = 0.39439907670021057\n",
      "Epoch 21/2, Step 200/600, Loss = 0.6170299649238586\n",
      "Epoch 21/2, Step 300/600, Loss = 0.6768676042556763\n",
      "Epoch 21/2, Step 400/600, Loss = 0.5733846426010132\n",
      "Epoch 21/2, Step 500/600, Loss = 0.8819712996482849\n",
      "Epoch 21/2, Step 600/600, Loss = 3.2813897132873535\n",
      "Epoch 22/2, Step 100/600, Loss = 0.5047858953475952\n",
      "Epoch 22/2, Step 200/600, Loss = 0.6917144060134888\n",
      "Epoch 22/2, Step 300/600, Loss = 1.1757091283798218\n",
      "Epoch 22/2, Step 400/600, Loss = 0.7386131286621094\n",
      "Epoch 22/2, Step 500/600, Loss = 0.8562155365943909\n",
      "Epoch 22/2, Step 600/600, Loss = 0.8531270027160645\n",
      "Epoch 23/2, Step 100/600, Loss = 0.8660126328468323\n",
      "Epoch 23/2, Step 200/600, Loss = 0.600343644618988\n",
      "Epoch 23/2, Step 300/600, Loss = 0.7649305462837219\n",
      "Epoch 23/2, Step 400/600, Loss = 0.6384910345077515\n",
      "Epoch 23/2, Step 500/600, Loss = 1.0258156061172485\n",
      "Epoch 23/2, Step 600/600, Loss = 0.6553298830986023\n",
      "Epoch 24/2, Step 100/600, Loss = 0.9524953961372375\n",
      "Epoch 24/2, Step 200/600, Loss = 1.0845978260040283\n",
      "Epoch 24/2, Step 300/600, Loss = 0.5988004207611084\n",
      "Epoch 24/2, Step 400/600, Loss = 0.7168669104576111\n",
      "Epoch 24/2, Step 500/600, Loss = 0.7573811411857605\n",
      "Epoch 24/2, Step 600/600, Loss = 0.8024942874908447\n",
      "Epoch 25/2, Step 100/600, Loss = 0.8617520928382874\n",
      "Epoch 25/2, Step 200/600, Loss = 0.7465834617614746\n",
      "Epoch 25/2, Step 300/600, Loss = 0.7329591512680054\n",
      "Epoch 25/2, Step 400/600, Loss = 0.5433840155601501\n",
      "Epoch 25/2, Step 500/600, Loss = 0.6190046072006226\n",
      "Epoch 25/2, Step 600/600, Loss = 0.6118175387382507\n",
      "Epoch 26/2, Step 100/600, Loss = 1.5131378173828125\n",
      "Epoch 26/2, Step 200/600, Loss = 0.7999809384346008\n",
      "Epoch 26/2, Step 300/600, Loss = 0.7084048986434937\n",
      "Epoch 26/2, Step 400/600, Loss = 0.8253446221351624\n",
      "Epoch 26/2, Step 500/600, Loss = 0.8204764723777771\n",
      "Epoch 26/2, Step 600/600, Loss = 0.8822677731513977\n",
      "Epoch 27/2, Step 100/600, Loss = 0.7891409397125244\n",
      "Epoch 27/2, Step 200/600, Loss = 0.7617610096931458\n",
      "Epoch 27/2, Step 300/600, Loss = 0.8034088015556335\n",
      "Epoch 27/2, Step 400/600, Loss = 0.6653174757957458\n",
      "Epoch 27/2, Step 500/600, Loss = 0.8489576578140259\n",
      "Epoch 27/2, Step 600/600, Loss = 1.1742459535598755\n",
      "Epoch 28/2, Step 100/600, Loss = 0.710274338722229\n",
      "Epoch 28/2, Step 200/600, Loss = 0.8989841938018799\n",
      "Epoch 28/2, Step 300/600, Loss = 0.8624563813209534\n",
      "Epoch 28/2, Step 400/600, Loss = 0.8329631090164185\n",
      "Epoch 28/2, Step 500/600, Loss = 0.6513237953186035\n",
      "Epoch 28/2, Step 600/600, Loss = 1.076900601387024\n",
      "Epoch 29/2, Step 100/600, Loss = 0.888508677482605\n",
      "Epoch 29/2, Step 200/600, Loss = 0.907931923866272\n",
      "Epoch 29/2, Step 300/600, Loss = 0.7209890484809875\n",
      "Epoch 29/2, Step 400/600, Loss = 0.7701407670974731\n",
      "Epoch 29/2, Step 500/600, Loss = 0.5986765027046204\n",
      "Epoch 29/2, Step 600/600, Loss = 0.8734838962554932\n",
      "Epoch 30/2, Step 100/600, Loss = 0.7614267468452454\n",
      "Epoch 30/2, Step 200/600, Loss = 0.8631970286369324\n",
      "Epoch 30/2, Step 300/600, Loss = 1.1526579856872559\n",
      "Epoch 30/2, Step 400/600, Loss = 0.7819502353668213\n",
      "Epoch 30/2, Step 500/600, Loss = 0.6480865478515625\n",
      "Epoch 30/2, Step 600/600, Loss = 0.887617290019989\n",
      "Epoch 31/2, Step 100/600, Loss = 0.9643345475196838\n",
      "Epoch 31/2, Step 200/600, Loss = 0.595050036907196\n",
      "Epoch 31/2, Step 300/600, Loss = 1.5502982139587402\n",
      "Epoch 31/2, Step 400/600, Loss = 0.7374916672706604\n",
      "Epoch 31/2, Step 500/600, Loss = 0.6865798830986023\n",
      "Epoch 31/2, Step 600/600, Loss = 0.7294139862060547\n",
      "Epoch 32/2, Step 100/600, Loss = 0.6465306282043457\n",
      "Epoch 32/2, Step 200/600, Loss = 0.8552631139755249\n",
      "Epoch 32/2, Step 300/600, Loss = 3.4819915294647217\n",
      "Epoch 32/2, Step 400/600, Loss = 0.7181789875030518\n",
      "Epoch 32/2, Step 500/600, Loss = 0.6711331009864807\n",
      "Epoch 32/2, Step 600/600, Loss = 0.6172187328338623\n",
      "Epoch 33/2, Step 100/600, Loss = 0.7470406293869019\n",
      "Epoch 33/2, Step 200/600, Loss = 0.6817488074302673\n",
      "Epoch 33/2, Step 300/600, Loss = 0.6035128235816956\n",
      "Epoch 33/2, Step 400/600, Loss = 0.5421103835105896\n",
      "Epoch 33/2, Step 500/600, Loss = 0.8045623898506165\n",
      "Epoch 33/2, Step 600/600, Loss = 0.781386137008667\n",
      "Epoch 34/2, Step 100/600, Loss = 0.7818173170089722\n",
      "Epoch 34/2, Step 200/600, Loss = 0.725482702255249\n",
      "Epoch 34/2, Step 300/600, Loss = 0.9114531874656677\n",
      "Epoch 34/2, Step 400/600, Loss = 0.8208680748939514\n",
      "Epoch 34/2, Step 500/600, Loss = 0.9238123297691345\n",
      "Epoch 34/2, Step 600/600, Loss = 0.809952974319458\n",
      "Epoch 35/2, Step 100/600, Loss = 0.7560074329376221\n",
      "Epoch 35/2, Step 200/600, Loss = 0.684143602848053\n",
      "Epoch 35/2, Step 300/600, Loss = 0.652571439743042\n",
      "Epoch 35/2, Step 400/600, Loss = 0.5949814319610596\n",
      "Epoch 35/2, Step 500/600, Loss = 0.8285614848136902\n",
      "Epoch 35/2, Step 600/600, Loss = 0.6646358966827393\n",
      "Epoch 36/2, Step 100/600, Loss = 0.5190450549125671\n",
      "Epoch 36/2, Step 200/600, Loss = 0.723816454410553\n",
      "Epoch 36/2, Step 300/600, Loss = 0.8312334418296814\n",
      "Epoch 36/2, Step 400/600, Loss = 0.8716031908988953\n",
      "Epoch 36/2, Step 500/600, Loss = 0.5928308963775635\n",
      "Epoch 36/2, Step 600/600, Loss = 0.903198778629303\n",
      "Epoch 37/2, Step 100/600, Loss = 0.7850469350814819\n",
      "Epoch 37/2, Step 200/600, Loss = 0.6187580227851868\n",
      "Epoch 37/2, Step 300/600, Loss = 0.9586089253425598\n",
      "Epoch 37/2, Step 400/600, Loss = 0.6985734701156616\n",
      "Epoch 37/2, Step 500/600, Loss = 0.7176287770271301\n",
      "Epoch 37/2, Step 600/600, Loss = 0.5085269808769226\n",
      "Epoch 38/2, Step 100/600, Loss = 0.9902064800262451\n",
      "Epoch 38/2, Step 200/600, Loss = 0.4828740358352661\n",
      "Epoch 38/2, Step 300/600, Loss = 0.7143802642822266\n",
      "Epoch 38/2, Step 400/600, Loss = 0.9414036273956299\n",
      "Epoch 38/2, Step 500/600, Loss = 0.9218670725822449\n",
      "Epoch 38/2, Step 600/600, Loss = 0.9076974987983704\n",
      "Epoch 39/2, Step 100/600, Loss = 0.8396714925765991\n",
      "Epoch 39/2, Step 200/600, Loss = 0.8204292058944702\n",
      "Epoch 39/2, Step 300/600, Loss = 0.9364330172538757\n",
      "Epoch 39/2, Step 400/600, Loss = 0.5672619938850403\n",
      "Epoch 39/2, Step 500/600, Loss = 0.8444033265113831\n",
      "Epoch 39/2, Step 600/600, Loss = 0.9053214192390442\n",
      "Epoch 40/2, Step 100/600, Loss = 0.8409113883972168\n",
      "Epoch 40/2, Step 200/600, Loss = 0.85040283203125\n",
      "Epoch 40/2, Step 300/600, Loss = 0.8429701328277588\n",
      "Epoch 40/2, Step 400/600, Loss = 0.6031198501586914\n",
      "Epoch 40/2, Step 500/600, Loss = 0.6203529834747314\n",
      "Epoch 40/2, Step 600/600, Loss = 0.6934195756912231\n",
      "Epoch 41/2, Step 100/600, Loss = 0.6637219786643982\n",
      "Epoch 41/2, Step 200/600, Loss = 0.738513708114624\n",
      "Epoch 41/2, Step 300/600, Loss = 0.6676951050758362\n",
      "Epoch 41/2, Step 400/600, Loss = 0.8608128428459167\n",
      "Epoch 41/2, Step 500/600, Loss = 0.8691378831863403\n",
      "Epoch 41/2, Step 600/600, Loss = 0.6413754224777222\n",
      "Epoch 42/2, Step 100/600, Loss = 0.9395986795425415\n",
      "Epoch 42/2, Step 200/600, Loss = 0.8113289475440979\n",
      "Epoch 42/2, Step 300/600, Loss = 0.5430511236190796\n",
      "Epoch 42/2, Step 400/600, Loss = 0.9028068780899048\n",
      "Epoch 42/2, Step 500/600, Loss = 0.8118918538093567\n",
      "Epoch 42/2, Step 600/600, Loss = 1.1058297157287598\n",
      "Epoch 43/2, Step 100/600, Loss = 1.0374239683151245\n",
      "Epoch 43/2, Step 200/600, Loss = 0.5511253476142883\n",
      "Epoch 43/2, Step 300/600, Loss = 0.9021445512771606\n",
      "Epoch 43/2, Step 400/600, Loss = 0.6996267437934875\n",
      "Epoch 43/2, Step 500/600, Loss = 0.9565595388412476\n",
      "Epoch 43/2, Step 600/600, Loss = 0.8424228429794312\n",
      "Epoch 44/2, Step 100/600, Loss = 1.6096408367156982\n",
      "Epoch 44/2, Step 200/600, Loss = 0.865861177444458\n",
      "Epoch 44/2, Step 300/600, Loss = 0.7614272236824036\n",
      "Epoch 44/2, Step 400/600, Loss = 1.0585589408874512\n",
      "Epoch 44/2, Step 500/600, Loss = 0.6986647844314575\n",
      "Epoch 44/2, Step 600/600, Loss = 0.7304273843765259\n",
      "Epoch 45/2, Step 100/600, Loss = 0.754163920879364\n",
      "Epoch 45/2, Step 200/600, Loss = 0.8604484796524048\n",
      "Epoch 45/2, Step 300/600, Loss = 0.9806235432624817\n",
      "Epoch 45/2, Step 400/600, Loss = 0.866619884967804\n",
      "Epoch 45/2, Step 500/600, Loss = 0.8604305386543274\n",
      "Epoch 45/2, Step 600/600, Loss = 0.9301736354827881\n",
      "Epoch 46/2, Step 100/600, Loss = 0.7244088649749756\n",
      "Epoch 46/2, Step 200/600, Loss = 0.9225350022315979\n",
      "Epoch 46/2, Step 300/600, Loss = 3.6537821292877197\n",
      "Epoch 46/2, Step 400/600, Loss = 0.9145888686180115\n",
      "Epoch 46/2, Step 500/600, Loss = 0.8453271985054016\n",
      "Epoch 46/2, Step 600/600, Loss = 0.7818326354026794\n",
      "Epoch 47/2, Step 100/600, Loss = 0.8238754868507385\n",
      "Epoch 47/2, Step 200/600, Loss = 0.7648931741714478\n",
      "Epoch 47/2, Step 300/600, Loss = 0.6933243274688721\n",
      "Epoch 47/2, Step 400/600, Loss = 0.8874970078468323\n",
      "Epoch 47/2, Step 500/600, Loss = 0.5355274081230164\n",
      "Epoch 47/2, Step 600/600, Loss = 0.6876716613769531\n",
      "Epoch 48/2, Step 100/600, Loss = 0.6383880376815796\n",
      "Epoch 48/2, Step 200/600, Loss = 0.6246317028999329\n",
      "Epoch 48/2, Step 300/600, Loss = 0.7892712354660034\n",
      "Epoch 48/2, Step 400/600, Loss = 0.7557430863380432\n",
      "Epoch 48/2, Step 500/600, Loss = 0.7588984966278076\n",
      "Epoch 48/2, Step 600/600, Loss = 0.8474327921867371\n",
      "Epoch 49/2, Step 100/600, Loss = 1.0829877853393555\n",
      "Epoch 49/2, Step 200/600, Loss = 0.7474930286407471\n",
      "Epoch 49/2, Step 300/600, Loss = 0.8560986518859863\n",
      "Epoch 49/2, Step 400/600, Loss = 0.9343897104263306\n",
      "Epoch 49/2, Step 500/600, Loss = 0.7338613867759705\n",
      "Epoch 49/2, Step 600/600, Loss = 0.843356728553772\n",
      "Epoch 50/2, Step 100/600, Loss = 0.8779810070991516\n",
      "Epoch 50/2, Step 200/600, Loss = 0.8285555839538574\n",
      "Epoch 50/2, Step 300/600, Loss = 0.9262111186981201\n",
      "Epoch 50/2, Step 400/600, Loss = 0.8336938619613647\n",
      "Epoch 50/2, Step 500/600, Loss = 0.6532850861549377\n",
      "Epoch 50/2, Step 600/600, Loss = 0.8745595812797546\n",
      "Epoch 51/2, Step 100/600, Loss = 0.8605689406394958\n",
      "Epoch 51/2, Step 200/600, Loss = 0.771584153175354\n",
      "Epoch 51/2, Step 300/600, Loss = 0.7493828535079956\n",
      "Epoch 51/2, Step 400/600, Loss = 0.7442831993103027\n",
      "Epoch 51/2, Step 500/600, Loss = 0.7894020676612854\n",
      "Epoch 51/2, Step 600/600, Loss = 0.6373575329780579\n",
      "Epoch 52/2, Step 100/600, Loss = 0.6643072366714478\n",
      "Epoch 52/2, Step 200/600, Loss = 0.8138694763183594\n",
      "Epoch 52/2, Step 300/600, Loss = 2.1739394664764404\n",
      "Epoch 52/2, Step 400/600, Loss = 0.7776638269424438\n",
      "Epoch 52/2, Step 500/600, Loss = 0.8706198334693909\n",
      "Epoch 52/2, Step 600/600, Loss = 0.9089850783348083\n",
      "Epoch 53/2, Step 100/600, Loss = 0.7240298390388489\n",
      "Epoch 53/2, Step 200/600, Loss = 0.8786166310310364\n",
      "Epoch 53/2, Step 300/600, Loss = 0.8061319589614868\n",
      "Epoch 53/2, Step 400/600, Loss = 0.7214257717132568\n",
      "Epoch 53/2, Step 500/600, Loss = 1.0539085865020752\n",
      "Epoch 53/2, Step 600/600, Loss = 0.7684303522109985\n",
      "Epoch 54/2, Step 100/600, Loss = 0.9607977271080017\n",
      "Epoch 54/2, Step 200/600, Loss = 0.772838294506073\n",
      "Epoch 54/2, Step 300/600, Loss = 0.822664737701416\n",
      "Epoch 54/2, Step 400/600, Loss = 0.8233007192611694\n",
      "Epoch 54/2, Step 500/600, Loss = 0.799691915512085\n",
      "Epoch 54/2, Step 600/600, Loss = 0.8335254788398743\n",
      "Epoch 55/2, Step 100/600, Loss = 1.3871296644210815\n",
      "Epoch 55/2, Step 200/600, Loss = 0.7128295302391052\n",
      "Epoch 55/2, Step 300/600, Loss = 0.6869668364524841\n",
      "Epoch 55/2, Step 400/600, Loss = 0.817756712436676\n",
      "Epoch 55/2, Step 500/600, Loss = 0.9235390424728394\n",
      "Epoch 55/2, Step 600/600, Loss = 0.9095311760902405\n",
      "Epoch 56/2, Step 100/600, Loss = 0.7642176151275635\n",
      "Epoch 56/2, Step 200/600, Loss = 0.9066895842552185\n",
      "Epoch 56/2, Step 300/600, Loss = 0.9389721155166626\n",
      "Epoch 56/2, Step 400/600, Loss = 0.7939554452896118\n",
      "Epoch 56/2, Step 500/600, Loss = 0.7562054991722107\n",
      "Epoch 56/2, Step 600/600, Loss = 0.7721818685531616\n",
      "Epoch 57/2, Step 100/600, Loss = 0.7752978801727295\n",
      "Epoch 57/2, Step 200/600, Loss = 0.8923677206039429\n",
      "Epoch 57/2, Step 300/600, Loss = 0.7458853125572205\n",
      "Epoch 57/2, Step 400/600, Loss = 1.1021661758422852\n",
      "Epoch 57/2, Step 500/600, Loss = 0.9921493530273438\n",
      "Epoch 57/2, Step 600/600, Loss = 1.1468864679336548\n",
      "Epoch 58/2, Step 100/600, Loss = 0.821464478969574\n",
      "Epoch 58/2, Step 200/600, Loss = 0.864921510219574\n",
      "Epoch 58/2, Step 300/600, Loss = 0.9042745232582092\n",
      "Epoch 58/2, Step 400/600, Loss = 0.6289334893226624\n",
      "Epoch 58/2, Step 500/600, Loss = 0.9342457056045532\n",
      "Epoch 58/2, Step 600/600, Loss = 1.04622483253479\n",
      "Epoch 59/2, Step 100/600, Loss = 0.95982825756073\n",
      "Epoch 59/2, Step 200/600, Loss = 0.6327556371688843\n",
      "Epoch 59/2, Step 300/600, Loss = 0.6799561381340027\n",
      "Epoch 59/2, Step 400/600, Loss = 0.9146010875701904\n",
      "Epoch 59/2, Step 500/600, Loss = 1.0242719650268555\n",
      "Epoch 59/2, Step 600/600, Loss = 0.9742301106452942\n",
      "Epoch 60/2, Step 100/600, Loss = 0.9237036108970642\n",
      "Epoch 60/2, Step 200/600, Loss = 0.7985848784446716\n",
      "Epoch 60/2, Step 300/600, Loss = 0.8218581676483154\n",
      "Epoch 60/2, Step 400/600, Loss = 0.8857024908065796\n",
      "Epoch 60/2, Step 500/600, Loss = 1.0288262367248535\n",
      "Epoch 60/2, Step 600/600, Loss = 1.7373173236846924\n",
      "Epoch 61/2, Step 100/600, Loss = 0.8786763548851013\n",
      "Epoch 61/2, Step 200/600, Loss = 0.9589526653289795\n",
      "Epoch 61/2, Step 300/600, Loss = 0.7869260907173157\n",
      "Epoch 61/2, Step 400/600, Loss = 0.6346504092216492\n",
      "Epoch 61/2, Step 500/600, Loss = 0.7409112453460693\n",
      "Epoch 61/2, Step 600/600, Loss = 0.6607446074485779\n",
      "Epoch 62/2, Step 100/600, Loss = 0.9944191575050354\n",
      "Epoch 62/2, Step 200/600, Loss = 0.9073131084442139\n",
      "Epoch 62/2, Step 300/600, Loss = 0.7909668684005737\n",
      "Epoch 62/2, Step 400/600, Loss = 0.8969340324401855\n",
      "Epoch 62/2, Step 500/600, Loss = 0.782854437828064\n",
      "Epoch 62/2, Step 600/600, Loss = 0.9825572371482849\n",
      "Epoch 63/2, Step 100/600, Loss = 0.9607381224632263\n",
      "Epoch 63/2, Step 200/600, Loss = 0.626170814037323\n",
      "Epoch 63/2, Step 300/600, Loss = 1.1679586172103882\n",
      "Epoch 63/2, Step 400/600, Loss = 0.8676430583000183\n",
      "Epoch 63/2, Step 500/600, Loss = 1.029709815979004\n",
      "Epoch 63/2, Step 600/600, Loss = 0.8421533703804016\n",
      "Epoch 64/2, Step 100/600, Loss = 0.9655473828315735\n",
      "Epoch 64/2, Step 200/600, Loss = 1.102617621421814\n",
      "Epoch 64/2, Step 300/600, Loss = 0.9997897148132324\n",
      "Epoch 64/2, Step 400/600, Loss = 1.0044769048690796\n",
      "Epoch 64/2, Step 500/600, Loss = 1.0231014490127563\n",
      "Epoch 64/2, Step 600/600, Loss = 0.8460622429847717\n",
      "Epoch 65/2, Step 100/600, Loss = 0.9563268423080444\n",
      "Epoch 65/2, Step 200/600, Loss = 1.0712289810180664\n",
      "Epoch 65/2, Step 300/600, Loss = 1.0145468711853027\n",
      "Epoch 65/2, Step 400/600, Loss = 0.936400294303894\n",
      "Epoch 65/2, Step 500/600, Loss = 0.8708556890487671\n",
      "Epoch 65/2, Step 600/600, Loss = 0.9188586473464966\n",
      "Epoch 66/2, Step 100/600, Loss = 0.9151170253753662\n",
      "Epoch 66/2, Step 200/600, Loss = 1.0221091508865356\n",
      "Epoch 66/2, Step 300/600, Loss = 0.9822889566421509\n",
      "Epoch 66/2, Step 400/600, Loss = 0.9971446394920349\n",
      "Epoch 66/2, Step 500/600, Loss = 0.9323722124099731\n",
      "Epoch 66/2, Step 600/600, Loss = 1.0345121622085571\n",
      "Epoch 67/2, Step 100/600, Loss = 0.8229498863220215\n",
      "Epoch 67/2, Step 200/600, Loss = 0.8262150287628174\n",
      "Epoch 67/2, Step 300/600, Loss = 0.9133915901184082\n",
      "Epoch 67/2, Step 400/600, Loss = 1.0396239757537842\n",
      "Epoch 67/2, Step 500/600, Loss = 0.7756903171539307\n",
      "Epoch 67/2, Step 600/600, Loss = 1.0034682750701904\n",
      "Epoch 68/2, Step 100/600, Loss = 0.8286200165748596\n",
      "Epoch 68/2, Step 200/600, Loss = 0.989119291305542\n",
      "Epoch 68/2, Step 300/600, Loss = 0.7903922200202942\n",
      "Epoch 68/2, Step 400/600, Loss = 1.0311951637268066\n",
      "Epoch 68/2, Step 500/600, Loss = 0.9589021801948547\n",
      "Epoch 68/2, Step 600/600, Loss = 0.9134978652000427\n",
      "Epoch 69/2, Step 100/600, Loss = 1.1227142810821533\n",
      "Epoch 69/2, Step 200/600, Loss = 0.8311914801597595\n",
      "Epoch 69/2, Step 300/600, Loss = 1.1528064012527466\n",
      "Epoch 69/2, Step 400/600, Loss = 1.0613876581192017\n",
      "Epoch 69/2, Step 500/600, Loss = 0.8016231060028076\n",
      "Epoch 69/2, Step 600/600, Loss = 0.9253050088882446\n",
      "Epoch 70/2, Step 100/600, Loss = 0.8102806806564331\n",
      "Epoch 70/2, Step 200/600, Loss = 0.9444114565849304\n",
      "Epoch 70/2, Step 300/600, Loss = 0.757402777671814\n",
      "Epoch 70/2, Step 400/600, Loss = 0.81574547290802\n",
      "Epoch 70/2, Step 500/600, Loss = 0.8763905167579651\n",
      "Epoch 70/2, Step 600/600, Loss = 0.959601640701294\n",
      "Epoch 71/2, Step 100/600, Loss = 0.9977297782897949\n",
      "Epoch 71/2, Step 200/600, Loss = 0.9363073706626892\n",
      "Epoch 71/2, Step 300/600, Loss = 1.0037455558776855\n",
      "Epoch 71/2, Step 400/600, Loss = 0.9176404476165771\n",
      "Epoch 71/2, Step 500/600, Loss = 0.8400599956512451\n",
      "Epoch 71/2, Step 600/600, Loss = 2.4334917068481445\n",
      "Epoch 72/2, Step 100/600, Loss = 0.8276694416999817\n",
      "Epoch 72/2, Step 200/600, Loss = 0.9166362285614014\n",
      "Epoch 72/2, Step 300/600, Loss = 0.8022077083587646\n",
      "Epoch 72/2, Step 400/600, Loss = 0.7659963965415955\n",
      "Epoch 72/2, Step 500/600, Loss = 0.8771619200706482\n",
      "Epoch 72/2, Step 600/600, Loss = 0.8225677609443665\n",
      "Epoch 73/2, Step 100/600, Loss = 0.7587130665779114\n",
      "Epoch 73/2, Step 200/600, Loss = 0.6616339683532715\n",
      "Epoch 73/2, Step 300/600, Loss = 0.7815963625907898\n",
      "Epoch 73/2, Step 400/600, Loss = 0.9546018242835999\n",
      "Epoch 73/2, Step 500/600, Loss = 5.921698093414307\n",
      "Epoch 73/2, Step 600/600, Loss = 0.9141525030136108\n",
      "Epoch 74/2, Step 100/600, Loss = 0.8893991112709045\n",
      "Epoch 74/2, Step 200/600, Loss = 0.7694206833839417\n",
      "Epoch 74/2, Step 300/600, Loss = 0.8998444080352783\n",
      "Epoch 74/2, Step 400/600, Loss = 0.7087329626083374\n",
      "Epoch 74/2, Step 500/600, Loss = 0.8680969476699829\n",
      "Epoch 74/2, Step 600/600, Loss = 0.9780117273330688\n",
      "Epoch 75/2, Step 100/600, Loss = 1.6939826011657715\n",
      "Epoch 75/2, Step 200/600, Loss = 0.7068251967430115\n",
      "Epoch 75/2, Step 300/600, Loss = 0.9376473426818848\n",
      "Epoch 75/2, Step 400/600, Loss = 1.14107084274292\n",
      "Epoch 75/2, Step 500/600, Loss = 0.955722451210022\n",
      "Epoch 75/2, Step 600/600, Loss = 0.8213096857070923\n",
      "Epoch 76/2, Step 100/600, Loss = 0.9172865152359009\n",
      "Epoch 76/2, Step 200/600, Loss = 0.8169113397598267\n",
      "Epoch 76/2, Step 300/600, Loss = 0.710384726524353\n",
      "Epoch 76/2, Step 400/600, Loss = 1.1048778295516968\n",
      "Epoch 76/2, Step 500/600, Loss = 0.8891716003417969\n",
      "Epoch 76/2, Step 600/600, Loss = 0.8272075057029724\n",
      "Epoch 77/2, Step 100/600, Loss = 0.8126407861709595\n",
      "Epoch 77/2, Step 200/600, Loss = 0.8466867208480835\n",
      "Epoch 77/2, Step 300/600, Loss = 1.0065170526504517\n",
      "Epoch 77/2, Step 400/600, Loss = 0.6775640845298767\n",
      "Epoch 77/2, Step 500/600, Loss = 0.9572769999504089\n",
      "Epoch 77/2, Step 600/600, Loss = 0.9532716870307922\n",
      "Epoch 78/2, Step 100/600, Loss = 1.0479949712753296\n",
      "Epoch 78/2, Step 200/600, Loss = 0.9154074788093567\n",
      "Epoch 78/2, Step 300/600, Loss = 0.6233729720115662\n",
      "Epoch 78/2, Step 400/600, Loss = 0.8596016764640808\n",
      "Epoch 78/2, Step 500/600, Loss = 0.9395269155502319\n",
      "Epoch 78/2, Step 600/600, Loss = 1.0131011009216309\n",
      "Epoch 79/2, Step 100/600, Loss = 0.8205362558364868\n",
      "Epoch 79/2, Step 200/600, Loss = 0.8916376233100891\n",
      "Epoch 79/2, Step 300/600, Loss = 0.8988061547279358\n",
      "Epoch 79/2, Step 400/600, Loss = 0.5995625257492065\n",
      "Epoch 79/2, Step 500/600, Loss = 0.9631499648094177\n",
      "Epoch 79/2, Step 600/600, Loss = 0.8346110582351685\n",
      "Epoch 80/2, Step 100/600, Loss = 0.7983418107032776\n",
      "Epoch 80/2, Step 200/600, Loss = 0.8950348496437073\n",
      "Epoch 80/2, Step 300/600, Loss = 0.8150343298912048\n",
      "Epoch 80/2, Step 400/600, Loss = 0.9952149987220764\n",
      "Epoch 80/2, Step 500/600, Loss = 1.126387596130371\n",
      "Epoch 80/2, Step 600/600, Loss = 0.8570480942726135\n",
      "Epoch 81/2, Step 100/600, Loss = 0.9779614210128784\n",
      "Epoch 81/2, Step 200/600, Loss = 1.0079542398452759\n",
      "Epoch 81/2, Step 300/600, Loss = 0.7070785760879517\n",
      "Epoch 81/2, Step 400/600, Loss = 0.6565331220626831\n",
      "Epoch 81/2, Step 500/600, Loss = 1.0250641107559204\n",
      "Epoch 81/2, Step 600/600, Loss = 0.877247154712677\n",
      "Epoch 82/2, Step 100/600, Loss = 0.8409615159034729\n",
      "Epoch 82/2, Step 200/600, Loss = 0.9361103773117065\n",
      "Epoch 82/2, Step 300/600, Loss = 0.7590369582176208\n",
      "Epoch 82/2, Step 400/600, Loss = 1.0127264261245728\n",
      "Epoch 82/2, Step 500/600, Loss = 0.9753439426422119\n",
      "Epoch 82/2, Step 600/600, Loss = 0.8921718001365662\n",
      "Epoch 83/2, Step 100/600, Loss = 0.8432972431182861\n",
      "Epoch 83/2, Step 200/600, Loss = 0.6873670816421509\n",
      "Epoch 83/2, Step 300/600, Loss = 0.8355440497398376\n",
      "Epoch 83/2, Step 400/600, Loss = 0.8778874278068542\n",
      "Epoch 83/2, Step 500/600, Loss = 1.1201670169830322\n",
      "Epoch 83/2, Step 600/600, Loss = 1.1318559646606445\n",
      "Epoch 84/2, Step 100/600, Loss = 1.057093620300293\n",
      "Epoch 84/2, Step 200/600, Loss = 0.856097936630249\n",
      "Epoch 84/2, Step 300/600, Loss = 1.0674091577529907\n",
      "Epoch 84/2, Step 400/600, Loss = 0.907604455947876\n",
      "Epoch 84/2, Step 500/600, Loss = 0.8491188883781433\n",
      "Epoch 84/2, Step 600/600, Loss = 0.7484123110771179\n",
      "Epoch 85/2, Step 100/600, Loss = 0.8030826449394226\n",
      "Epoch 85/2, Step 200/600, Loss = 0.7657334804534912\n",
      "Epoch 85/2, Step 300/600, Loss = 0.9630773067474365\n",
      "Epoch 85/2, Step 400/600, Loss = 0.8245598077774048\n",
      "Epoch 85/2, Step 500/600, Loss = 0.8036302328109741\n",
      "Epoch 85/2, Step 600/600, Loss = 0.7438449859619141\n",
      "Epoch 86/2, Step 100/600, Loss = 0.9383687376976013\n",
      "Epoch 86/2, Step 200/600, Loss = 0.9587273597717285\n",
      "Epoch 86/2, Step 300/600, Loss = 0.9953945279121399\n",
      "Epoch 86/2, Step 400/600, Loss = 0.7897275686264038\n",
      "Epoch 86/2, Step 500/600, Loss = 0.92121422290802\n",
      "Epoch 86/2, Step 600/600, Loss = 0.9359541535377502\n",
      "Epoch 87/2, Step 100/600, Loss = 1.0321874618530273\n",
      "Epoch 87/2, Step 200/600, Loss = 1.0204918384552002\n",
      "Epoch 87/2, Step 300/600, Loss = 0.9164116382598877\n",
      "Epoch 87/2, Step 400/600, Loss = 1.0213702917099\n",
      "Epoch 87/2, Step 500/600, Loss = 0.8573287129402161\n",
      "Epoch 87/2, Step 600/600, Loss = 0.9878326654434204\n",
      "Epoch 88/2, Step 100/600, Loss = 0.8011358380317688\n",
      "Epoch 88/2, Step 200/600, Loss = 1.0026497840881348\n",
      "Epoch 88/2, Step 300/600, Loss = 0.7799833416938782\n",
      "Epoch 88/2, Step 400/600, Loss = 0.944084644317627\n",
      "Epoch 88/2, Step 500/600, Loss = 0.755047619342804\n",
      "Epoch 88/2, Step 600/600, Loss = 0.9036090970039368\n",
      "Epoch 89/2, Step 100/600, Loss = 1.4520796537399292\n",
      "Epoch 89/2, Step 200/600, Loss = 1.122624397277832\n",
      "Epoch 89/2, Step 300/600, Loss = 0.7912085652351379\n",
      "Epoch 89/2, Step 400/600, Loss = 1.0401997566223145\n",
      "Epoch 89/2, Step 500/600, Loss = 3.5015125274658203\n",
      "Epoch 89/2, Step 600/600, Loss = 1.0399471521377563\n",
      "Epoch 90/2, Step 100/600, Loss = 0.8917107582092285\n",
      "Epoch 90/2, Step 200/600, Loss = 0.9354947805404663\n",
      "Epoch 90/2, Step 300/600, Loss = 0.9291160702705383\n",
      "Epoch 90/2, Step 400/600, Loss = 0.869597315788269\n",
      "Epoch 90/2, Step 500/600, Loss = 0.9538225531578064\n",
      "Epoch 90/2, Step 600/600, Loss = 0.9695737957954407\n",
      "Epoch 91/2, Step 100/600, Loss = 1.6480680704116821\n",
      "Epoch 91/2, Step 200/600, Loss = 0.9642238020896912\n",
      "Epoch 91/2, Step 300/600, Loss = 0.9838369488716125\n",
      "Epoch 91/2, Step 400/600, Loss = 0.9681933522224426\n",
      "Epoch 91/2, Step 500/600, Loss = 0.8909254670143127\n",
      "Epoch 91/2, Step 600/600, Loss = 1.2341666221618652\n",
      "Epoch 92/2, Step 100/600, Loss = 0.9124168157577515\n",
      "Epoch 92/2, Step 200/600, Loss = 1.1711885929107666\n",
      "Epoch 92/2, Step 300/600, Loss = 0.9614720940589905\n",
      "Epoch 92/2, Step 400/600, Loss = 0.9646548628807068\n",
      "Epoch 92/2, Step 500/600, Loss = 0.9675973653793335\n",
      "Epoch 92/2, Step 600/600, Loss = 0.9125069379806519\n",
      "Epoch 93/2, Step 100/600, Loss = 1.2194929122924805\n",
      "Epoch 93/2, Step 200/600, Loss = 0.8865460753440857\n",
      "Epoch 93/2, Step 300/600, Loss = 1.0864875316619873\n",
      "Epoch 93/2, Step 400/600, Loss = 0.7770749926567078\n",
      "Epoch 93/2, Step 500/600, Loss = 0.9440035223960876\n",
      "Epoch 93/2, Step 600/600, Loss = 1.0080904960632324\n",
      "Epoch 94/2, Step 100/600, Loss = 0.9012796878814697\n",
      "Epoch 94/2, Step 200/600, Loss = 1.0530650615692139\n",
      "Epoch 94/2, Step 300/600, Loss = 1.1763874292373657\n",
      "Epoch 94/2, Step 400/600, Loss = 1.0310311317443848\n",
      "Epoch 94/2, Step 500/600, Loss = 0.9436666965484619\n",
      "Epoch 94/2, Step 600/600, Loss = 1.0767890214920044\n",
      "Epoch 95/2, Step 100/600, Loss = 0.9275002479553223\n",
      "Epoch 95/2, Step 200/600, Loss = 1.2771872282028198\n",
      "Epoch 95/2, Step 300/600, Loss = 1.1046490669250488\n",
      "Epoch 95/2, Step 400/600, Loss = 0.9791279435157776\n",
      "Epoch 95/2, Step 500/600, Loss = 1.1177902221679688\n",
      "Epoch 95/2, Step 600/600, Loss = 1.2872079610824585\n",
      "Epoch 96/2, Step 100/600, Loss = 0.9654865264892578\n",
      "Epoch 96/2, Step 200/600, Loss = 0.9471855163574219\n",
      "Epoch 96/2, Step 300/600, Loss = 0.8630461692810059\n",
      "Epoch 96/2, Step 400/600, Loss = 0.9796386957168579\n",
      "Epoch 96/2, Step 500/600, Loss = 0.9356235265731812\n",
      "Epoch 96/2, Step 600/600, Loss = 4.69180154800415\n",
      "Epoch 97/2, Step 100/600, Loss = 1.0166940689086914\n",
      "Epoch 97/2, Step 200/600, Loss = 1.0060267448425293\n",
      "Epoch 97/2, Step 300/600, Loss = 1.0530285835266113\n",
      "Epoch 97/2, Step 400/600, Loss = 0.9554122686386108\n",
      "Epoch 97/2, Step 500/600, Loss = 1.1762325763702393\n",
      "Epoch 97/2, Step 600/600, Loss = 0.8420665264129639\n",
      "Epoch 98/2, Step 100/600, Loss = 0.9024140238761902\n",
      "Epoch 98/2, Step 200/600, Loss = 0.9047486782073975\n",
      "Epoch 98/2, Step 300/600, Loss = 1.0019949674606323\n",
      "Epoch 98/2, Step 400/600, Loss = 0.9895340800285339\n",
      "Epoch 98/2, Step 500/600, Loss = 1.1961630582809448\n",
      "Epoch 98/2, Step 600/600, Loss = 1.0367282629013062\n",
      "Epoch 99/2, Step 100/600, Loss = 0.9898934364318848\n",
      "Epoch 99/2, Step 200/600, Loss = 1.0586587190628052\n",
      "Epoch 99/2, Step 300/600, Loss = 0.9589261412620544\n",
      "Epoch 99/2, Step 400/600, Loss = 1.0684748888015747\n",
      "Epoch 99/2, Step 500/600, Loss = 1.2666999101638794\n",
      "Epoch 99/2, Step 600/600, Loss = 1.1515848636627197\n",
      "Epoch 100/2, Step 100/600, Loss = 0.8500739336013794\n",
      "Epoch 100/2, Step 200/600, Loss = 0.826601505279541\n",
      "Epoch 100/2, Step 300/600, Loss = 0.875834047794342\n",
      "Epoch 100/2, Step 400/600, Loss = 1.0153920650482178\n",
      "Epoch 100/2, Step 500/600, Loss = 1.005009412765503\n",
      "Epoch 100/2, Step 600/600, Loss = 0.907603919506073\n",
      "Epoch 101/2, Step 100/600, Loss = 1.0590808391571045\n",
      "Epoch 101/2, Step 200/600, Loss = 0.9065120220184326\n",
      "Epoch 101/2, Step 300/600, Loss = 1.0213462114334106\n",
      "Epoch 101/2, Step 400/600, Loss = 1.0667978525161743\n",
      "Epoch 101/2, Step 500/600, Loss = 0.955455482006073\n",
      "Epoch 101/2, Step 600/600, Loss = 3.5367236137390137\n",
      "Epoch 102/2, Step 100/600, Loss = 0.9317905306816101\n",
      "Epoch 102/2, Step 200/600, Loss = 1.0963882207870483\n",
      "Epoch 102/2, Step 300/600, Loss = 0.6783016324043274\n",
      "Epoch 102/2, Step 400/600, Loss = 1.168168306350708\n",
      "Epoch 102/2, Step 500/600, Loss = 0.8513214588165283\n",
      "Epoch 102/2, Step 600/600, Loss = 0.9252774119377136\n",
      "Epoch 103/2, Step 100/600, Loss = 0.8893258571624756\n",
      "Epoch 103/2, Step 200/600, Loss = 0.9581059813499451\n",
      "Epoch 103/2, Step 300/600, Loss = 0.7706426382064819\n",
      "Epoch 103/2, Step 400/600, Loss = 0.7496181726455688\n",
      "Epoch 103/2, Step 500/600, Loss = 0.8433468341827393\n",
      "Epoch 103/2, Step 600/600, Loss = 0.9925335049629211\n",
      "Epoch 104/2, Step 100/600, Loss = 0.8410821557044983\n",
      "Epoch 104/2, Step 200/600, Loss = 0.9447781443595886\n",
      "Epoch 104/2, Step 300/600, Loss = 0.9556362628936768\n",
      "Epoch 104/2, Step 400/600, Loss = 0.8837915658950806\n",
      "Epoch 104/2, Step 500/600, Loss = 1.0019049644470215\n",
      "Epoch 104/2, Step 600/600, Loss = 1.0533541440963745\n",
      "Epoch 105/2, Step 100/600, Loss = 1.3196237087249756\n",
      "Epoch 105/2, Step 200/600, Loss = 0.919048547744751\n",
      "Epoch 105/2, Step 300/600, Loss = 1.0391062498092651\n",
      "Epoch 105/2, Step 400/600, Loss = 1.037855625152588\n",
      "Epoch 105/2, Step 500/600, Loss = 0.8680476546287537\n",
      "Epoch 105/2, Step 600/600, Loss = 1.2745983600616455\n",
      "Epoch 106/2, Step 100/600, Loss = 1.1984158754348755\n",
      "Epoch 106/2, Step 200/600, Loss = 0.9801643490791321\n",
      "Epoch 106/2, Step 300/600, Loss = 0.9832670092582703\n",
      "Epoch 106/2, Step 400/600, Loss = 0.9933632612228394\n",
      "Epoch 106/2, Step 500/600, Loss = 1.0529181957244873\n",
      "Epoch 106/2, Step 600/600, Loss = 0.9988800287246704\n",
      "Epoch 107/2, Step 100/600, Loss = 0.9070172905921936\n",
      "Epoch 107/2, Step 200/600, Loss = 1.1326758861541748\n",
      "Epoch 107/2, Step 300/600, Loss = 0.9648040533065796\n",
      "Epoch 107/2, Step 400/600, Loss = 1.170366644859314\n",
      "Epoch 107/2, Step 500/600, Loss = 0.8446612358093262\n",
      "Epoch 107/2, Step 600/600, Loss = 0.9501925706863403\n",
      "Epoch 108/2, Step 100/600, Loss = 0.9924488663673401\n",
      "Epoch 108/2, Step 200/600, Loss = 1.048979640007019\n",
      "Epoch 108/2, Step 300/600, Loss = 1.0696320533752441\n",
      "Epoch 108/2, Step 400/600, Loss = 0.8801918029785156\n",
      "Epoch 108/2, Step 500/600, Loss = 1.2900538444519043\n",
      "Epoch 108/2, Step 600/600, Loss = 1.052261471748352\n",
      "Epoch 109/2, Step 100/600, Loss = 1.144097924232483\n",
      "Epoch 109/2, Step 200/600, Loss = 1.0813944339752197\n",
      "Epoch 109/2, Step 300/600, Loss = 1.1906710863113403\n",
      "Epoch 109/2, Step 400/600, Loss = 1.2320189476013184\n",
      "Epoch 109/2, Step 500/600, Loss = 0.9057646989822388\n",
      "Epoch 109/2, Step 600/600, Loss = 0.9706736207008362\n",
      "Epoch 110/2, Step 100/600, Loss = 0.8514292240142822\n",
      "Epoch 110/2, Step 200/600, Loss = 1.1878613233566284\n",
      "Epoch 110/2, Step 300/600, Loss = 1.4449124336242676\n",
      "Epoch 110/2, Step 400/600, Loss = 1.163486361503601\n",
      "Epoch 110/2, Step 500/600, Loss = 1.0940959453582764\n",
      "Epoch 110/2, Step 600/600, Loss = 1.1176899671554565\n",
      "Epoch 111/2, Step 100/600, Loss = 0.9693779945373535\n",
      "Epoch 111/2, Step 200/600, Loss = 1.1693451404571533\n",
      "Epoch 111/2, Step 300/600, Loss = 0.8107582926750183\n",
      "Epoch 111/2, Step 400/600, Loss = 0.9729028940200806\n",
      "Epoch 111/2, Step 500/600, Loss = 1.0023093223571777\n",
      "Epoch 111/2, Step 600/600, Loss = 0.9537531137466431\n",
      "Epoch 112/2, Step 100/600, Loss = 0.969768762588501\n",
      "Epoch 112/2, Step 200/600, Loss = 1.0881648063659668\n",
      "Epoch 112/2, Step 300/600, Loss = 0.8262264728546143\n",
      "Epoch 112/2, Step 400/600, Loss = 1.0947322845458984\n",
      "Epoch 112/2, Step 500/600, Loss = 0.8692848086357117\n",
      "Epoch 112/2, Step 600/600, Loss = 1.0472480058670044\n",
      "Epoch 113/2, Step 100/600, Loss = 0.9491748809814453\n",
      "Epoch 113/2, Step 200/600, Loss = 1.2301030158996582\n",
      "Epoch 113/2, Step 300/600, Loss = 0.8939557075500488\n",
      "Epoch 113/2, Step 400/600, Loss = 1.1831598281860352\n",
      "Epoch 113/2, Step 500/600, Loss = 0.8809939026832581\n",
      "Epoch 113/2, Step 600/600, Loss = 1.2115919589996338\n",
      "Epoch 114/2, Step 100/600, Loss = 1.0799161195755005\n",
      "Epoch 114/2, Step 200/600, Loss = 1.2084256410598755\n",
      "Epoch 114/2, Step 300/600, Loss = 1.292026400566101\n",
      "Epoch 114/2, Step 400/600, Loss = 0.997346818447113\n",
      "Epoch 114/2, Step 500/600, Loss = 0.8822446465492249\n",
      "Epoch 114/2, Step 600/600, Loss = 0.9611825346946716\n",
      "Epoch 115/2, Step 100/600, Loss = 1.2011313438415527\n",
      "Epoch 115/2, Step 200/600, Loss = 0.9471514821052551\n",
      "Epoch 115/2, Step 300/600, Loss = 1.0996307134628296\n",
      "Epoch 115/2, Step 400/600, Loss = 0.9519289135932922\n",
      "Epoch 115/2, Step 500/600, Loss = 1.0959968566894531\n",
      "Epoch 115/2, Step 600/600, Loss = 1.023389458656311\n",
      "Epoch 116/2, Step 100/600, Loss = 1.091320514678955\n",
      "Epoch 116/2, Step 200/600, Loss = 1.5815461874008179\n",
      "Epoch 116/2, Step 300/600, Loss = 1.1701761484146118\n",
      "Epoch 116/2, Step 400/600, Loss = 0.9376722574234009\n",
      "Epoch 116/2, Step 500/600, Loss = 0.9826606512069702\n",
      "Epoch 116/2, Step 600/600, Loss = 1.155881404876709\n",
      "Epoch 117/2, Step 100/600, Loss = 1.14252507686615\n",
      "Epoch 117/2, Step 200/600, Loss = 1.2842718362808228\n",
      "Epoch 117/2, Step 300/600, Loss = 0.9447442889213562\n",
      "Epoch 117/2, Step 400/600, Loss = 0.9432163238525391\n",
      "Epoch 117/2, Step 500/600, Loss = 0.9805875420570374\n",
      "Epoch 117/2, Step 600/600, Loss = 0.8650454878807068\n",
      "Epoch 118/2, Step 100/600, Loss = 1.0672706365585327\n",
      "Epoch 118/2, Step 200/600, Loss = 0.9769856333732605\n",
      "Epoch 118/2, Step 300/600, Loss = 1.25490140914917\n",
      "Epoch 118/2, Step 400/600, Loss = 1.1666159629821777\n",
      "Epoch 118/2, Step 500/600, Loss = 1.1793015003204346\n",
      "Epoch 118/2, Step 600/600, Loss = 1.308611273765564\n",
      "Epoch 119/2, Step 100/600, Loss = 0.9204397797584534\n",
      "Epoch 119/2, Step 200/600, Loss = 1.1694483757019043\n",
      "Epoch 119/2, Step 300/600, Loss = 1.0589327812194824\n",
      "Epoch 119/2, Step 400/600, Loss = 1.120448350906372\n",
      "Epoch 119/2, Step 500/600, Loss = 1.0813891887664795\n",
      "Epoch 119/2, Step 600/600, Loss = 1.0140063762664795\n",
      "Epoch 120/2, Step 100/600, Loss = 1.1402735710144043\n",
      "Epoch 120/2, Step 200/600, Loss = 1.0905108451843262\n",
      "Epoch 120/2, Step 300/600, Loss = 1.172010898590088\n",
      "Epoch 120/2, Step 400/600, Loss = 1.1697546243667603\n",
      "Epoch 120/2, Step 500/600, Loss = 0.9990275502204895\n",
      "Epoch 120/2, Step 600/600, Loss = 0.9151715040206909\n",
      "Epoch 121/2, Step 100/600, Loss = 1.0135456323623657\n",
      "Epoch 121/2, Step 200/600, Loss = 1.139292597770691\n",
      "Epoch 121/2, Step 300/600, Loss = 1.008939266204834\n",
      "Epoch 121/2, Step 400/600, Loss = 0.9783251285552979\n",
      "Epoch 121/2, Step 500/600, Loss = 1.1758157014846802\n",
      "Epoch 121/2, Step 600/600, Loss = 1.0372105836868286\n",
      "Epoch 122/2, Step 100/600, Loss = 0.9431461095809937\n",
      "Epoch 122/2, Step 200/600, Loss = 0.9736201763153076\n",
      "Epoch 122/2, Step 300/600, Loss = 1.1448436975479126\n",
      "Epoch 122/2, Step 400/600, Loss = 1.0905797481536865\n",
      "Epoch 122/2, Step 500/600, Loss = 0.917555570602417\n",
      "Epoch 122/2, Step 600/600, Loss = 1.107119083404541\n",
      "Epoch 123/2, Step 100/600, Loss = 1.223411202430725\n",
      "Epoch 123/2, Step 200/600, Loss = 0.9071429371833801\n",
      "Epoch 123/2, Step 300/600, Loss = 1.3191577196121216\n",
      "Epoch 123/2, Step 400/600, Loss = 1.002575159072876\n",
      "Epoch 123/2, Step 500/600, Loss = 1.2181555032730103\n",
      "Epoch 123/2, Step 600/600, Loss = 0.9828943610191345\n",
      "Epoch 124/2, Step 100/600, Loss = 1.0466817617416382\n",
      "Epoch 124/2, Step 200/600, Loss = 1.2025505304336548\n",
      "Epoch 124/2, Step 300/600, Loss = 1.2675625085830688\n",
      "Epoch 124/2, Step 400/600, Loss = 13.327413558959961\n",
      "Epoch 124/2, Step 500/600, Loss = 1.0765613317489624\n",
      "Epoch 124/2, Step 600/600, Loss = 1.1245561838150024\n",
      "Epoch 125/2, Step 100/600, Loss = 1.0164203643798828\n",
      "Epoch 125/2, Step 200/600, Loss = 1.06329345703125\n",
      "Epoch 125/2, Step 300/600, Loss = 1.078710675239563\n",
      "Epoch 125/2, Step 400/600, Loss = 0.8508219718933105\n",
      "Epoch 125/2, Step 500/600, Loss = 1.2426761388778687\n",
      "Epoch 125/2, Step 600/600, Loss = 1.2162847518920898\n",
      "Epoch 126/2, Step 100/600, Loss = 1.0583581924438477\n",
      "Epoch 126/2, Step 200/600, Loss = 1.088435411453247\n",
      "Epoch 126/2, Step 300/600, Loss = 1.1687506437301636\n",
      "Epoch 126/2, Step 400/600, Loss = 1.2771549224853516\n",
      "Epoch 126/2, Step 500/600, Loss = 1.088971495628357\n",
      "Epoch 126/2, Step 600/600, Loss = 1.2270170450210571\n",
      "Epoch 127/2, Step 100/600, Loss = 0.7867478728294373\n",
      "Epoch 127/2, Step 200/600, Loss = 1.0641082525253296\n",
      "Epoch 127/2, Step 300/600, Loss = 1.2510879039764404\n",
      "Epoch 127/2, Step 400/600, Loss = 1.1165189743041992\n",
      "Epoch 127/2, Step 500/600, Loss = 1.0194871425628662\n",
      "Epoch 127/2, Step 600/600, Loss = 1.165619969367981\n",
      "Epoch 128/2, Step 100/600, Loss = 0.9449299573898315\n",
      "Epoch 128/2, Step 200/600, Loss = 1.2520818710327148\n",
      "Epoch 128/2, Step 300/600, Loss = 1.0289477109909058\n",
      "Epoch 128/2, Step 400/600, Loss = 1.1345975399017334\n",
      "Epoch 128/2, Step 500/600, Loss = 1.2280763387680054\n",
      "Epoch 128/2, Step 600/600, Loss = 1.1817834377288818\n",
      "Epoch 129/2, Step 100/600, Loss = 1.1544253826141357\n",
      "Epoch 129/2, Step 200/600, Loss = 1.2719769477844238\n",
      "Epoch 129/2, Step 300/600, Loss = 0.952714204788208\n",
      "Epoch 129/2, Step 400/600, Loss = 1.0924572944641113\n",
      "Epoch 129/2, Step 500/600, Loss = 1.0895817279815674\n",
      "Epoch 129/2, Step 600/600, Loss = 1.0272266864776611\n",
      "Epoch 130/2, Step 100/600, Loss = 1.3978177309036255\n",
      "Epoch 130/2, Step 200/600, Loss = 0.9922372698783875\n",
      "Epoch 130/2, Step 300/600, Loss = 1.2450186014175415\n",
      "Epoch 130/2, Step 400/600, Loss = 1.3136422634124756\n",
      "Epoch 130/2, Step 500/600, Loss = 1.1767349243164062\n",
      "Epoch 130/2, Step 600/600, Loss = 1.0402214527130127\n",
      "Epoch 131/2, Step 100/600, Loss = 0.9488855600357056\n",
      "Epoch 131/2, Step 200/600, Loss = 1.2891830205917358\n",
      "Epoch 131/2, Step 300/600, Loss = 5.819146156311035\n",
      "Epoch 131/2, Step 400/600, Loss = 1.0394635200500488\n",
      "Epoch 131/2, Step 500/600, Loss = 1.2524383068084717\n",
      "Epoch 131/2, Step 600/600, Loss = 0.9309182167053223\n",
      "Epoch 132/2, Step 100/600, Loss = 1.3829259872436523\n",
      "Epoch 132/2, Step 200/600, Loss = 1.2943233251571655\n",
      "Epoch 132/2, Step 300/600, Loss = 1.3904768228530884\n",
      "Epoch 132/2, Step 400/600, Loss = 1.1972460746765137\n",
      "Epoch 132/2, Step 500/600, Loss = 1.125728726387024\n",
      "Epoch 132/2, Step 600/600, Loss = 1.0503937005996704\n",
      "Epoch 133/2, Step 100/600, Loss = 1.2865720987319946\n",
      "Epoch 133/2, Step 200/600, Loss = 1.1195122003555298\n",
      "Epoch 133/2, Step 300/600, Loss = 1.0243679285049438\n",
      "Epoch 133/2, Step 400/600, Loss = 1.222158432006836\n",
      "Epoch 133/2, Step 500/600, Loss = 1.1251733303070068\n",
      "Epoch 133/2, Step 600/600, Loss = 1.098484754562378\n",
      "Epoch 134/2, Step 100/600, Loss = 1.0416301488876343\n",
      "Epoch 134/2, Step 200/600, Loss = 1.18071448802948\n",
      "Epoch 134/2, Step 300/600, Loss = 1.4911465644836426\n",
      "Epoch 134/2, Step 400/600, Loss = 1.0304076671600342\n",
      "Epoch 134/2, Step 500/600, Loss = 1.1119945049285889\n",
      "Epoch 134/2, Step 600/600, Loss = 1.034232258796692\n",
      "Epoch 135/2, Step 100/600, Loss = 1.2132830619812012\n",
      "Epoch 135/2, Step 200/600, Loss = 0.9041855335235596\n",
      "Epoch 135/2, Step 300/600, Loss = 1.2977746725082397\n",
      "Epoch 135/2, Step 400/600, Loss = 1.0435404777526855\n",
      "Epoch 135/2, Step 500/600, Loss = 1.2607557773590088\n",
      "Epoch 135/2, Step 600/600, Loss = 0.9320670366287231\n",
      "Epoch 136/2, Step 100/600, Loss = 1.1915870904922485\n",
      "Epoch 136/2, Step 200/600, Loss = 1.0273414850234985\n",
      "Epoch 136/2, Step 300/600, Loss = 1.0501269102096558\n",
      "Epoch 136/2, Step 400/600, Loss = 1.119990587234497\n",
      "Epoch 136/2, Step 500/600, Loss = 1.1824095249176025\n",
      "Epoch 136/2, Step 600/600, Loss = 1.0864462852478027\n",
      "Epoch 137/2, Step 100/600, Loss = 1.0646263360977173\n",
      "Epoch 137/2, Step 200/600, Loss = 1.2496929168701172\n",
      "Epoch 137/2, Step 300/600, Loss = 1.0207858085632324\n",
      "Epoch 137/2, Step 400/600, Loss = 1.3947466611862183\n",
      "Epoch 137/2, Step 500/600, Loss = 1.2226680517196655\n",
      "Epoch 137/2, Step 600/600, Loss = 1.1095365285873413\n",
      "Epoch 138/2, Step 100/600, Loss = 1.0456393957138062\n",
      "Epoch 138/2, Step 200/600, Loss = 1.0698119401931763\n",
      "Epoch 138/2, Step 300/600, Loss = 1.166853666305542\n",
      "Epoch 138/2, Step 400/600, Loss = 1.1210620403289795\n",
      "Epoch 138/2, Step 500/600, Loss = 1.3521723747253418\n",
      "Epoch 138/2, Step 600/600, Loss = 1.2136839628219604\n",
      "Epoch 139/2, Step 100/600, Loss = 1.1335511207580566\n",
      "Epoch 139/2, Step 200/600, Loss = 1.2536982297897339\n",
      "Epoch 139/2, Step 300/600, Loss = 0.9985596537590027\n",
      "Epoch 139/2, Step 400/600, Loss = 1.0793848037719727\n",
      "Epoch 139/2, Step 500/600, Loss = 1.1473824977874756\n",
      "Epoch 139/2, Step 600/600, Loss = 1.1756478548049927\n",
      "Epoch 140/2, Step 100/600, Loss = 1.2361572980880737\n",
      "Epoch 140/2, Step 200/600, Loss = 1.094132661819458\n",
      "Epoch 140/2, Step 300/600, Loss = 0.9738401770591736\n",
      "Epoch 140/2, Step 400/600, Loss = 1.1800965070724487\n",
      "Epoch 140/2, Step 500/600, Loss = 1.2730231285095215\n",
      "Epoch 140/2, Step 600/600, Loss = 1.018224835395813\n",
      "Epoch 141/2, Step 100/600, Loss = 1.2341639995574951\n",
      "Epoch 141/2, Step 200/600, Loss = 1.0333207845687866\n",
      "Epoch 141/2, Step 300/600, Loss = 1.3870291709899902\n",
      "Epoch 141/2, Step 400/600, Loss = 1.2132821083068848\n",
      "Epoch 141/2, Step 500/600, Loss = 1.3050148487091064\n",
      "Epoch 141/2, Step 600/600, Loss = 0.997501015663147\n",
      "Epoch 142/2, Step 100/600, Loss = 1.3193330764770508\n",
      "Epoch 142/2, Step 200/600, Loss = 1.3172357082366943\n",
      "Epoch 142/2, Step 300/600, Loss = 1.3333957195281982\n",
      "Epoch 142/2, Step 400/600, Loss = 1.2446669340133667\n",
      "Epoch 142/2, Step 500/600, Loss = 1.2386672496795654\n",
      "Epoch 142/2, Step 600/600, Loss = 0.9944078326225281\n",
      "Epoch 143/2, Step 100/600, Loss = 1.3145158290863037\n",
      "Epoch 143/2, Step 200/600, Loss = 1.359148383140564\n",
      "Epoch 143/2, Step 300/600, Loss = 1.0515986680984497\n",
      "Epoch 143/2, Step 400/600, Loss = 1.0801550149917603\n",
      "Epoch 143/2, Step 500/600, Loss = 1.046901822090149\n",
      "Epoch 143/2, Step 600/600, Loss = 1.2093522548675537\n",
      "Epoch 144/2, Step 100/600, Loss = 1.2091902494430542\n",
      "Epoch 144/2, Step 200/600, Loss = 1.3072932958602905\n",
      "Epoch 144/2, Step 300/600, Loss = 1.1951725482940674\n",
      "Epoch 144/2, Step 400/600, Loss = 1.2403814792633057\n",
      "Epoch 144/2, Step 500/600, Loss = 1.0948610305786133\n",
      "Epoch 144/2, Step 600/600, Loss = 1.31231689453125\n",
      "Epoch 145/2, Step 100/600, Loss = 1.2031632661819458\n",
      "Epoch 145/2, Step 200/600, Loss = 1.2033263444900513\n",
      "Epoch 145/2, Step 300/600, Loss = 1.0942308902740479\n",
      "Epoch 145/2, Step 400/600, Loss = 0.9675359129905701\n",
      "Epoch 145/2, Step 500/600, Loss = 1.801308035850525\n",
      "Epoch 145/2, Step 600/600, Loss = 1.1572340726852417\n",
      "Epoch 146/2, Step 100/600, Loss = 1.1104809045791626\n",
      "Epoch 146/2, Step 200/600, Loss = 1.1997987031936646\n",
      "Epoch 146/2, Step 300/600, Loss = 1.2436091899871826\n",
      "Epoch 146/2, Step 400/600, Loss = 1.105444312095642\n",
      "Epoch 146/2, Step 500/600, Loss = 1.122992753982544\n",
      "Epoch 146/2, Step 600/600, Loss = 1.062130093574524\n",
      "Epoch 147/2, Step 100/600, Loss = 1.1248481273651123\n",
      "Epoch 147/2, Step 200/600, Loss = 1.174499273300171\n",
      "Epoch 147/2, Step 300/600, Loss = 1.1245090961456299\n",
      "Epoch 147/2, Step 400/600, Loss = 1.2787727117538452\n",
      "Epoch 147/2, Step 500/600, Loss = 1.0317386388778687\n",
      "Epoch 147/2, Step 600/600, Loss = 1.2237049341201782\n",
      "Epoch 148/2, Step 100/600, Loss = 1.137756586074829\n",
      "Epoch 148/2, Step 200/600, Loss = 1.3253456354141235\n",
      "Epoch 148/2, Step 300/600, Loss = 1.2172574996948242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader)):\n\u001b[1;32m----> 2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tahah\\anaconda3\\envs\\riemann\\Lib\\site-packages\\torchvision\\transforms\\functional.py:170\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 170\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sequence_length = 28\n",
    "input_size = 28\n",
    "\n",
    "for epoch in range(len(train_loader)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1)% 100 ==0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(train_loader)}, Loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 784).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape(0)\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy = {acc}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riemann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
